<!--
=== 网页自动化HTML快照 (已清理) ===
URL: https://arxiv.org/search/advanced?advanced=1&terms-0-operator=AND&terms-0-term=large&terms-0-field=title&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2025&date-to_date=2025&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first
最后更新: 2025-09-09T16:11:36.168767
原始大小: 258 KB
清理后大小: 198 KB
压缩率: 23.3%
生成时间: 2025-09-09T16:11:38.317308

清理说明:
- 已删除: <script>、<style>、<noscript> 标签
- 已删除: style属性、onclick等事件属性
- 已删除: CSS样式定义
- 保留: id、class、name、type等选择器属性
- 保留: 文本内容和DOM结构
-->
<!DOCTYPE html>
<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><meta content="width=device-width, initial-scale=1" name="viewport"/><link href="...apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/><link href="...favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/><link href="...favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/><link href="...site.webmanifest" rel="manifest"/><link href="...safari-pinned-tab.svg" rel="mask-icon"/><link href="...favicon.ico" rel="shortcut icon"/><meta content="#b31b1b" name="msapplication-TileColor"/><meta content="images/icons/browserconfig.xml" name="msapplication-config"/><meta content="#b31b1b" name="theme-color"/><title>Advanced Search | arXiv e-print repository</title><link href="...arxivstyle.css" rel="stylesheet"/><link href="...bulma-tooltip.min.css" rel="stylesheet"/><link href="...search.css" rel="stylesheet"/></head><body><div><div id="MathJax_Hidden"><span class="MathJax MathJax_Processing" id="MathJax-Element-1-Frame" style="" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-2-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-3-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-5-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-6-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-7-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-8-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-9-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-13-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-14-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-15-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-16-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-18-Frame" tabindex="0"></span><br/></div></div><div id="MathJax_Message"></div><header><a class="is-sr-only" href="#main-container">Skip to main content</a><div class="attribution level is-marginless" role="banner"><div class="level-left"><a class="level-item" href="..."><img alt="Cornell University" aria-label="logo" src="...cornell-reduced-white-SMALL.svg"/></a></div><div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br/> the Simons Foundation, <a href="...ourmembers.html">member institutions</a>, and all contributors. <a href="...donate.html">Donate</a></span></p></div></div><div class="identity level is-marginless"><div class="level-left"><div class="level-item"><a aria-label="arxiv-logo" class="arxiv" href="..."><img alt="arxiv logo" aria-label="logo" src="...arxiv-logo-one-color-white.svg"/></a></div></div><div class="search-block level-right"><form action="https://arxiv.org/search" class="level-item mini-search" method="GET"><div class="field has-addons"><div class="control"><input aria-label="Search term or terms" class="input is-small" name="query" placeholder="Search..." type="text"/><p class="help"><a href="...help">Help</a> | <a href="...advanced">Advanced Search</a></p></div><div class="control"><div class="select is-small"><select aria-label="Field to search" name="searchtype"><option selected="selected" value="all">All fields</option><option value="title">Title</option><option value="author">Author</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select></div></div><input name="source" type="hidden" value="header"/><button class="button is-small is-cul-darker">Search</button></div></form></div></div><div class="container"><div aria-label="User menu" class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation"><a href="...login">Login</a></div></div></header><main class="container" id="main-container"><div class="level is-marginless"><div class="level-left"><h1 class="title is-clearfix"> Showing 1–50 of 9,947 results </h1></div><div class="level-right is-hidden-mobile"><span class="help"><a href="...releases">Search v0.5.6 released 2020-02-24</a></span></div></div><div class="content"><div class="columns"><div class="column is-two-thirds-tablet"><p>Query: <a href="...advanced?terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first">order: -announced_date_first; size: 50; date_range: from 2025-01-01 to 2025-12-31; include_cross_list: True; terms: AND title=large</a></p><div class="buttons"><a class="button is-link" href="...advanced?terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first">Refine query</a><a class="button" href="...advanced">New search</a></div></div><div class="column is-one-third-tablet is-hidden-mobile"><p class="has-text-right"><a href="...?order=-announced_date_first&amp;size=50">Simple Search</a></p></div></div><div class="level breathe-horizontal"><div class="level-left"><form action="/search/advanced" method="GET"><div><input id="advanced" name="advanced" type="hidden" value="1"/><ul id="terms"><li><label for="terms-0">Terms-0</label><table id="terms-0"><tbody><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="large"/></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected="" value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option selected="" value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option value="all">All fields</option></select></td></tr></tbody></table></li></ul><table id="classification"><tbody><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected="" value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked="" id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"/><label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"/><label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></tbody></table><table id="date"><tbody><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"/><label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"/><label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"/><label for="date-filter_by-2">Specific year</label></li><li><input checked="" id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"/><label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""/></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value="2025"/></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value="2025"/></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked="" id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"/><label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"/><label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"/><label for="date-date_type-2">Announcement date</label></li></ul></td></tr></tbody></table><input id="include_older_versions" name="include_older_versions" type="checkbox" value="y"/><ul id="abstracts"><li><input checked="" id="abstracts-0" name="abstracts" type="radio" value="show"/><label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"/><label for="abstracts-1">Hide abstracts</label></li></ul></div><div class="box field is-grouped is-grouped-multiline level-item"><div class="control"><span class="select is-small"><select id="size" name="size"><option value="25">25</option><option selected="" value="50">50</option><option value="100">100</option><option value="200">200</option></select></span><label for="size">results per page</label>. </div><div class="control"><label for="order">Sort results by</label><span class="select is-small"><select id="order" name="order"><option selected="" value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select></span></div><div class="control"><button class="button is-small is-link">Go</button></div></div></form></div></div><nav aria-label="pagination" class="pagination is-small is-centered breathe-horizontal" role="navigation"><a class="pagination-previous is-invisible" href="">Previous </a><a class="pagination-next" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">Next </a><ul class="pagination-list"><li><a aria-label="Goto page 1" class="pagination-link is-current" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0">1 </a></li><li><a aria-current="page" aria-label="Page 2" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">2 </a></li><li><a aria-current="page" aria-label="Page 3" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100">3 </a></li><li><a aria-current="page" aria-label="Page 4" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150">4 </a></li><li><a aria-current="page" aria-label="Page 5" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200">5 </a></li><li><span class="pagination-ellipsis">…</span></li></ul></nav><ol class="breathe-horizontal" start="1"><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06949">arXiv:2509.06949</a><span> [<a href="...2509.06949">pdf</a>, <a href="...2509.06949">ps</a>, <a href="...2509.06949">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Revolutionizing Reinforcement Learning Framework for Diffusion <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Wang%2C+Y">Yinjie Wang</a>, <a href="...?searchtype=author&amp;query=Yang%2C+L">Ling Yang</a>, <a href="...?searchtype=author&amp;query=Li%2C+B">Bowen Li</a>, <a href="...?searchtype=author&amp;query=Tian%2C+Y">Ye Tian</a>, <a href="...?searchtype=author&amp;query=Shen%2C+K">Ke Shen</a>, <a href="...?searchtype=author&amp;query=Wang%2C+M">Mengdi Wang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06949v1-abstract-short"> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it ca… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06949v1-abstract-full"> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Code and Models: https://github.com/Gen-Verse/dLLM-RL</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06858">arXiv:2509.06858</a><span> [<a href="...2509.06858">pdf</a>, <a href="...2509.06858">ps</a>, <a href="...2509.06858">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Adaptation and Self-Organizing Systems">nlin.AO</span></div></div><p class="title is-5 mathjax"> Disentangling Interaction and Bias Effects in Opinion Dynamics of <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a class="" href="...?searchtype=author&amp;query=Brockers%2C+V+C">Vincent C. Brockers</a>, <a href="...?searchtype=author&amp;query=Ehrlich%2C+D+A">David A. Ehrlich</a>, <a href="...?searchtype=author&amp;query=Priesemann%2C+V">Viola Priesemann</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06858v1-abstract-short"> Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias to… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06858v1-abstract-full"> Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias toward the initiating agent's stance. Applying this framework to multi-step dialogues reveals that opinion trajectories tend to quickly converge to a shared attractor, with the influence of the interaction fading over time, and the impact of biases differing between LLMs. In addition, we fine-tune an LLM on different sets of strongly opinionated statements (incl. misinformation) and demonstrate that the opinion attractor shifts correspondingly. Exposing stark differences between LLMs and providing quantitative tools to compare them to human subjects in the future, our approach highlights both chances and pitfalls in using LLMs as proxies for human behavior. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06838">arXiv:2509.06838</a><span> [<a href="...2509.06838">pdf</a>, <a href="...2509.06838">ps</a>, <a href="...2509.06838">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span></div></div><p class="title is-5 mathjax"> EPT Benchmark: Evaluation of Persian Trustworthiness in <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Mirbagheri%2C+M+R">Mohammad Reza Mirbagheri</a>, <a href="...?searchtype=author&amp;query=Mirkamali%2C+M+M">Mohammad Mahdi Mirkamali</a>, <a href="...?searchtype=author&amp;query=Arani%2C+Z+M">Zahra Motoshaker Arani</a>, <a href="...?searchtype=author&amp;query=Javeri%2C+A">Ali Javeri</a>, <a href="...?searchtype=author&amp;query=Sadeghzadeh%2C+A+M">Amir Mahdi Sadeghzadeh</a>, <a href="...?searchtype=author&amp;query=Jalili%2C+R">Rasool Jalili</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06838v1-abstract-short"> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cu… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06838v1-abstract-full"> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06813">arXiv:2509.06813</a><span> [<a href="...2509.06813">pdf</a>, <a href="...2509.06813">ps</a>, <a href="...2509.06813">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> A Comparative Benchmark of <span class="search-hit mathjax">Large</span> Language Models for Labelling Wind Turbine Maintenance Logs </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Malyi%2C+M">Max Malyi</a>, <a href="...?searchtype=author&amp;query=Shek%2C+J">Jonathan Shek</a>, <a href="...?searchtype=author&amp;query=McDonald%2C+A">Alasdair McDonald</a>, <a href="...?searchtype=author&amp;query=Biscaya%2C+A">Andre Biscaya</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06813v1-abstract-short"> Effective Operation and Maintenance (O&amp;M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06813v1-abstract-full"> Effective Operation and Maintenance (O&amp;M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&amp;M data quality and downstream reliability analysis. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Associated GitHub repository: https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06801">arXiv:2509.06801</a><span> [<a href="...2509.06801">pdf</a>, <a href="...2509.06801">ps</a>, <a href="...2509.06801">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Astrophysics of Galaxies">astro-ph.GA</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span> eddy simulations in astrophysics </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Schmidt-Br%C3%BCckner%2C+W">Wolfram Schmidt-Brückner</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06801v1-abstract-short"> In this review, the methodology of large eddy simulations (LES) is introduced and applications in astrophysics are discussed. As theoretical framework, the scale decomposition of the dynamical equations for compressible neutral fluids by means of spatial filtering is explained. For cosmological applications, the filtered equations in co-moving coordinates are formulated. Moreover, the decompositio… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06801v1-abstract-full"> In this review, the methodology of large eddy simulations (LES) is introduced and applications in astrophysics are discussed. As theoretical framework, the scale decomposition of the dynamical equations for compressible neutral fluids by means of spatial filtering is explained. For cosmological applications, the filtered equations in co-moving coordinates are formulated. Moreover, the decomposition is extended to magnetohydrodynamics (MHD). While energy is dissipated through numerical diffusivities in implicit large eddy simulations (ILES), explicit subgrid-scale (SGS) models are applied in LES to compute energy dissipation, mixing, and dynamo action due to numerically unresolved turbulent eddies. The most commonly used models in astrophysics are the Smagorinsky model, the hydrodynamical SGS turbulence energy equation model, and the non-linear structural model for both non-relativistic and relativistic MHD. Model validation is carried out a priori by testing correlations between model and data for specific terms or a posteriori by comparing turbulence statistics in LES and ILES. Since most solvers in astrophysical simulation codes have significant numerical diffusion, the additional effect of SGS models is generally small. However, convergence with resolution increases in some cases. A recent example is magnetic field amplification in binary neutron star mergers. For mesh-free codes, it has been shown that explicit modelling of turbulent diffusion of metals has a significant impact. Moreover, SGS models can help to compute the turbulent velocity dispersion consistently and to parameterize sub-resolution processes that are influenced by turbulence, such as the star formation efficiency in galaxy simulations. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Completely revised and extended 2. edition, 77 pages, 18 figures, accepted for publication by Living Reviews in Computational Astrophysics</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06759">arXiv:2509.06759</a><span> [<a href="...2509.06759">pdf</a>, <a href="...2509.06759">ps</a>, <a href="...2509.06759">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Aligning <span class="search-hit mathjax">Large</span> Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nguyen%2C+T+T">Thanh Thi Nguyen</a>, <a href="...?searchtype=author&amp;query=Wilson%2C+C">Campbell Wilson</a>, <a href="...?searchtype=author&amp;query=Dalins%2C+J">Janis Dalins</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06759v1-abstract-short"> Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remain… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06759v1-abstract-full"> Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remains a critical challenge. Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) offer promising frameworks for this aligning process. While DRL enables models to optimize actions using reward signals instead of relying solely on supervised preference data, DPO directly aligns the policy with preferences, eliminating the need for an explicit reward model. This overview explores paradigms for fine-tuning LVLMs, highlighting how DRL and DPO techniques can be used to align models with human preferences and values, improve task performance, and enable adaptive multimodal interaction. We categorize key approaches, examine sources of preference data, reward signals, and discuss open challenges such as scalability, sample efficiency, continual learning, generalization, and safety. The goal is to provide a clear understanding of how DRL and DPO contribute to the evolution of robust and human-aligned LVLMs. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Accepted for publication in the Proceedings of the 8th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06685">arXiv:2509.06685</a><span> [<a href="...2509.06685">pdf</a>, <a href="...2509.06685">ps</a>, <a href="...2509.06685">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in <span class="search-hit mathjax">Large</span> Scenes </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhang%2C+S">Shengkai Zhang</a>, <a href="...?searchtype=author&amp;query=Liu%2C+Y">Yuhe Liu</a>, <a href="...?searchtype=author&amp;query=Wu%2C+G">Guanjun Wu</a>, <a href="...?searchtype=author&amp;query=He%2C+J">Jianhua He</a>, <a href="...?searchtype=author&amp;query=Wang%2C+X">Xinggang Wang</a>, <a href="...?searchtype=author&amp;query=Chen%2C+M">Mozi Chen</a>, <a href="...?searchtype=author&amp;query=Liu%2C+K">Kezhong Liu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06685v1-abstract-short"> VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Althoug… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06685v1-abstract-full"> VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06606">arXiv:2509.06606</a><span> [<a href="...2509.06606">pdf</a>, <a href="...2509.06606">ps</a>, <a href="...2509.06606">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span></div></div><p class="title is-5 mathjax"> Unveiling the Listener Structure Underlying K-pop's Global Success: A <span class="search-hit mathjax">Large</span>-Scale Listening Data Analysis </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nakamura%2C+R">Ryota Nakamura</a>, <a href="...?searchtype=author&amp;query=Nishimoto%2C+K">Keita Nishimoto</a>, <a href="...?searchtype=author&amp;query=Sakata%2C+I">Ichiro Sakata</a>, <a href="...?searchtype=author&amp;query=Asatani%2C+K">Kimitaka Asatani</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06606v1-abstract-short"> From the mid-2000s to the 2010s, K-pop moved beyond its status as a regionally popular genre in Asia and established itself as a global music genre with enthusiastic fans around the world. However, little is known about how the vast number of music listeners across the globe have listened to and perceived K-pop. This study addresses this question by analyzing a large-scale listening dataset from L… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06606v1-abstract-full"> From the mid-2000s to the 2010s, K-pop moved beyond its status as a regionally popular genre in Asia and established itself as a global music genre with enthusiastic fans around the world. However, little is known about how the vast number of music listeners across the globe have listened to and perceived K-pop. This study addresses this question by analyzing a large-scale listening dataset from Last.fm. An analysis of the distribution of play counts reveals that K-pop experienced a significant increase in plays between 2005 and 2019, largely supported by a small group of heavy listeners. The Gini coefficient in play counts is notably greater than that of existing mainstream genres and other growing niche genres. Furthermore, an analysis based on user-assigned genre tags quantitatively demonstrates that between 2005 and 2010, K-pop shed its status as a local Asian genre and established itself as a distinct music genre in its own right. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06596">arXiv:2509.06596</a><span> [<a href="...2509.06596">pdf</a>, <a href="...2509.06596">ps</a>, <a href="...2509.06596">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Tong%2C+X">Xin Tong</a>, <a href="...?searchtype=author&amp;query=Lin%2C+Z">Zhi Lin</a>, <a href="...?searchtype=author&amp;query=Wang%2C+J">Jingya Wang</a>, <a href="...?searchtype=author&amp;query=Jin%2C+B">Bo Jin</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06596v1-abstract-short"> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that d… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06596v1-abstract-full"> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06525">arXiv:2509.06525</a><span> [<a href="...2509.06525">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span></div></div><p class="title is-5 mathjax"> GPUTB: Efficient Machine Learning Tight-Binding Method for <span class="search-hit mathjax">Large</span>-Scale Electronic Properties Calculations </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Wang%2C+Y">Yunlong Wang</a>, <a href="...?searchtype=author&amp;query=Liang%2C+Z">Zhixin Liang</a>, <a href="...?searchtype=author&amp;query=Ding%2C+C">Chi Ding</a>, <a href="...?searchtype=author&amp;query=Wang%2C+J">Junjie Wang</a>, <a href="...?searchtype=author&amp;query=Fan%2C+Z">Zheyong Fan</a>, <a href="...?searchtype=author&amp;query=Wang%2C+H">Hui-Tian Wang</a>, <a href="...?searchtype=author&amp;query=Xing%2C+D">Dingyu Xing</a>, <a href="...?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06525v1-abstract-short"> The high computational cost of ab-initio methods limits their application in predicting electronic properties at the device scale. Therefore, an efficient method is needed to map the atomic structure to the electronic structure quickly. Here, we develop GPUTB, a GPU-accelerated tight-binding (TB) machine learning framework. GPUTB employs atomic environment descriptors, enabling the model parameter… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06525v1-abstract-full"> The high computational cost of ab-initio methods limits their application in predicting electronic properties at the device scale. Therefore, an efficient method is needed to map the atomic structure to the electronic structure quickly. Here, we develop GPUTB, a GPU-accelerated tight-binding (TB) machine learning framework. GPUTB employs atomic environment descriptors, enabling the model parameters to incorporate environmental dependence. This allows the model to transfer to different basis, xc-functionals, and allotropes easily. Combined with the linear scaling quantum transport method, we have calculated the electronic density of states for up to 100 million atoms in pristine graphene. Trained on finite-temperature structures, the model can be easily extended to millions of atom finite-temperature systems. Furthermore, GPUTB can also successfully describe h-BN/graphene heterojunction systems, demonstrating its capability to handle complex material with high precision. We accurately reproduce the relationship between carrier concentration and room temperature mobility in graphene to verify the framework's accuracy. Therefore, our GPUTB framework presents a delicate balance between computational accuracy and efficiency, providing a powerful computational tool for investing electronic properties for large systems with millions of atoms. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06460">arXiv:2509.06460</a><span> [<a href="...2509.06460">pdf</a>, <a href="...2509.06460">ps</a>, <a href="...2509.06460">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Chemical Physics">physics.chem-ph</span></div></div><p class="title is-5 mathjax"> Stochastic resolution of identity to CC2 for <span class="search-hit mathjax">large</span> systems: Excited-state gradients and derivative couplings </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhao%2C+C">Chongxiao Zhao</a>, <a href="...?searchtype=author&amp;query=Li%2C+C">Chenyang Li</a>, <a href="...?searchtype=author&amp;query=Dou%2C+W">Wenjie Dou</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06460v1-abstract-short"> Excited-state gradients and derivative couplings are critical for simulating excited-state dynamics. However, their calculations are very expensive within the coupled-cluster framework due to the steep scaling. In this work, we present two implementations of stochastic resolution of identity to CC2 (sRI-CC2) for excited-state analytical gradients and derivative couplings. The first method employs… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06460v1-abstract-full"> Excited-state gradients and derivative couplings are critical for simulating excited-state dynamics. However, their calculations are very expensive within the coupled-cluster framework due to the steep scaling. In this work, we present two implementations of stochastic resolution of identity to CC2 (sRI-CC2) for excited-state analytical gradients and derivative couplings. The first method employs sRI for both Coulomb and exchange terms, reducing the formal scaling to cubic. However, this method has a significant stochastic noise. Consequently, we introduce a substitute, termed partial sRI-CC2, which applies sRI selectively to the exchange terms only. The partial sRI-CC2 shows a quartic scaling with a modest prefactor, rendering it a practical alternative. Compared to conventional RI-CC2, the partial sRI-CC2 can handle systems with hundreds or even thousands of electrons. This work is an extension to our previous implementation of sRI-CC2 method and provides essential ingredients for large-scale nonadiabatic dynamics. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06456">arXiv:2509.06456</a><span> [<a href="...2509.06456">pdf</a>, <a href="...2509.06456">ps</a>, <a href="...2509.06456">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> Cross3DReg: Towards a <span class="search-hit mathjax">Large</span>-scale Real-world Cross-source Point Cloud Registration Benchmark </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Xu%2C+Z">Zongyi Xu</a>, <a href="...?searchtype=author&amp;query=Lang%2C+Z">Zhongpeng Lang</a>, <a href="...?searchtype=author&amp;query=Chen%2C+Y">Yilong Chen</a>, <a href="...?searchtype=author&amp;query=Zhao%2C+S">Shanshan Zhao</a>, <a href="...?searchtype=author&amp;query=Huang%2C+X">Xiaoshui Huang</a>, <a href="...?searchtype=author&amp;query=Zuo%2C+Y">Yifan Zuo</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yan Zhang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Q">Qianni Zhang</a>, <a href="...?searchtype=author&amp;query=Gao%2C+X">Xinbo Gao</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06456v1-abstract-short"> Cross-source point cloud registration, which aims to align point cloud data from different sensors, is a fundamental task in 3D vision. However, compared to the same-source point cloud registration, cross-source registration faces two core challenges: the lack of publicly available large-scale real-world datasets for training the deep registration models, and the inherent differences in point clou… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06456v1-abstract-full"> Cross-source point cloud registration, which aims to align point cloud data from different sensors, is a fundamental task in 3D vision. However, compared to the same-source point cloud registration, cross-source registration faces two core challenges: the lack of publicly available large-scale real-world datasets for training the deep registration models, and the inherent differences in point clouds captured by multiple sensors. The diverse patterns induced by the sensors pose great challenges in robust and accurate point cloud feature extraction and matching, which negatively influence the registration accuracy. To advance research in this field, we construct Cross3DReg, the currently largest and real-world multi-modal cross-source point cloud registration dataset, which is collected by a rotating mechanical lidar and a hybrid semi-solid-state lidar, respectively. Moreover, we design an overlap-based cross-source registration framework, which utilizes unaligned images to predict the overlapping region between source and target point clouds, effectively filtering out redundant points in the irrelevant regions and significantly mitigating the interference caused by noise in non-overlapping areas. Then, a visual-geometric attention guided matching module is proposed to enhance the consistency of cross-source point cloud features by fusing image and geometric information to establish reliable correspondences and ultimately achieve accurate and robust registration. Extensive experiments show that our method achieves state-of-the-art registration performance. Our framework reduces the relative rotation error (RRE) and relative translation error (RTE) by <span class="MathJax_Preview" style="">63.2\%</span> and <span class="MathJax_Preview" style="">40.2\%</span>, respectively, and improves the registration recall (RR) by <span class="MathJax_Preview" style="">5.4\%</span>, which validates its effectiveness in achieving accurate cross-source registration. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06436">arXiv:2509.06436</a><span> [<a href="...2509.06436">pdf</a>, <a href="...2509.06436">ps</a>, <a href="...2509.06436">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Tree of Agents: Improving Long-Context Capabilities of <span class="search-hit mathjax">Large</span> Language Models through Multi-Perspective Reasoning </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Yu%2C+S">Song Yu</a>, <a href="...?searchtype=author&amp;query=Xu%2C+X">Xiaofei Xu</a>, <a href="...?searchtype=author&amp;query=Deng%2C+K">Ke Deng</a>, <a href="...?searchtype=author&amp;query=Li%2C+L">Li Li</a>, <a href="...?searchtype=author&amp;query=Tian%2C+L">Lin Tian</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06436v1-abstract-short"> Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limit… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06436v1-abstract-full"> Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">19 pages, 5 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06429">arXiv:2509.06429</a><span> [<a href="...2509.06429">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span></div></div><p class="title is-5 mathjax"> Analyzing the Instability of <span class="search-hit mathjax">Large</span> Language Models in Automated Bug Injection and Correction </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Er%2C+M+B">Mehmet Bilal Er</a>, <a href="...?searchtype=author&amp;query=%C4%B0lhan%2C+N">Nagehan İlhan</a>, <a href="...?searchtype=author&amp;query=Kuran%2C+U">Umut Kuran</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06429v1-abstract-short"> The use of Large Language Models (LLMs) in software engineering tasks is growing, especially in the areas of bug fixing and code generation. Nevertheless, these models often yield unstable results; when executed at different times with the same input, they can generate radically different code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly assessed, despite the fact that… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06429v1-abstract-full"> The use of Large Language Models (LLMs) in software engineering tasks is growing, especially in the areas of bug fixing and code generation. Nevertheless, these models often yield unstable results; when executed at different times with the same input, they can generate radically different code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly assessed, despite the fact that this instability has typically been discussed in the literature in relation to code generation. The purpose of this study is to look into how unstable an LLM like ChatGPT is when it comes to fixing code bugs. We examine the structural, syntactic, and functional variations among several fix recommendations made in response to the same prompt using code samples with various error types. Additionally, we assess how instability is affected by the temperature settings (0, 0.5, and 1) used for the model's deterministic operation. For a total of 20 problems in the experimental analysis, the model produced three fix suggestions at each temperature value, comparing nine distinct outputs for each problem. The Syntax Similarity and Output Equivalence Rate (OER) metrics were used to assess the outputs' structural and functional consistency. The results demonstrate that the model's outputs become much more unstable and variable as the temperature rises, with high temperatures showing especially high rates of functional failure. According to syntax similarity analyses, the suggested fixes show notable structural differences at high temperatures but are fairly similar at low temperatures. The purpose of this study is to provide important methodological insights into how LLM-based error correction systems can be applied more consistently in software development processes while also casting doubt on their dependability. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06400">arXiv:2509.06400</a><span> [<a href="...2509.06400">pdf</a>, <a href="...2509.06400">ps</a>, <a href="...2509.06400">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> 3DOF+Quantization: 3DGS quantization for <span class="search-hit mathjax">large</span> scenes with limited Degrees of Freedom </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Gendrin%2C+M">Matthieu Gendrin</a>, <a href="...?searchtype=author&amp;query=Pateux%2C+S">Stéphane Pateux</a>, <a href="...?searchtype=author&amp;query=Ladune%2C+T">Théo Ladune</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06400v1-abstract-short"> 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06400v1-abstract-full"> 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of camera (3 other degrees). On large scenes, though, the input views are acquired from a limited zone in space, and the reconstruction is valuable for novel views from the same zone, even if the scene itself is almost unlimited in size. We refer to this particular case as 3DoF+, meaning that the 3 degrees of freedom of camera position are limited to small offsets around the central position. Considering the problem of coordinate quantization, the impact of position error on the projection error in pixels is studied. It is shown that the projection error is proportional to the squared inverse distance of the point being projected. Consequently, a new quantization scheme based on spherical coordinates is proposed. Rate-distortion performance of the proposed method are illustrated on the well-known Garden scene. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> CORESA - COmpression et REprésentation des Signaux Audiovisuels, Institut National des Sciences Appliquées - Rennes [INSA Rennes], Nov 2024, Rennes, France </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06337">arXiv:2509.06337</a><span> [<a href="...2509.06337">pdf</a>, <a href="...2509.06337">ps</a>, <a href="...2509.06337">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span> Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhao%2C+J">Jianpeng Zhao</a>, <a href="...?searchtype=author&amp;query=Yuan%2C+C">Chenyu Yuan</a>, <a href="...?searchtype=author&amp;query=Luo%2C+W">Weiming Luo</a>, <a href="...?searchtype=author&amp;query=Xie%2C+H">Haoling Xie</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+G">Guangwei Zhang</a>, <a href="...?searchtype=author&amp;query=Quan%2C+S+J">Steven Jige Quan</a>, <a href="...?searchtype=author&amp;query=Yuan%2C+Z">Zixuan Yuan</a>, <a href="...?searchtype=author&amp;query=Wang%2C+P">Pengyang Wang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+D">Denghui Zhang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06337v1-abstract-short"> Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribut… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06337v1-abstract-full"> Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), to systematically evaluate the ability of LLMs to generate accurate and demographically coherent responses. In PAS, the model predicts missing attributes based on partial respondent profiles, whereas FAS involves generating complete synthetic datasets under both zero-context and context-enhanced conditions. We curate a comprehensive benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey Simulation), that spans 11 real-world public datasets across four sociological domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B) reveals consistent trends in prediction performance, highlights failure modes, and demonstrates how context and prompt design impact simulation fidelity. This work establishes a rigorous foundation for LLM-driven survey simulations, offering scalable and cost-effective tools for sociological research and policy evaluation. Our code and dataset are available at: https://github.com/dart-lab-research/LLM-S-Cube-Benchmark <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06324">arXiv:2509.06324</a><span> [<a href="...2509.06324">pdf</a>, <a href="...2509.06324">ps</a>, <a href="...2509.06324">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span></div></div><p class="title is-5 mathjax"> A Generic and Efficient Python Runtime Verification System and its <span class="search-hit mathjax">Large</span>-scale Evaluation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Shen%2C+Z">Zhuohang Shen</a>, <a href="...?searchtype=author&amp;query=Yaseen%2C+M">Mohammed Yaseen</a>, <a href="...?searchtype=author&amp;query=Silva%2C+D">Denini Silva</a>, <a href="...?searchtype=author&amp;query=Guan%2C+K">Kevin Guan</a>, <a href="...?searchtype=author&amp;query=Lee%2C+J">Junho Lee</a>, <a href="...?searchtype=author&amp;query=d%27Amorim%2C+M">Marcelo d'Amorim</a>, <a href="...?searchtype=author&amp;query=Legunsen%2C+O">Owolabi Legunsen</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06324v1-abstract-short"> Runtime verification (RV) now scales for testing thousands of open-source Java projects, helping find hundreds of bugs. The popular Python ecosystem could use such benefits. But, today's Python RV systems are limited to a domain or specification logic, or slow. We propose PyMOP, a generic, extensible, and efficient RV system for Python. PyMOP supports five logics, implements five existing monitori… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06324v1-abstract-full"> Runtime verification (RV) now scales for testing thousands of open-source Java projects, helping find hundreds of bugs. The popular Python ecosystem could use such benefits. But, today's Python RV systems are limited to a domain or specification logic, or slow. We propose PyMOP, a generic, extensible, and efficient RV system for Python. PyMOP supports five logics, implements five existing monitoring algorithms, ships with 73 API specs of Python and widely-used libraries, supports three instrumentation strategies, and users can easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we find mainly that (i) the default monitoring algorithm for Java is often not the fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were fixed by developers. PyMOP's generality and efficiency position it well as an excellent platform for the next advances on RV for Python. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">23 pages, 7 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06307">arXiv:2509.06307</a><span> [<a href="...2509.06307">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Can AI Make Energy Retrofit Decisions? An Evaluation of <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Shu%2C+L">Lei Shu</a>, <a href="...?searchtype=author&amp;query=Zhao%2C+D">Dong Zhao</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06307v1-abstract-short"> Conventional approaches to building energy retrofit decision making suffer from limited generalizability and low interpretability, hindering adoption in diverse residential contexts. With the growth of Smart and Connected Communities, generative AI, especially large language models (LLMs), may help by processing contextual information and producing practitioner readable recommendations. We evaluat… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06307v1-abstract-full"> Conventional approaches to building energy retrofit decision making suffer from limited generalizability and low interpretability, hindering adoption in diverse residential contexts. With the growth of Smart and Connected Communities, generative AI, especially large language models (LLMs), may help by processing contextual information and producing practitioner readable recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok, Llama, and Claude) on residential retrofit decisions under two objectives: maximizing CO2 reduction (technical) and minimizing payback period (sociotechnical). Performance is assessed on four dimensions: accuracy, consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49 US states. LLMs generate effective recommendations in many cases, reaching up to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning. Performance is stronger for the technical objective, while sociotechnical decisions are limited by economic trade offs and local context. Agreement across models is low, and higher performing models tend to diverge from others. LLMs are sensitive to location and building geometry but less sensitive to technology and occupant behavior. Most models show step by step, engineering style reasoning, but it is often simplified and lacks deeper contextual awareness. Overall, LLMs are promising assistants for energy retrofit decision making, but improvements in accuracy, consistency, and context handling are needed for reliable practice. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06295">arXiv:2509.06295</a><span> [<a href="...2509.06295">pdf</a>, <a href="...2509.06295">ps</a>, <a href="...2509.06295">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Econometrics">econ.EM</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation">stat.CO</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span></div></div><p class="title is-5 mathjax"> Largevars: An R Package for Testing <span class="search-hit mathjax">Large</span> VARs for the Presence of Cointegration </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Bykhovskaya%2C+A">Anna Bykhovskaya</a>, <a href="...?searchtype=author&amp;query=Gorin%2C+V">Vadim Gorin</a>, <a href="...?searchtype=author&amp;query=Kiss%2C+E">Eszter Kiss</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06295v1-abstract-short"> Cointegration is a property of multivariate time series that determines whether its non-stationary, growing components have a stationary linear combination. Largevars R package conducts a cointegration test for high-dimensional vector autoregressions of order k based on the large N, T asymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a modification of the Johansen likeliho… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06295v1-abstract-full"> Cointegration is a property of multivariate time series that determines whether its non-stationary, growing components have a stationary linear combination. Largevars R package conducts a cointegration test for high-dimensional vector autoregressions of order k based on the large N, T asymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a modification of the Johansen likelihood ratio test. In the absence of cointegration the test converges to the partial sum of the Airy_1 point process, an object arising in random matrix theory. The package and this article contain simulated quantiles of the first ten partial sums of the Airy_1 point process that are precise up to the first 3 digits. We also include two examples using Largevars: an empirical example on S&amp;P100 stocks and a simulated VAR(2) example. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">21 pages</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06293">arXiv:2509.06293</a><span> [<a href="...2509.06293">pdf</a>, <a href="...2509.06293">ps</a>, <a href="...2509.06293">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Analysis of PDEs">math.AP</span></div></div><p class="title is-5 mathjax"> Global well-posedness and <span class="search-hit mathjax">large</span> time behavior of 3D incompressible inhomogeneous magnetohydrodynamic equations in the exterior of a cylinder </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Liu%2C+J">Jitao Liu</a>, <a href="...?searchtype=author&amp;query=Liu%2C+M">Min Liu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06293v1-abstract-short"> When the vaccum is allowed, if the global existence and uniqueness of strong solutions to three dimensional incompressible inhomogeneous magnetohydrodynamic equations holds true or not has always been a challenging open problem, even for the magnetofluids with special structures. In this paper, through deeply exploring the internal structure and characteristic of axisymmetric flows, we obtain some… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06293v1-abstract-full"> When the vaccum is allowed, if the global existence and uniqueness of strong solutions to three dimensional incompressible inhomogeneous magnetohydrodynamic equations holds true or not has always been a challenging open problem, even for the magnetofluids with special structures. In this paper, through deeply exploring the internal structure and characteristic of axisymmetric flows, we obtain some new discoveries and give a partial answer to above issue. More precisely, we prove that the axisymmetric magnetofluids flowing in the exterior of a cylinder will definitely admits a unique strong solution that exists globally in time without any compatibility conditions and small assumptions imposed on the initial data. Furthermore, we establish the algebraic decay rates for the time and spatial derivatives of both velocity and magnetic fields. To the best of our knowledge, this result gives the first unique 3D large solution existing globally in time. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">34 pages</span></p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span> 35B07; 35B40; 76D03; 76D05; 76W05 </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06240">arXiv:2509.06240</a><span> [<a href="...2509.06240">pdf</a>, <a href="...2509.06240">ps</a>, <a href="...2509.06240">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Nuclear Experiment">nucl-ex</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="High Energy Physics - Phenomenology">hep-ph</span></div></div><p class="title is-5 mathjax"> Quarkonia collectivity in <span class="search-hit mathjax">large</span> collision systems with ALICE </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhang%2C+C">Chi Zhang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06240v1-abstract-short"> Quarkonium production is one of the golden probes to study the quark--gluon plasma (QGP). Among many observables, the measurement of azimuthal anisotropies in quarkonium production sheds light on the collective behavior of heavy-flavor particles in a strongly interacting medium. In particular, the measurements of the elliptic flow (<span class="MathJax_Preview" style="">v_{2}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-4-Frame" tabindex="0"></span>) of quarkonia in Pb--Pb collisions at the LHC provide us d… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06240v1-abstract-full"> Quarkonium production is one of the golden probes to study the quark--gluon plasma (QGP). Among many observables, the measurement of azimuthal anisotropies in quarkonium production sheds light on the collective behavior of heavy-flavor particles in a strongly interacting medium. In particular, the measurements of the elliptic flow (<span class="MathJax_Preview" style="">v_{2}</span>) of quarkonia in Pb--Pb collisions at the LHC provide us direct evidence of heavy quark thermalization in the QGP. In these proceedings, new results of inclusive <span class="MathJax_Preview" style="">\mathrm{J/ψ}</span> elliptic flow measurement in Pb--Pb collisions carried out by the ALICE collaboration in Run 3 using three methods including event-plane, scalar-product and multi-particle correlation (cumulant) will be presented. The method of cumulant will give access to the <span class="MathJax_Preview" style="">\mathrm{J/ψ}</span> flow fluctuations at forward rapidity. Alongside the new flow measurements of <span class="MathJax_Preview" style="">\mathrm{J/ψ}</span>, new results of <span class="MathJax_Preview" style="">\mathrmΥ(1S)</span> flow measurement at forward rapidity in ALICE Run 3 will be presented as well with comparison to model calculations. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">4 pages, 4 figures. Contribution to proceedings of Quark Matter 2025 (6-12 April 2025, Frankfurt, Germany)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06185">arXiv:2509.06185</a><span> [<a href="...2509.06185">pdf</a>, <a href="...2509.06185">ps</a>, <a href="...2509.06185">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"> Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily <span class="search-hit mathjax">large</span> product catalogs </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Jarboui%2C+F">Firas Jarboui</a>, <a href="...?searchtype=author&amp;query=Memari%2C+I">Issa Memari</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06185v1-abstract-short"> Conversational recommender systems promise rich interactions for e-commerce, but balancing exploration (clarifying user needs) and exploitation (making recommendations) remains challenging, especially when deploying large language models (LLMs) with vast product catalogs. We address this challenge by modeling the breadth of user interest via the entropy of retrieval score distributions. Our method… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06185v1-abstract-full"> Conversational recommender systems promise rich interactions for e-commerce, but balancing exploration (clarifying user needs) and exploitation (making recommendations) remains challenging, especially when deploying large language models (LLMs) with vast product catalogs. We address this challenge by modeling the breadth of user interest via the entropy of retrieval score distributions. Our method uses a neural retriever to fetch relevant items for a user query and computes the entropy of the re-ranked scores to dynamically route the dialogue policy: low-entropy (specific) queries trigger direct recommendations, whereas high-entropy (ambiguous) queries prompt exploratory questions. This simple yet effective strategy allows an LLM-driven agent to remain aware of an arbitrarily large catalog in real-time without bloating its context window. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06164">arXiv:2509.06164</a><span> [<a href="...2509.06164">pdf</a>, <a href="...2509.06164">ps</a>, <a href="...2509.06164">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"> Benchmarking Gender and Political Bias in <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Yang%2C+J">Jinrui Yang</a>, <a href="...?searchtype=author&amp;query=Han%2C+X">Xudong Han</a>, <a href="...?searchtype=author&amp;query=Baldwin%2C+T">Timothy Baldwin</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06164v1-abstract-short"> We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tas… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06164v1-abstract-full"> We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tasks -- gender classification and vote prediction -- revealing consistent patterns of bias. We find that LLMs frequently misclassify female MEPs as male and demonstrate reduced accuracy when simulating votes for female speakers. Politically, LLMs tend to favor centrist groups while underperforming on both far-left and far-right ones. Proprietary models like GPT-4o outperform open-weight alternatives in terms of both robustness and fairness. We release the EuroParlVote dataset, code, and demo to support future research on fairness and accountability in NLP within political contexts. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">The 8th International Conference on Natural Language and Speech Processing (Oral)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06100">arXiv:2509.06100</a><span> [<a href="...2509.06100">pdf</a>, <a href="...2509.06100">ps</a>, <a href="...2509.06100">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Cao%2C+K">Kefan Cao</a>, <a href="...?searchtype=author&amp;query=Wu%2C+S">Shuaicheng Wu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06100v1-abstract-short"> Large language models (LLMs) are prone to catastrophic forgetting in sequential multi-task settings. Parameter regularization methods such as O-LoRA and N-LoRA alleviate task interference by enforcing low-rank subspace orthogonality, but they overlook the fact that conventional additive fine-tuning disrupts the intrinsic geometric structure of LLM parameters, limiting performance. Our key insight… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06100v1-abstract-full"> Large language models (LLMs) are prone to catastrophic forgetting in sequential multi-task settings. Parameter regularization methods such as O-LoRA and N-LoRA alleviate task interference by enforcing low-rank subspace orthogonality, but they overlook the fact that conventional additive fine-tuning disrupts the intrinsic geometric structure of LLM parameters, limiting performance. Our key insight is that the parameter space of LLMs possesses a geometric structure, which must be preserved in addition to enforcing orthogonality. Based on this, we propose Orthogonal Low-rank Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM fine-tuning: leveraging multiplicative updates to preserve parameter geometry while applying orthogonality constraints to task subspaces. Experiments demonstrate that OLieRA achieves state-of-the-art results on the Standard CL benchmark and remains among the top-performing methods in the Large Number of Tasks setting. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">13 pages, 3 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06093">arXiv:2509.06093</a><span> [<a href="...2509.06093">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Language Native Lightly Structured Databases for <span class="search-hit mathjax">Large</span> Language Model Driven Composite Materials Research </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Liu%2C+Y">Yuze Liu</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Z">Zhaoyuan Zhang</a>, <a href="...?searchtype=author&amp;query=Zeng%2C+X">Xiangsheng Zeng</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yihe Zhang</a>, <a href="...?searchtype=author&amp;query=Yu%2C+L">Leping Yu</a>, <a href="...?searchtype=author&amp;query=Wang%2C+L">Lejia Wang</a>, <a href="...?searchtype=author&amp;query=Yu%2C+X">Xi Yu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06093v1-abstract-short"> Chemical and materials research has traditionally relied heavily on knowledge narrative, with progress often driven by language-based descriptions of principles, mechanisms, and experimental experiences, rather than tables, limiting what conventional databases and ML can exploit. We present a language-native database for boron nitride nanosheet (BNNS) polymer thermally conductive composites that c… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06093v1-abstract-full"> Chemical and materials research has traditionally relied heavily on knowledge narrative, with progress often driven by language-based descriptions of principles, mechanisms, and experimental experiences, rather than tables, limiting what conventional databases and ML can exploit. We present a language-native database for boron nitride nanosheet (BNNS) polymer thermally conductive composites that captures lightly structured information from papers across preparation, characterization, theory-computation, and mechanistic reasoning, with evidence-linked snippets. Records are organized in a heterogeneous database and queried via composite retrieval with semantics, key words and value filters. The system can synthesizes literature into accurate, verifiable, and expert style guidance. This substrate enables high fidelity efficient Retrieval Augmented Generation (RAG) and tool augmented agents to interleave retrieval with reasoning and deliver actionable SOP. The framework supplies the language rich foundation required for LLM-driven materials discovery. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06078">arXiv:2509.06078</a><span> [<a href="...2509.06078">pdf</a>, <a href="...2509.06078">ps</a>, <a href="...2509.06078">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Analysis of PDEs">math.AP</span></div></div><p class="title is-5 mathjax"> Global strong solutions to the <span class="MathJax_Preview" style="">3</span><span class="MathJax MathJax_Processing" id="MathJax-Element-10-Frame" tabindex="0"></span>D rotating compressible Navier--Stokes--Korteweg system for <span class="search-hit mathjax">large</span> data in the critical <span class="MathJax_Preview" style="">\widehat{L^p}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-11-Frame" tabindex="0"></span> framework </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Fujii%2C+M">Mikihiro Fujii</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+S">Shunhang Zhang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06078v1-abstract-short"> Let us consider the <span class="MathJax_Preview" style="">3</span><span class="MathJax MathJax_Processing" id="MathJax-Element-12-Frame" tabindex="0"></span>D compressible Navier--Stokes--Korteweg system in the rotational framework. Although there is a wealth of literature on the weak solutions to this system, there seem to be no results on the strong solutions. In this paper, we show the unique existence of global solutions for {\it large} initial data in the critical Besov-type spaces based on the Fourier--Lebesgue spaces… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06078v1-abstract-full"> Let us consider the <span class="MathJax_Preview" style="">3</span>D compressible Navier--Stokes--Korteweg system in the rotational framework. Although there is a wealth of literature on the weak solutions to this system, there seem to be no results on the strong solutions. In this paper, we show the unique existence of global solutions for {\it large} initial data in the critical Besov-type spaces based on the Fourier--Lebesgue spaces <span class="MathJax_Preview" style="">\widehat{L^p}(\mathbb{R}^3)</span> with <span class="MathJax_Preview" style="">2 \leq p &lt; 3</span>, provided that the rotation speed and the Mach number are sufficiently large and small, respectively. The key ingredient of the proof is to establish the Strichartz-type estimates due to the dispersion caused by the mixture of the rotation and acoustic waves in the Fourier--Lebesgue spaces, and focus on the better structure of dissipation from the Korteweg term and the nonlinear terms of the divergence form in the momentum formulation. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06065">arXiv:2509.06065</a><span> [<a href="...2509.06065">pdf</a>, <a href="...2509.06065">ps</a>, <a href="...2509.06065">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> KatotohananQA: Evaluating Truthfulness of <span class="search-hit mathjax">Large</span> Language Models in Filipino </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nery%2C+L+A">Lorenzo Alfred Nery</a>, <a href="...?searchtype=author&amp;query=Catignas%2C+R+D">Ronald Dawson Catignas</a>, <a href="...?searchtype=author&amp;query=Tiam-Lee%2C+T+J">Thomas James Tiam-Lee</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06065v1-abstract-short"> Large Language Models (LLMs) achieve remarkable performance across various tasks, but their tendency to produce hallucinations limits reliable adoption. Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet they are primarily available in English, leaving a gap in evaluating LLMs in low-resource languages. To address this, we present KatotohananQA, a Filipino translation o… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06065v1-abstract-full"> Large Language Models (LLMs) achieve remarkable performance across various tasks, but their tendency to produce hallucinations limits reliable adoption. Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet they are primarily available in English, leaving a gap in evaluating LLMs in low-resource languages. To address this, we present KatotohananQA, a Filipino translation of the TruthfulQA benchmark. Seven free-tier proprietary models were assessed using a binary-choice framework. Findings show a significant performance gap between English and Filipino truthfulness, with newer OpenAI models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness. Results also reveal disparities across question characteristics, suggesting that some question types, categories, and topics are less robust to multilingual transfer which highlight the need for broader multilingual evaluation to ensure fairness and reliability in LLM usage. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">14 pages, 1 figure, 9 tables, 1 listing. To appear in Proceedings of NLPIR 2025</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06052">arXiv:2509.06052</a><span> [<a href="...2509.06052">pdf</a>, <a href="...2509.06052">ps</a>, <a href="...2509.06052">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span></div></div><p class="title is-5 mathjax"> Empirical Study of Code <span class="search-hit mathjax">Large</span> Language Models for Binary Security Patch Detection </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Li%2C+Q">Qingyuan Li</a>, <a href="...?searchtype=author&amp;query=Li%2C+B">Binchang Li</a>, <a href="...?searchtype=author&amp;query=Gao%2C+C">Cuiyun Gao</a>, <a href="...?searchtype=author&amp;query=Gao%2C+S">Shuzheng Gao</a>, <a href="...?searchtype=author&amp;query=Li%2C+Z">Zongjie Li</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06052v1-abstract-short"> Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of re… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06052v1-abstract-full"> Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of real-world software, as they release patches only with binary files, and the source code is inaccessible. Given the impressive performance of code large language models (LLMs) in code intelligence and binary analysis tasks such as decompilation and compilation optimization, their potential for detecting binary security patches remains unexplored, exposing a significant research gap between their demonstrated low-level code understanding capabilities and this critical security task. To address this gap, we construct a large-scale binary patch dataset containing \textbf{19,448} samples, with two levels of representation: assembly code and pseudo-code, and systematically evaluate \textbf{19} code LLMs of varying scales to investigate their capability in binary SPD tasks. Our initial exploration demonstrates that directly prompting vanilla code LLMs struggles to accurately identify security patches from binary patches, and even state-of-the-art prompting techniques fail to mitigate the lack of domain knowledge in binary SPD within vanilla models. Drawing on the initial findings, we further investigate the fine-tuning strategy for injecting binary SPD domain knowledge into code LLMs through two levels of representation. Experimental results demonstrate that fine-tuned LLMs achieve outstanding performance, with the best results obtained on the pseudo-code representation. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06024">arXiv:2509.06024</a><span> [<a href="...2509.06024">pdf</a>, <a href="...2509.06024">ps</a>, <a href="...2509.06024">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Rethinking Reasoning Quality in <span class="search-hit mathjax">Large</span> Language Models through Enhanced Chain-of-Thought via RL </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=He%2C+H">Haoyang He</a>, <a href="...?searchtype=author&amp;query=Rong%2C+Z">Zihua Rong</a>, <a href="...?searchtype=author&amp;query=Ji%2C+K">Kun Ji</a>, <a href="...?searchtype=author&amp;query=Li%2C+C">Chenyang Li</a>, <a href="...?searchtype=author&amp;query=Huang%2C+Q">Qing Huang</a>, <a href="...?searchtype=author&amp;query=Xia%2C+C">Chong Xia</a>, <a href="...?searchtype=author&amp;query=Yang%2C+L">Lan Yang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+H">Honggang Zhang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06024v1-abstract-short"> Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such tas… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06024v1-abstract-full"> Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such task-specific training offers limited control over logical depth and therefore may fail to reveal a model's genuine reasoning capacity. We propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward framework that reshapes both reward and advantage signals. (i) A Reasoning Quality Reward assigns fine-grained credit to those reasoning chains that demonstrably raise the likelihood of the correct answer, directly incentivising the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage decays the advantage of responses whose length deviates from a validation-derived threshold, stabilising training. To facilitate rigorous assessment, we also release Logictree, a dynamically constructed deductive reasoning dataset that functions both as RL training data and as a comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B model attains GPT-o3-mini level performance on Logictree with 400 trianing steps, while the average confidence of CoT-augmented answers rises by 30%. The model further exhibits generalisation across diverse logical-reasoning datasets, and the mathematical benchmark AIME24. These results illuminate how RL shapes CoT behaviour and chart a practical path toward enhancing formal-reasoning skills in large language models. All code and data are available in repository https://github.com/Henryhe09/DRER. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05948">arXiv:2509.05948</a><span> [<a href="...2509.05948">pdf</a>, <a href="...2509.05948">ps</a>, <a href="...2509.05948">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Instrumentation and Detectors">physics.ins-det</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Nuclear Experiment">nucl-ex</span></div></div><p class="title is-5 mathjax"> Upgrades of the ATLAS Zero Degree Calorimeter System for Run 3 at the <span class="search-hit mathjax">Large</span> Hadron Collider </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Avoni%2C+G">Giulio Avoni</a>, <a href="...?searchtype=author&amp;query=Bruschi%2C+M">Marco Bruschi</a>, <a href="...?searchtype=author&amp;query=Canale%2C+G">Gianluca Canale</a>, <a href="...?searchtype=author&amp;query=Citron%2C+Z">Zvi Citron</a>, <a href="...?searchtype=author&amp;query=Cole%2C+B">Brian Cole</a>, <a href="...?searchtype=author&amp;query=Dahle%2C+E">Eitan Dahle</a>, <a href="...?searchtype=author&amp;query=Dziedzic%2C+B">Bartosz Dziedzic</a>, <a href="...?searchtype=author&amp;query=Glik%2C+B">Bar Glik</a>, <a href="...?searchtype=author&amp;query=Grosse-Perdekamp%2C+M">Matthias Grosse-Perdekamp</a>, <a href="...?searchtype=author&amp;query=Guo%2C+Y">Yhan Guo</a>, <a href="...?searchtype=author&amp;query=Korcy%2C+K">Krzysztof Korcy</a>, <a href="...?searchtype=author&amp;query=Hoppesch%2C+M">Matthew Hoppesch</a>, <a href="...?searchtype=author&amp;query=Housenga%2C+M">Mason Housenga</a>, <a href="...?searchtype=author&amp;query=Lantz%2C+C">Chad Lantz</a>, <a href="...?searchtype=author&amp;query=Liu%2C+Y">Yi Liu</a>, <a href="...?searchtype=author&amp;query=Longo%2C+R">Riccardo Longo</a>, <a href="...?searchtype=author&amp;query=Lund%2C+S">Samantha Lund</a>, <a href="...?searchtype=author&amp;query=MacLean%2C+D">Daniel MacLean</a>, <a href="...?searchtype=author&amp;query=Meneghini%2C+S">Stefano Meneghini</a>, <a href="...?searchtype=author&amp;query=Milovanovic%2C+M">Marco Milovanovic</a>, <a href="...?searchtype=author&amp;query=Mladenovic%2C+G">Goran Mladenovic</a>, <a href="...?searchtype=author&amp;query=Mohapatra%2C+S">Soumya Mohapatra</a>, <a href="...?searchtype=author&amp;query=Rafee%2C+F+M">Farah Mohammed Rafee</a>, <a href="...?searchtype=author&amp;query=Moyal%2C+Y">Yftach Moyal</a>, <a href="...?searchtype=author&amp;query=Sbarra%2C+C">Carla Sbarra</a> , et al. (7 additional authors not shown) </p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05948v1-abstract-short"> Experimental studies of ultra-relativistic heavy ion collisions at the Large Hadron Collider (LHC) depend crucially on Zero Degree Calorimeters (ZDCs) that measure neutrons produced at near-beam rapidity in nucleus-nucleus collisions. In hadronic nuclear collisions these neutrons are mainly spectator neutrons, those that do not scatter from opposing nucleons during the collision. As a result, the… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05948v1-abstract-full"> Experimental studies of ultra-relativistic heavy ion collisions at the Large Hadron Collider (LHC) depend crucially on Zero Degree Calorimeters (ZDCs) that measure neutrons produced at near-beam rapidity in nucleus-nucleus collisions. In hadronic nuclear collisions these neutrons are mainly spectator neutrons, those that do not scatter from opposing nucleons during the collision. As a result, the ZDCs provide a vital probe of heavy ion collision geometry. The ZDCs are also essential in the study of ultra-peripheral collisions that are initiated by photons associated with the electric fields of one or both nuclei. Coherent photon emission typically leaves the photon emitter intact, making the observation of no ZDC signal, on one or both sides, a tag of such processes. The ATLAS ZDCs, built prior to Run 1 were substantially upgraded for LHC Run 3. The primary upgrades included replacement of the quartz Cherenkov radiator with <span class="MathJax_Preview" style="">\text{H}_2</span>-doped fused silica rods; installation of fast air-core signal cables between the ZDC and the ATLAS USA15 cavern; new LED-based calibration system; and new electronics implemented for readout and fully-digital triggering. The ZDCs were also augmented with new "Reaction Plane Detectors" (RPDs) designed to measure the transverse centroid of multi-neutron showers to allow event-by-event reconstruction of the directed-flow plane in nuclear collisions. The Run~3 ZDC detectors, including the RPDs, are described in detail with emphasis on aspects that are new for Run~3. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">To be submitted to JINST</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05946">arXiv:2509.05946</a><span> [<a href="...2509.05946">pdf</a>, <a href="...2509.05946">ps</a>, <a href="...2509.05946">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span> Language Models for Next-Generation Wireless Network Management: A Survey and Tutorial </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Wei%2C+B">Bisheng Wei</a>, <a href="...?searchtype=author&amp;query=Jiang%2C+R">Ruihong Jiang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+R">Ruichen Zhang</a>, <a href="...?searchtype=author&amp;query=Liu%2C+Y">Yinqiu Liu</a>, <a href="...?searchtype=author&amp;query=Niyato%2C+D">Dusit Niyato</a>, <a href="...?searchtype=author&amp;query=Sun%2C+Y">Yaohua Sun</a>, <a href="...?searchtype=author&amp;query=Lu%2C+Y">Yang Lu</a>, <a href="...?searchtype=author&amp;query=Li%2C+Y">Yonghui Li</a>, <a href="...?searchtype=author&amp;query=Mao%2C+S">Shiwen Mao</a>, <a href="...?searchtype=author&amp;query=Yuen%2C+C">Chau Yuen</a>, <a href="...?searchtype=author&amp;query=Di+Renzo%2C+M">Marco Di Renzo</a>, <a href="...?searchtype=author&amp;query=Peng%2C+M">Mugen Peng</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05946v1-abstract-short"> The rapid advancement toward sixth-generation (6G) wireless networks has significantly intensified the complexity and scale of optimization problems, including resource allocation and trajectory design, often formulated as combinatorial problems in large discrete decision spaces. However, traditional optimization methods, such as heuristics and deep reinforcement learning (DRL), struggle to meet t… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05946v1-abstract-full"> The rapid advancement toward sixth-generation (6G) wireless networks has significantly intensified the complexity and scale of optimization problems, including resource allocation and trajectory design, often formulated as combinatorial problems in large discrete decision spaces. However, traditional optimization methods, such as heuristics and deep reinforcement learning (DRL), struggle to meet the demanding requirements of real-time adaptability, scalability, and dynamic handling of user intents in increasingly heterogeneous and resource-constrained network environments. Large language models (LLMs) present a transformative paradigm by enabling natural language-driven problem formulation, context-aware reasoning, and adaptive solution refinement through advanced semantic understanding and structured reasoning capabilities. This paper provides a systematic and comprehensive survey of LLM-enabled optimization frameworks tailored for wireless networks. We first introduce foundational design concepts and distinguish LLM-enabled methods from conventional optimization paradigms. Subsequently, we critically analyze key enabling methodologies, including natural language modeling, solver collaboration, and solution verification processes. Moreover, we explore representative case studies to demonstrate LLMs' transformative potential in practical scenarios such as optimization formulation, low-altitude economy networking, and intent networking. Finally, we discuss current research challenges, examine prominent open-source frameworks and datasets, and identify promising future directions to facilitate robust, scalable, and trustworthy LLM-enabled optimization solutions for next-generation wireless networks. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05937">arXiv:2509.05937</a><span> [<a href="...2509.05937">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span></div></div><p class="title is-5 mathjax"> Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in <span class="search-hit mathjax">Large</span>-Scale Systems </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Huang%2C+W">Wei-Hsing Huang</a>, <a href="...?searchtype=author&amp;query=Jia%2C+J">Jianwei Jia</a>, <a href="...?searchtype=author&amp;query=Kong%2C+Y">Yuyao Kong</a>, <a href="...?searchtype=author&amp;query=Waqar%2C+F">Faaiq Waqar</a>, <a href="...?searchtype=author&amp;query=Wen%2C+T">Tai-Hao Wen</a>, <a href="...?searchtype=author&amp;query=Chang%2C+M">Meng-Fan Chang</a>, <a href="...?searchtype=author&amp;query=Yu%2C+S">Shimeng Yu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05937v1-abstract-short"> Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an innovative architectural paradigm capable of replicating conventional deep neural network (DNN) capabilities while utilizing significantly reduced parameter counts through the employment of parameterized B-spline functions with trainable coefficients. Nevertheless, the B-spline functional components inherent to KAN architectu… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05937v1-abstract-full"> Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an innovative architectural paradigm capable of replicating conventional deep neural network (DNN) capabilities while utilizing significantly reduced parameter counts through the employment of parameterized B-spline functions with trainable coefficients. Nevertheless, the B-spline functional components inherent to KAN architectures introduce distinct hardware acceleration complexities. While B-spline function evaluation can be accomplished through look-up table (LUT) implementations that directly encode functional mappings, thus minimizing computational overhead, such approaches continue to demand considerable circuit infrastructure, including LUTs, multiplexers, decoders, and related components. This work presents an algorithm-hardware co-design approach for KAN acceleration. At the algorithmic level, techniques include Alignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity aware mapping strategy, and circuit-level techniques include N:1 Time Modulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM) circuits. This work conducts evaluations on large-scale KAN networks to validate the proposed methodologies. Non-ideality factors, including partial sum deviations from process variations, have been evaluated with statistics measured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally determined KAN hyperparameters in conjunction with circuit optimizations fabricated at the 22nm technology node, despite the parameter count for large-scale tasks in this work increasing by 500Kx to 807Kx compared to tiny-scale tasks in previous work, the area overhead increases by only 28Kx to 41Kx, with power consumption rising by merely 51x to 94x, while accuracy degradation remains minimal at 0.11% to 0.23%, demonstrating the scaling potential of our proposed architecture. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05921">arXiv:2509.05921</a><span> [<a href="...2509.05921">pdf</a>, <a href="...2509.05921">ps</a>, <a href="...2509.05921">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span></div></div><p class="title is-5 mathjax"> Dataset Ownership in the Era of <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Li%2C+K">Kun Li</a>, <a href="...?searchtype=author&amp;query=Wang%2C+C">Cheng Wang</a>, <a href="...?searchtype=author&amp;query=Xu%2C+M">Minghui Xu</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yue Zhang</a>, <a href="...?searchtype=author&amp;query=Cheng%2C+X">Xiuzhen Cheng</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05921v1-abstract-short"> As datasets become critical assets in modern machine learning systems, ensuring robust copyright protection has emerged as an urgent challenge. Traditional legal mechanisms often fail to address the technical complexities of digital data replication and unauthorized use, particularly in opaque or decentralized environments. This survey provides a comprehensive review of technical approaches for da… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05921v1-abstract-full"> As datasets become critical assets in modern machine learning systems, ensuring robust copyright protection has emerged as an urgent challenge. Traditional legal mechanisms often fail to address the technical complexities of digital data replication and unauthorized use, particularly in opaque or decentralized environments. This survey provides a comprehensive review of technical approaches for dataset copyright protection, systematically categorizing them into three main classes: non-intrusive methods, which detect unauthorized use without modifying data; minimally-intrusive methods, which embed lightweight, reversible changes to enable ownership verification; and maximally-intrusive methods, which apply aggressive data alterations, such as reversible adversarial examples, to enforce usage restrictions. We synthesize key techniques, analyze their strengths and limitations, and highlight open research challenges. This work offers an organized perspective on the current landscape and suggests future directions for developing unified, scalable, and ethically sound solutions to protect datasets in increasingly complex machine learning ecosystems. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">15 pages, 1 table, accepted by the 2025 International Conference on Blockchain and Web3.0 Technology Innovation and Application Exchange (BWTAC)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05915">arXiv:2509.05915</a><span> [<a href="...2509.05915">pdf</a>, <a href="...2509.05915">ps</a>, <a href="...2509.05915">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Accelerating <span class="search-hit mathjax">Large</span> Language Model Inference via Early-Exiting Algorithms </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Bae%2C+S">Sangmin Bae</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05915v1-abstract-short"> Large language models have achieved remarkable capabilities, but their practical deployment is hindered by significant computational costs. While adaptive computation methods like early-exiting promise to reduce these costs, they introduce a fundamental conflict: the per-token dynamism intended to save computation often creates system-level bottlenecks that can paradoxically reduce throughput in b… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05915v1-abstract-full"> Large language models have achieved remarkable capabilities, but their practical deployment is hindered by significant computational costs. While adaptive computation methods like early-exiting promise to reduce these costs, they introduce a fundamental conflict: the per-token dynamism intended to save computation often creates system-level bottlenecks that can paradoxically reduce throughput in batched inference. This dissertation resolves this conflict by co-designing adaptive algorithms and model architectures to strike an optimal balance between dynamism and efficiency. To this end, our work first addresses critical sources of overhead in conventional early-exiting by proposing an efficient parallel decoding mechanism. We then show that deep parameter sharing provides an architectural foundation that not only yields compact, parameter-efficient models but also inherently mitigates the critical synchronization issues affecting dynamic inference. Finally, this work presents a unified framework where lightweight routers are pretrained to dynamically assign an optimal recursion depth for each token. This approach establishes a new Pareto frontier between efficiency and performance by effectively optimizing for both adaptive computation and parameter efficiency within a single model. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">PhD Dissertation</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05903">arXiv:2509.05903</a><span> [<a href="...2509.05903">pdf</a>, <a href="...2509.05903">ps</a>, <a href="...2509.05903">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span></div></div><p class="title is-5 mathjax"> Optimal Anchor Deployment and Topology Design for <span class="search-hit mathjax">Large</span>-Scale AUV Navigation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Huang%2C+W">Wei Huang</a>, <a href="...?searchtype=author&amp;query=Lu%2C+J">Junpeng Lu</a>, <a href="...?searchtype=author&amp;query=Xu%2C+T">Tianhe Xu</a>, <a href="...?searchtype=author&amp;query=Shu%2C+J">Jianxu Shu</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+H">Hao Zhang</a>, <a href="...?searchtype=author&amp;query=Meng%2C+K">Kaitao Meng</a>, <a href="...?searchtype=author&amp;query=Wu%2C+Y">Yanan Wu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05903v1-abstract-short"> Seafloor acoustic anchors are an important component of AUV navigation, providing absolute updates that correct inertial dead-reckoning. Unlike terrestrial positioning systems, the deployment of underwater anchor nodes is usually sparse due to the uneven distribution of underwater users, as well as the high economic cost and difficult maintenance of underwater equipment. These anchor nodes lack sa… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05903v1-abstract-full"> Seafloor acoustic anchors are an important component of AUV navigation, providing absolute updates that correct inertial dead-reckoning. Unlike terrestrial positioning systems, the deployment of underwater anchor nodes is usually sparse due to the uneven distribution of underwater users, as well as the high economic cost and difficult maintenance of underwater equipment. These anchor nodes lack satellite coverage and cannot form ubiquitous backhaul as terrestrial nodes do. In this paper, we investigate the optimal anchor deployment topology to provide high-quality AUV navigation and positioning services. We first analyze the possible deployment mode in large-scale underwater navigation system, and formulate a topology optimization for underwater anchor node deployment. Then, we derive a scaling law about the influence of anchors in each cluster on the navigation performance within a given area and demonstrate a service area coverage condition with a high probability of reaching the destination. Finally, the optimization performance is evaluated through experimental results. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05895">arXiv:2509.05895</a><span> [<a href="...2509.05895">pdf</a>, <a href="...2509.05895">ps</a>, <a href="...2509.05895">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal <span class="search-hit mathjax">Large</span> Language Model </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Li%2C+Y">Yujie Li</a>, <a href="...?searchtype=author&amp;query=Xu%2C+W">Wenjia Xu</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yuanben Zhang</a>, <a href="...?searchtype=author&amp;query=Wei%2C+Z">Zhiwei Wei</a>, <a href="...?searchtype=author&amp;query=Peng%2C+M">Mugen Peng</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05895v1-abstract-short"> Bi-temporal satellite imagery supports critical applications such as urban development monitoring and disaster assessment. Although powerful multimodal large language models (MLLMs) have been applied in bi-temporal change analysis, previous methods process image pairs through direct concatenation, inadequately modeling temporal correlations and spatial semantic changes. This deficiency hampers vis… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05895v1-abstract-full"> Bi-temporal satellite imagery supports critical applications such as urban development monitoring and disaster assessment. Although powerful multimodal large language models (MLLMs) have been applied in bi-temporal change analysis, previous methods process image pairs through direct concatenation, inadequately modeling temporal correlations and spatial semantic changes. This deficiency hampers visual-semantic alignment in change understanding, thereby constraining the overall effectiveness of current approaches. To address this gap, we propose BTCChat, a multi-temporal MLLM with advanced bi-temporal change understanding capability. BTCChat supports bi-temporal change captioning and retains single-image interpretation capability. To better capture temporal features and spatial semantic changes in image pairs, we design a Change Extraction module. Moreover, to enhance the model's attention to spatial details, we introduce a Prompt Augmentation mechanism, which incorporates contextual clues into the prompt to enhance model performance. Experimental results demonstrate that BTCChat achieves state-of-the-art performance on change captioning and visual question answering tasks. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">5 pages, 2 figures Submitted to ICASSP 2026</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05881">arXiv:2509.05881</a><span> [<a href="...2509.05881">pdf</a>, <a href="...2509.05881">ps</a>, <a href="...2509.05881">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> GeoAnalystBench: A GeoAI benchmark for assessing <span class="search-hit mathjax">large</span> language models for spatial analysis workflow and code generation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhang%2C+Q">Qianheng Zhang</a>, <a href="...?searchtype=author&amp;query=Gao%2C+S">Song Gao</a>, <a href="...?searchtype=author&amp;query=Wei%2C+C">Chen Wei</a>, <a href="...?searchtype=author&amp;query=Zhao%2C+Y">Yibo Zhao</a>, <a href="...?searchtype=author&amp;query=Nie%2C+Y">Ying Nie</a>, <a href="...?searchtype=author&amp;query=Chen%2C+Z">Ziru Chen</a>, <a href="...?searchtype=author&amp;query=Chen%2C+S">Shijie Chen</a>, <a href="...?searchtype=author&amp;query=Su%2C+Y">Yu Su</a>, <a href="...?searchtype=author&amp;query=Sun%2C+H">Huan Sun</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05881v1-abstract-short"> Recent advances in large language models (LLMs) have fueled growing interest in automating geospatial analysis and GIS workflows, yet their actual capabilities remain uncertain. In this work, we call for rigorous evaluation of LLMs on well-defined geoprocessing tasks before making claims about full GIS automation. To this end, we present GeoAnalystBench, a benchmark of 50 Python-based tasks derive… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05881v1-abstract-full"> Recent advances in large language models (LLMs) have fueled growing interest in automating geospatial analysis and GIS workflows, yet their actual capabilities remain uncertain. In this work, we call for rigorous evaluation of LLMs on well-defined geoprocessing tasks before making claims about full GIS automation. To this end, we present GeoAnalystBench, a benchmark of 50 Python-based tasks derived from real-world geospatial problems and carefully validated by GIS experts. Each task is paired with a minimum deliverable product, and evaluation covers workflow validity, structural alignment, semantic similarity, and code quality (CodeBLEU). Using this benchmark, we assess both proprietary and open source models. Results reveal a clear gap: proprietary models such as ChatGPT-4o-mini achieve high validity 95% and stronger code alignment (CodeBLEU 0.39), while smaller open source models like DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5% validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as spatial relationship detection or optimal site selection, remain the most challenging across all models. These findings demonstrate both the promise and limitations of current LLMs in GIS automation and provide a reproducible framework to advance GeoAI research with human-in-the-loop support. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">34 pages, 8 figures</span></p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span> I.2 </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> Transactions in GIS, 2025 </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05852">arXiv:2509.05852</a><span> [<a href="...2509.05852">pdf</a>, <a href="...2509.05852">ps</a>, <a href="...2509.05852">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistics Theory">math.ST</span></div></div><p class="title is-5 mathjax"> Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for <span class="search-hit mathjax">Large</span> Language Model Evaluation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yichi Zhang</a>, <a href="...?searchtype=author&amp;query=Belloni%2C+A">Alexander Belloni</a>, <a href="...?searchtype=author&amp;query=Fang%2C+E+X">Ethan X. Fang</a>, <a href="...?searchtype=author&amp;query=Lu%2C+J">Junwei Lu</a>, <a href="...?searchtype=author&amp;query=Xu%2C+X">Xiaoan Xu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05852v1-abstract-short"> Motivated by the need for rigorous and scalable evaluation of large language models, we study contextual preference inference for pairwise comparison functionals of context-dependent preference score functions across domains. Focusing on the contextual Bradley-Terry-Luce model, we develop a semiparametric efficient estimator that automates the debiased estimation through aggregating weighted resid… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05852v1-abstract-full"> Motivated by the need for rigorous and scalable evaluation of large language models, we study contextual preference inference for pairwise comparison functionals of context-dependent preference score functions across domains. Focusing on the contextual Bradley-Terry-Luce model, we develop a semiparametric efficient estimator that automates the debiased estimation through aggregating weighted residual balancing terms across the comparison graph. We show that the efficiency is achieved when the weights are derived from a novel strategy called Fisher random walk. We also propose a computationally feasible method to compute the weights by a potential representation of nuisance weight functions. We show our inference procedure is valid for general score function estimators accommodating the practitioners' need to implement flexible deep learning methods. We extend the procedure to multiple hypothesis testing using a Gaussian multiplier bootstrap that controls familywise error and to distributional shift via a cross-fitted importance-sampling adjustment for target-domain inference. Numerical studies, including language model evaluations under diverse contexts, corroborate the accuracy, efficiency, and practical utility of our method. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05773">arXiv:2509.05773</a><span> [<a href="...2509.05773">pdf</a>, <a href="...2509.05773">ps</a>, <a href="...2509.05773">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> PictOBI-20k: Unveiling <span class="search-hit mathjax">Large</span> Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Chen%2C+Z">Zijian Chen</a>, <a href="...?searchtype=author&amp;query=Hua%2C+W">Wenjie Hua</a>, <a href="...?searchtype=author&amp;query=Li%2C+J">Jinhao Li</a>, <a href="...?searchtype=author&amp;query=Deng%2C+L">Lirong Deng</a>, <a href="...?searchtype=author&amp;query=Du%2C+F">Fan Du</a>, <a href="...?searchtype=author&amp;query=Chen%2C+T">Tingzhu Chen</a>, <a href="...?searchtype=author&amp;query=Zhai%2C+G">Guangtao Zhai</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05773v1-abstract-short"> Deciphering oracle bone characters (OBCs), the oldest attested form of written Chinese, has remained the ultimate, unwavering goal of scholars, offering an irreplaceable key to understanding humanity's early modes of production. Current decipherment methodologies of OBC are primarily constrained by the sporadic nature of archaeological excavations and the limited corpus of inscriptions. With the p… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05773v1-abstract-full"> Deciphering oracle bone characters (OBCs), the oldest attested form of written Chinese, has remained the ultimate, unwavering goal of scholars, offering an irreplaceable key to understanding humanity's early modes of production. Current decipherment methodologies of OBC are primarily constrained by the sporadic nature of archaeological excavations and the limited corpus of inscriptions. With the powerful visual perception capability of large multimodal models (LMMs), the potential of using LMMs for visually deciphering OBCs has increased. In this paper, we introduce PictOBI-20k, a dataset designed to evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It includes 20k meticulously collected OBC and real object images, forming over 15k multi-choice questions. We also conduct subjective annotations to investigate the consistency of the reference point between humans and LMMs in visual reasoning. Experiments indicate that general LMMs possess preliminary visual decipherment skills, and LMMs are not effectively using visual information, while most of the time they are limited by language priors. We hope that our dataset can facilitate the evaluation and optimization of visual attention in future OBC-oriented LMMs. The code and dataset will be available at https://github.com/OBI-Future/PictOBI-20k. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">6 pages, 6 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05757">arXiv:2509.05757</a><span> [<a href="...2509.05757">pdf</a>, <a href="...2509.05757">ps</a>, <a href="...2509.05757">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Hyperbolic <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Patil%2C+S">Sarang Patil</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Z">Zeyong Zhang</a>, <a href="...?searchtype=author&amp;query=Huang%2C+Y">Yiran Huang</a>, <a href="...?searchtype=author&amp;query=Ma%2C+T">Tengfei Ma</a>, <a href="...?searchtype=author&amp;query=Xu%2C+M">Mengjia Xu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05757v1-abstract-short"> Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation net… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05757v1-abstract-full"> Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation networks, financial networks, brain networks, and linguistic structures or syntactic trees in natural languages. Effectively learning intrinsic semantic entailment and hierarchical relationships from these raw, unstructured input data using LLMs remains an underexplored area. Due to its effectiveness in modeling tree-like hierarchical structures, hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity as an expressive latent representation space for complex data modeling across domains such as graphs, images, languages, and multi-modal data. Here, we provide a comprehensive and contextual exposition of recent advancements in LLMs that leverage hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning. Specifically, the paper presents a taxonomy of the principal techniques of Hyperbolic LLMs (HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4) hyperbolic state-space models. We also explore crucial potential applications and outline future research directions. A repository of key papers, models, datasets, and code implementations is available at https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">32 pages, 6 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05747">arXiv:2509.05747</a><span> [<a href="...2509.05747">pdf</a>, <a href="...2509.05747">ps</a>, <a href="...2509.05747">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span></div><div class="is-inline-block"><div class="tags has-addons"><span class="tag is-dark is-size-7">doi</span><span class="tag is-light is-size-7"><a class="" href="...3747871">10.1145/3747871 <i aria-hidden="true" class="fa fa-external-link"></i></a></span></div></div></div><p class="title is-5 mathjax"> InterAct: A <span class="search-hit mathjax">Large</span>-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Ho%2C+L">Leo Ho</a>, <a href="...?searchtype=author&amp;query=Huang%2C+Y">Yinghao Huang</a>, <a href="...?searchtype=author&amp;query=Qin%2C+D">Dafei Qin</a>, <a href="...?searchtype=author&amp;query=Shi%2C+M">Mingyi Shi</a>, <a href="...?searchtype=author&amp;query=Tse%2C+W">Wangpok Tse</a>, <a href="...?searchtype=author&amp;query=Liu%2C+W">Wei Liu</a>, <a href="...?searchtype=author&amp;query=Yamagishi%2C+J">Junichi Yamagishi</a>, <a href="...?searchtype=author&amp;query=Komura%2C+T">Taku Komura</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05747v1-abstract-short"> We address the problem of accurate capture of interactive behaviors between two people in daily scenarios. Most previous works either only consider one person or solely focus on conversational gestures of two people, assuming the body orientation and/or position of each actor are constant or barely change over each interaction. In contrast, we propose to simultaneously model two people's activitie… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05747v1-abstract-full"> We address the problem of accurate capture of interactive behaviors between two people in daily scenarios. Most previous works either only consider one person or solely focus on conversational gestures of two people, assuming the body orientation and/or position of each actor are constant or barely change over each interaction. In contrast, we propose to simultaneously model two people's activities, and target objective-driven, dynamic, and semantically consistent interactions which often span longer duration and cover bigger space. To this end, we capture a new multi-modal dataset dubbed InterAct, which is composed of 241 motion sequences where two people perform a realistic and coherent scenario for one minute or longer over a complete interaction. For each sequence, two actors are assigned different roles and emotion labels, and collaborate to finish one task or conduct a common interaction activity. The audios, body motions, and facial expressions of both persons are captured. InterAct contains diverse and complex motions of individuals and interesting and relatively long-term interaction patterns barely seen before. We also demonstrate a simple yet effective diffusion-based method that estimates interactive face expressions and body motions of two people from speech inputs. Our method regresses the body motions in a hierarchical manner, and we also propose a novel fine-tuning mechanism to improve the lip accuracy of facial expressions. To facilitate further research, the data and code is made available at https://hku-cg.github.io/interact/ . <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">The first two authors contributed equally to this work</span></p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span> I.5.4 </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> Proceedings of the ACM on Computer Graphics and Interactive Techniques 8.4 (2025) 53:1-27 </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05695">arXiv:2509.05695</a><span> [<a href="...2509.05695">pdf</a>, <a href="...2509.05695">ps</a>, <a href="...2509.05695">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> Leveraging Vision-Language <span class="search-hit mathjax">Large</span> Models for Interpretable Video Action Recognition with Semantic Tokenization </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Peng%2C+J">Jingwei Peng</a>, <a href="...?searchtype=author&amp;query=Qiu%2C+Z">Zhixuan Qiu</a>, <a href="...?searchtype=author&amp;query=Jin%2C+B">Boyu Jin</a>, <a href="...?searchtype=author&amp;query=Siripong%2C+S">Surasakdi Siripong</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05695v1-abstract-short"> Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05695v1-abstract-full"> Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision-Language Large Models (LVLMs) to video action recognition, emphasizing enhanced accuracy and interpretability. Our method features a Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video sequences into discrete, semantically and temporally consistent "semantic action tokens," effectively crafting an "action narrative" that is comprehensible to an LVLM. These tokens, combined with natural language instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B) for robust action classification and semantic reasoning. LVLM-VAR not only achieves state-of-the-art or highly competitive performance on challenging benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set), but also substantially boosts model interpretability by generating natural language explanations for its predictions. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05668">arXiv:2509.05668</a><span> [<a href="...2509.05668">pdf</a>, <a href="...2509.05668">ps</a>, <a href="...2509.05668">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Llama-GENBA-10B: A Trilingual <span class="search-hit mathjax">Large</span> Language Model for German, English and Bavarian </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Hoffmann%2C+M">Michael Hoffmann</a>, <a href="...?searchtype=author&amp;query=John%2C+J">Jophin John</a>, <a href="...?searchtype=author&amp;query=Schweter%2C+S">Stefan Schweter</a>, <a href="...?searchtype=author&amp;query=Ramakrishnan%2C+G">Gokul Ramakrishnan</a>, <a href="...?searchtype=author&amp;query=Mak%2C+H">Hoi-Fong Mak</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+A">Alice Zhang</a>, <a href="...?searchtype=author&amp;query=Gaynullin%2C+D">Dmitry Gaynullin</a>, <a href="...?searchtype=author&amp;query=Hammer%2C+N+J">Nicolay J. Hammer</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05668v1-abstract-short"> We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05668v1-abstract-full"> We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Michael Hoffmann and Jophin John contributed equally to this work</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05660">arXiv:2509.05660</a><span> [<a href="...2509.05660">pdf</a>, <a href="...2509.05660">ps</a>, <a href="...2509.05660">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Cross-Question Method Reuse in <span class="search-hit mathjax">Large</span> Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Su%2C+H">Hong Su</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05660v1-abstract-short"> Large language models (LLMs) have been widely applied to assist in finding solutions for diverse questions. Prior work has proposed representing a method as a pair of a question and its corresponding solution, enabling method reuse. However, existing approaches typically require the questions to be highly similar. In this paper, we extend the scope of method reuse to address questions with low sim… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05660v1-abstract-full"> Large language models (LLMs) have been widely applied to assist in finding solutions for diverse questions. Prior work has proposed representing a method as a pair of a question and its corresponding solution, enabling method reuse. However, existing approaches typically require the questions to be highly similar. In this paper, we extend the scope of method reuse to address questions with low similarity or with hidden similarities that are not explicitly observable. For questions that are similar in a general-specific sense (i.e., broader or narrower in scope), we propose to first separate the question and solution, rather than directly feeding the pair to the LLM. The LLM is then guided to adapt the solution to new but related questions, allowing it to focus on solution transfer rather than question recognition. Furthermore, we extend this approach to cases where questions only share partial features or hidden characteristics. This enables cross-question method reuse beyond conventional similarity constraints. Experimental verification shows that our scope-extension approach increases the probability of filtering out reusable solutions, thereby improving the effectiveness of cross-question method reuse. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05622">arXiv:2509.05622</a><span> [<a href="...2509.05622">pdf</a>, <a href="...2509.05622">ps</a>, <a href="...2509.05622">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Probability">math.PR</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span> and moderate deviation principles for stochastic partial differential equation on graph </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Cui%2C+J">Jianbo Cui</a>, <a href="...?searchtype=author&amp;query=Sheng%2C+D">Derui Sheng</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05622v1-abstract-short"> In this paper, we study large and moderate deviation principles for stochastic partial differential equations (SPDEs) on metric graphs and their associated multiscale models via the weak convergence approach, providing a refined characterization of the probabilities of rare events. Several challenges unique to the graph setting are encountered, including operator degeneracy near vertices and the l… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05622v1-abstract-full"> In this paper, we study large and moderate deviation principles for stochastic partial differential equations (SPDEs) on metric graphs and their associated multiscale models via the weak convergence approach, providing a refined characterization of the probabilities of rare events. Several challenges unique to the graph setting are encountered, including operator degeneracy near vertices and the lack of compactness on non-compact graphs. To address these difficulties, we introduce novel weighted Sobolev spaces on graphs, and prove compact embedding results specifically adapted to the degeneracy structure. Our analysis is particularly applicable to SPDEs on graphs arising as limits of stochastic reaction-diffusion systems on narrow domains and from fast-flow asymptotics of stochastic incompressible fluids, yielding new deviation results for these models. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05605">arXiv:2509.05605</a><span> [<a href="...2509.05605">pdf</a>, <a href="...2509.05605">ps</a>, <a href="...2509.05605">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Icon<span class="MathJax_Preview" style="">^{2}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-17-Frame" tabindex="0"></span>: Aligning <span class="search-hit mathjax">Large</span> Language Models Using Self-Synthetic Preference Data via Inherent Regulation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Chen%2C+Q">Qiyuan Chen</a>, <a href="...?searchtype=author&amp;query=Huang%2C+H">Hongsen Huang</a>, <a href="...?searchtype=author&amp;query=Shao%2C+Q">Qian Shao</a>, <a href="...?searchtype=author&amp;query=Chen%2C+J">Jiahe Chen</a>, <a href="...?searchtype=author&amp;query=Chen%2C+J">Jintai Chen</a>, <a href="...?searchtype=author&amp;query=Xu%2C+H">Hongxia Xu</a>, <a href="...?searchtype=author&amp;query=Hua%2C+R">Renjie Hua</a>, <a href="...?searchtype=author&amp;query=Chuan%2C+R">Ren Chuan</a>, <a href="...?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05605v1-abstract-short"> Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In t… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05605v1-abstract-full"> Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In this work, we explore a paradigm shift by leveraging inherent regulation of LLMs' representation space for efficient and tailored preference dataset construction, named Icon<span class="MathJax_Preview" style="">^{2}</span>. Specifically, it first extracts layer-wise direction vectors to encode sophisticated human preferences and then uses these vectors to filter self-synthesized instructions based on their inherent consistency. During decoding, bidirectional inherent control is applied to steer token representations, enabling the precise generation of response pairs with clear alignment distinctions. Experimental results demonstrate significant improvements in both alignment and efficiency. Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by up to 48.1%. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">EMNLP 2025 Main</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05564">arXiv:2509.05564</a><span> [<a href="...2509.05564">pdf</a>, <a href="...2509.05564">ps</a>, <a href="...2509.05564">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span></div></div><p class="title is-5 mathjax"> Knowledge-Augmented Relation Learning for Complementary Recommendation with <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Yamasaki%2C+C">Chihiro Yamasaki</a>, <a href="...?searchtype=author&amp;query=Sugahara%2C+K">Kai Sugahara</a>, <a href="...?searchtype=author&amp;query=Okamoto%2C+K">Kazushi Okamoto</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05564v1-abstract-short"> Complementary recommendations play a crucial role in e-commerce by enhancing user experience through suggestions of compatible items. Accurate classification of complementary item relationships requires reliable labels, but their creation presents a dilemma. Behavior-based labels are widely used because they can be easily generated from interaction logs; however, they often contain significant noi… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05564v1-abstract-full"> Complementary recommendations play a crucial role in e-commerce by enhancing user experience through suggestions of compatible items. Accurate classification of complementary item relationships requires reliable labels, but their creation presents a dilemma. Behavior-based labels are widely used because they can be easily generated from interaction logs; however, they often contain significant noise and lack reliability. While function-based labels (FBLs) provide high-quality definitions of complementary relationships by carefully articulating them based on item functions, their reliance on costly manual annotation severely limits a model's ability to generalize to diverse items. To resolve this trade-off, we propose Knowledge-Augmented Relation Learning (KARL), a framework that strategically fuses active learning with large language models (LLMs). KARL efficiently expands a high-quality FBL dataset at a low cost by selectively sampling data points that the classifier finds the most difficult and uses the label extension of the LLM. Our experiments showed that in out-of-distribution (OOD) settings, an unexplored item feature space, KARL improved the baseline accuracy by up to 37%. In contrast, in in-distribution (ID) settings, the learned item feature space, the improvement was less than 0.5%, with prolonged learning could degrade accuracy. These contrasting results are due to the data diversity driven by KARL's knowledge expansion, suggesting the need for a dynamic sampling strategy that adjusts diversity based on the prediction context (ID or OOD). <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> The 2nd Workshop on Generative AI for E-Commerce 2025 in conjunction with the 19th ACM Conference on Recommender Systems (RecSys 2025) </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05553">arXiv:2509.05553</a><span> [<a href="...2509.05553">pdf</a>, <a href="...2509.05553">ps</a>, <a href="...2509.05553">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Using Contrastive Learning to Improve Two-Way Reasoning in <span class="search-hit mathjax">Large</span> Language Models: The Obfuscation Task as a Case Study </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nikiema%2C+S+L">Serge Lionel Nikiema</a>, <a href="...?searchtype=author&amp;query=Samhi%2C+J">Jordan Samhi</a>, <a href="...?searchtype=author&amp;query=Moumoula%2C+M+B">Micheline Bénédicte Moumoula</a>, <a href="...?searchtype=author&amp;query=Djir%C3%A9%2C+A+E">Albérick Euraste Djiré</a>, <a href="...?searchtype=author&amp;query=Kabor%C3%A9%2C+A+K">Abdoul Kader Kaboré</a>, <a href="...?searchtype=author&amp;query=Klein%2C+J">Jacques Klein</a>, <a href="...?searchtype=author&amp;query=Bissyand%C3%A9%2C+T+F">Tegawendé F. Bissyandé</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05553v1-abstract-short"> This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow revers… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05553v1-abstract-full"> This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow reversibility. For example, a model that can change a variable name like userIndex to i should also be able to infer that i represents a user index without reverse training. The researchers tested current language models and discovered what they term cognitive specialization: when models are fine-tuned on forward tasks, their performance on those tasks improves, but their ability to reason bidirectionally becomes significantly worse. To address this issue, they developed Contrastive Fine-Tuning (CFT), which trains models using three types of examples: positive examples that maintain semantic meaning, negative examples with different semantics, and forward-direction obfuscation examples. This approach aims to develop deeper understanding rather than surface-level pattern recognition and allows reverse capabilities to develop naturally without explicit reverse training. Their experiments demonstrated that CFT successfully achieved bidirectional reasoning, enabling strong reverse performance while maintaining forward task capabilities. The authors conclude that bidirectional reasoning serves both as a theoretical framework for assessing genuine understanding and as a practical training approach for developing more capable AI systems. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05513">arXiv:2509.05513</a><span> [<a href="...2509.05513">pdf</a>, <a href="...2509.05513">ps</a>, <a href="...2509.05513">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span></div></div><p class="title is-5 mathjax"> OpenEgo: A <span class="search-hit mathjax">Large</span>-Scale Multimodal Egocentric Dataset for Dexterous Manipulation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Jawaid%2C+A">Ahad Jawaid</a>, <a href="...?searchtype=author&amp;query=Xiang%2C+Y">Yu Xiang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05513v1-abstract-short"> Egocentric human videos provide scalable demonstrations for imitation learning, but existing corpora often lack either fine-grained, temporally localized action descriptions or dexterous hand annotations. We introduce OpenEgo, a multimodal egocentric manipulation dataset with standardized hand-pose annotations and intention-aligned action primitives. OpenEgo totals 1107 hours across six public dat… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05513v1-abstract-full"> Egocentric human videos provide scalable demonstrations for imitation learning, but existing corpora often lack either fine-grained, temporally localized action descriptions or dexterous hand annotations. We introduce OpenEgo, a multimodal egocentric manipulation dataset with standardized hand-pose annotations and intention-aligned action primitives. OpenEgo totals 1107 hours across six public datasets, covering 290 manipulation tasks in 600+ environments. We unify hand-pose layouts and provide descriptive, timestamped action primitives. To validate its utility, we train language-conditioned imitation-learning policies to predict dexterous hand trajectories. OpenEgo is designed to lower the barrier to learning dexterous manipulation from egocentric video and to support reproducible research in vision-language-action learning. All resources and instructions will be released at www.openegocentric.com. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">4 pages, 1 figure</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05471">arXiv:2509.05471</a><span> [<a href="...2509.05471">pdf</a>, <a href="...2509.05471">ps</a>, <a href="...2509.05471">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Behind the Mask: Benchmarking Camouflaged Jailbreaks in <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zheng%2C+Y">Youjia Zheng</a>, <a href="...?searchtype=author&amp;query=Zandsalimy%2C+M">Mohammad Zandsalimy</a>, <a href="...?searchtype=author&amp;query=Sushmita%2C+S">Shanu Sushmita</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05471v1-abstract-short"> Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current def… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05471v1-abstract-full"> Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current defense systems. This paper investigates the construction and impact of camouflaged jailbreak prompts, emphasizing their deceptive characteristics and the limitations of traditional keyword-based detection methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts, containing 500 curated examples (400 harmful and 100 benign prompts) designed to rigorously stress-test LLM safety protocols. In addition, we propose a multi-faceted evaluation framework that measures harmfulness across seven dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards, Harmful Potential, Educational Value, Content Quality, and Compliance Score. Our findings reveal a stark contrast in LLM behavior: while models demonstrate high safety and content quality with benign inputs, they exhibit a significant decline in performance and safety when confronted with camouflaged jailbreak attempts. This disparity underscores a pervasive vulnerability, highlighting the urgent need for more nuanced and adaptive security strategies to ensure the responsible and robust deployment of LLMs in real-world applications. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li></ol><nav aria-label="pagination" class="pagination is-small is-centered breathe-horizontal" role="navigation"><a class="pagination-previous is-invisible" href="">Previous </a><a class="pagination-next" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">Next </a><ul class="pagination-list"><li><a aria-label="Goto page 1" class="pagination-link is-current" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0">1 </a></li><li><a aria-current="page" aria-label="Page 2" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">2 </a></li><li><a aria-current="page" aria-label="Page 3" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100">3 </a></li><li><a aria-current="page" aria-label="Page 4" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150">4 </a></li><li><a aria-current="page" aria-label="Page 5" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200">5 </a></li><li><span class="pagination-ellipsis">…</span></li></ul></nav><div class="is-hidden-tablet"><span class="help"><a href="...releases">Search v0.5.6 released 2020-02-24</a></span></div></div></main><footer><div aria-label="Secondary" class="columns is-desktop" role="navigation"><div class="column"><div class="columns"><div class="column"><ul class="nav-spaced"><li><a href="...about">About</a></li><li><a href="...help">Help</a></li></ul></div><div class="column"><ul class="nav-spaced"><li><svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg><a href="...contact.html"> Contact</a></li><li><svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"></path></svg><a href="...subscribe"> Subscribe</a></li></ul></div></div></div><div class="column"><div class="columns"><div class="column"><ul class="nav-spaced"><li><a href="...index.html">Copyright</a></li><li><a href="...privacy_policy.html">Privacy Policy</a></li></ul></div><div class="column sorry-app-links"><ul class="nav-spaced"><li><a href="...web_accessibility.html">Web Accessibility Assistance</a></li><li><p class="help"><a class="a11y-main-link" href="...status.arxiv.org" target="_blank">arXiv Operational Status <svg class="icon filter-dark_grey" role="presentation" viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"></path></svg></a><br/> Get status notifications via <a class="is-link" href="...new" target="_blank"><svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg>email</a> or <a class="is-link" href="...new" target="_blank"><svg class="icon filter-black" role="presentation" viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg>slack</a></p></li></ul></div></div></div></div></footer><div><div id="MathJax_Font_Test"></div></div></body></html>