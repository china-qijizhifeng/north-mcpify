<!--
=== 网页自动化HTML快照 (已清理) ===
URL: https://arxiv.org/search/advanced?advanced=1&terms-0-operator=AND&terms-0-term=large&terms-0-field=title&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2025&date-to_date=2025&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first
最后更新: 2025-09-09T17:18:54.980265
原始大小: 100 KB
清理后大小: 79 KB
压缩率: 21.0%
生成时间: 2025-09-09T17:18:55.810100

清理说明:
- 已删除: <script>、<style>、<noscript> 标签
- 已删除: style属性、onclick等事件属性
- 已删除: CSS样式定义
- 保留: id、class、name、type等选择器属性
- 保留: 文本内容和DOM结构
-->
<!DOCTYPE html>
<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><meta content="width=device-width, initial-scale=1" name="viewport"/><link href="...apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/><link href="...favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/><link href="...favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/><link href="...site.webmanifest" rel="manifest"/><link href="...safari-pinned-tab.svg" rel="mask-icon"/><link href="...favicon.ico" rel="shortcut icon"/><meta content="#b31b1b" name="msapplication-TileColor"/><meta content="images/icons/browserconfig.xml" name="msapplication-config"/><meta content="#b31b1b" name="theme-color"/><title>Advanced Search | arXiv e-print repository</title><link href="...arxivstyle.css" rel="stylesheet"/><link href="...bulma-tooltip.min.css" rel="stylesheet"/><link href="...search.css" rel="stylesheet"/></head><body><header><a class="is-sr-only" href="#main-container">Skip to main content</a><div class="attribution level is-marginless" role="banner"><div class="level-left"><a class="level-item" href="..."><img alt="Cornell University" aria-label="logo" src="...cornell-reduced-white-SMALL.svg"/></a></div><div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br/> the Simons Foundation, <a href="...ourmembers.html">member institutions</a>, and all contributors. <a href="...donate.html">Donate</a></span></p></div></div><div class="identity level is-marginless"><div class="level-left"><div class="level-item"><a aria-label="arxiv-logo" class="arxiv" href="..."><img alt="arxiv logo" aria-label="logo" src="...arxiv-logo-one-color-white.svg"/></a></div></div><div class="search-block level-right"><form action="https://arxiv.org/search" class="level-item mini-search" method="GET"><div class="field has-addons"><div class="control"><input aria-label="Search term or terms" class="input is-small" name="query" placeholder="Search..." type="text"/><p class="help"><a href="...help">Help</a> | <a href="...advanced">Advanced Search</a></p></div><div class="control"><div class="select is-small"><select aria-label="Field to search" name="searchtype"><option selected="selected" value="all">All fields</option><option value="title">Title</option><option value="author">Author</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select></div></div><input name="source" type="hidden" value="header"/><button class="button is-small is-cul-darker">Search</button></div></form></div></div><div class="container"><div aria-label="User menu" class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation"><a href="...login">Login</a></div></div></header><main class="container" id="main-container"><div class="level is-marginless"><div class="level-left"><h1 class="title is-clearfix"> Showing 1–50 of 9,947 results </h1></div><div class="level-right is-hidden-mobile"><span class="help"><a href="...releases">Search v0.5.6 released 2020-02-24</a></span></div></div><div class="content"><div class="columns"><div class="column is-two-thirds-tablet"><p>Query: <a href="...advanced?terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first">order: -announced_date_first; size: 50; date_range: from 2025-01-01 to 2025-12-31; include_cross_list: True; terms: AND title=large</a></p><div class="buttons"><a class="button is-link" href="...advanced?terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first">Refine query</a><a class="button" href="...advanced">New search</a></div></div><div class="column is-one-third-tablet is-hidden-mobile"><p class="has-text-right"><a href="...?order=-announced_date_first&amp;size=50">Simple Search</a></p></div></div><div class="level breathe-horizontal"><div class="level-left"><form action="/search/advanced" method="GET"><div><input id="advanced" name="advanced" type="hidden" value="1"/><ul id="terms"><li><label for="terms-0">Terms-0</label><table id="terms-0"><tbody><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="large"/></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected="" value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option selected="" value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option value="all">All fields</option></select></td></tr></tbody></table></li></ul><table id="classification"><tbody><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected="" value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked="" id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"/><label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"/><label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></tbody></table><table id="date"><tbody><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"/><label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"/><label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"/><label for="date-filter_by-2">Specific year</label></li><li><input checked="" id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"/><label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""/></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value="2025"/></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value="2025"/></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked="" id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"/><label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"/><label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"/><label for="date-date_type-2">Announcement date</label></li></ul></td></tr></tbody></table><input id="include_older_versions" name="include_older_versions" type="checkbox" value="y"/><ul id="abstracts"><li><input checked="" id="abstracts-0" name="abstracts" type="radio" value="show"/><label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"/><label for="abstracts-1">Hide abstracts</label></li></ul></div><div class="box field is-grouped is-grouped-multiline level-item"><div class="control"><span class="select is-small"><select id="size" name="size"><option value="25">25</option><option selected="" value="50">50</option><option value="100">100</option><option value="200">200</option></select></span><label for="size">results per page</label>. </div><div class="control"><label for="order">Sort results by</label><span class="select is-small"><select id="order" name="order"><option selected="" value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select></span></div><div class="control"><button class="button is-small is-link">Go</button></div></div></form></div></div><nav aria-label="pagination" class="pagination is-small is-centered breathe-horizontal" role="navigation"><a class="pagination-previous is-invisible" href="">Previous </a><a class="pagination-next" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">Next </a><ul class="pagination-list"><li><a aria-label="Goto page 1" class="pagination-link is-current" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0">1 </a></li><li><a aria-current="page" aria-label="Page 2" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">2 </a></li><li><a aria-current="page" aria-label="Page 3" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100">3 </a></li><li><a aria-current="page" aria-label="Page 4" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150">4 </a></li><li><a aria-current="page" aria-label="Page 5" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=large&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200">5 </a></li><li><span class="pagination-ellipsis">…</span></li></ul></nav><ol class="breathe-horizontal" start="1"><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06949">arXiv:2509.06949</a><span> [<a href="...2509.06949">pdf</a>, <a href="...2509.06949">ps</a>, <a href="...2509.06949">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Revolutionizing Reinforcement Learning Framework for Diffusion <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Wang%2C+Y">Yinjie Wang</a>, <a href="...?searchtype=author&amp;query=Yang%2C+L">Ling Yang</a>, <a href="...?searchtype=author&amp;query=Li%2C+B">Bowen Li</a>, <a href="...?searchtype=author&amp;query=Tian%2C+Y">Ye Tian</a>, <a href="...?searchtype=author&amp;query=Shen%2C+K">Ke Shen</a>, <a href="...?searchtype=author&amp;query=Wang%2C+M">Mengdi Wang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06949v1-abstract-short"> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it ca… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06949v1-abstract-full"> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Code and Models: https://github.com/Gen-Verse/dLLM-RL</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06858">arXiv:2509.06858</a><span> [<a href="...2509.06858">pdf</a>, <a href="...2509.06858">ps</a>, <a href="...2509.06858">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Adaptation and Self-Organizing Systems">nlin.AO</span></div></div><p class="title is-5 mathjax"> Disentangling Interaction and Bias Effects in Opinion Dynamics of <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Brockers%2C+V+C">Vincent C. Brockers</a>, <a href="...?searchtype=author&amp;query=Ehrlich%2C+D+A">David A. Ehrlich</a>, <a href="...?searchtype=author&amp;query=Priesemann%2C+V">Viola Priesemann</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06858v1-abstract-short"> Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias to… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06858v1-abstract-full"> Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias toward the initiating agent's stance. Applying this framework to multi-step dialogues reveals that opinion trajectories tend to quickly converge to a shared attractor, with the influence of the interaction fading over time, and the impact of biases differing between LLMs. In addition, we fine-tune an LLM on different sets of strongly opinionated statements (incl. misinformation) and demonstrate that the opinion attractor shifts correspondingly. Exposing stark differences between LLMs and providing quantitative tools to compare them to human subjects in the future, our approach highlights both chances and pitfalls in using LLMs as proxies for human behavior. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06838">arXiv:2509.06838</a><span> [<a href="...2509.06838">pdf</a>, <a href="...2509.06838">ps</a>, <a href="...2509.06838">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span></div></div><p class="title is-5 mathjax"> EPT Benchmark: Evaluation of Persian Trustworthiness in <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Mirbagheri%2C+M+R">Mohammad Reza Mirbagheri</a>, <a href="...?searchtype=author&amp;query=Mirkamali%2C+M+M">Mohammad Mahdi Mirkamali</a>, <a href="...?searchtype=author&amp;query=Arani%2C+Z+M">Zahra Motoshaker Arani</a>, <a href="...?searchtype=author&amp;query=Javeri%2C+A">Ali Javeri</a>, <a href="...?searchtype=author&amp;query=Sadeghzadeh%2C+A+M">Amir Mahdi Sadeghzadeh</a>, <a href="...?searchtype=author&amp;query=Jalili%2C+R">Rasool Jalili</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06838v1-abstract-short"> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cu… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06838v1-abstract-full"> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06813">arXiv:2509.06813</a><span> [<a href="...2509.06813">pdf</a>, <a href="...2509.06813">ps</a>, <a href="...2509.06813">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> A Comparative Benchmark of <span class="search-hit mathjax">Large</span> Language Models for Labelling Wind Turbine Maintenance Logs </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Malyi%2C+M">Max Malyi</a>, <a href="...?searchtype=author&amp;query=Shek%2C+J">Jonathan Shek</a>, <a href="...?searchtype=author&amp;query=McDonald%2C+A">Alasdair McDonald</a>, <a href="...?searchtype=author&amp;query=Biscaya%2C+A">Andre Biscaya</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06813v1-abstract-short"> Effective Operation and Maintenance (O&amp;M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06813v1-abstract-full"> Effective Operation and Maintenance (O&amp;M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&amp;M data quality and downstream reliability analysis. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Associated GitHub repository: https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06801">arXiv:2509.06801</a><span> [<a href="...2509.06801">pdf</a>, <a href="...2509.06801">ps</a>, <a href="...2509.06801">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Astrophysics of Galaxies">astro-ph.GA</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span> eddy simulations in astrophysics </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Schmidt-Br%C3%BCckner%2C+W">Wolfram Schmidt-Brückner</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06801v1-abstract-short"> In this review, the methodology of large eddy simulations (LES) is introduced and applications in astrophysics are discussed. As theoretical framework, the scale decomposition of the dynamical equations for compressible neutral fluids by means of spatial filtering is explained. For cosmological applications, the filtered equations in co-moving coordinates are formulated. Moreover, the decompositio… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06801v1-abstract-full"> In this review, the methodology of large eddy simulations (LES) is introduced and applications in astrophysics are discussed. As theoretical framework, the scale decomposition of the dynamical equations for compressible neutral fluids by means of spatial filtering is explained. For cosmological applications, the filtered equations in co-moving coordinates are formulated. Moreover, the decomposition is extended to magnetohydrodynamics (MHD). While energy is dissipated through numerical diffusivities in implicit large eddy simulations (ILES), explicit subgrid-scale (SGS) models are applied in LES to compute energy dissipation, mixing, and dynamo action due to numerically unresolved turbulent eddies. The most commonly used models in astrophysics are the Smagorinsky model, the hydrodynamical SGS turbulence energy equation model, and the non-linear structural model for both non-relativistic and relativistic MHD. Model validation is carried out a priori by testing correlations between model and data for specific terms or a posteriori by comparing turbulence statistics in LES and ILES. Since most solvers in astrophysical simulation codes have significant numerical diffusion, the additional effect of SGS models is generally small. However, convergence with resolution increases in some cases. A recent example is magnetic field amplification in binary neutron star mergers. For mesh-free codes, it has been shown that explicit modelling of turbulent diffusion of metals has a significant impact. Moreover, SGS models can help to compute the turbulent velocity dispersion consistently and to parameterize sub-resolution processes that are influenced by turbulence, such as the star formation efficiency in galaxy simulations. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Completely revised and extended 2. edition, 77 pages, 18 figures, accepted for publication by Living Reviews in Computational Astrophysics</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06759">arXiv:2509.06759</a><span> [<a href="...2509.06759">pdf</a>, <a href="...2509.06759">ps</a>, <a href="...2509.06759">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Aligning <span class="search-hit mathjax">Large</span> Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nguyen%2C+T+T">Thanh Thi Nguyen</a>, <a href="...?searchtype=author&amp;query=Wilson%2C+C">Campbell Wilson</a>, <a href="...?searchtype=author&amp;query=Dalins%2C+J">Janis Dalins</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06759v1-abstract-short"> Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remain… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06759v1-abstract-full"> Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remains a critical challenge. Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) offer promising frameworks for this aligning process. While DRL enables models to optimize actions using reward signals instead of relying solely on supervised preference data, DPO directly aligns the policy with preferences, eliminating the need for an explicit reward model. This overview explores paradigms for fine-tuning LVLMs, highlighting how DRL and DPO techniques can be used to align models with human preferences and values, improve task performance, and enable adaptive multimodal interaction. We categorize key approaches, examine sources of preference data, reward signals, and discuss open challenges such as scalability, sample efficiency, continual learning, generalization, and safety. The goal is to provide a clear understanding of how DRL and DPO contribute to the evolution of robust and human-aligned LVLMs. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Accepted for publication in the Proceedings of the 8th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06685">arXiv:2509.06685</a><span> [<a href="...2509.06685">pdf</a>, <a href="...2509.06685">ps</a>, <a href="...2509.06685">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in <span class="search-hit mathjax">Large</span> Scenes </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhang%2C+S">Shengkai Zhang</a>, <a href="...?searchtype=author&amp;query=Liu%2C+Y">Yuhe Liu</a>, <a href="...?searchtype=author&amp;query=Wu%2C+G">Guanjun Wu</a>, <a href="...?searchtype=author&amp;query=He%2C+J">Jianhua He</a>, <a href="...?searchtype=author&amp;query=Wang%2C+X">Xinggang Wang</a>, <a href="...?searchtype=author&amp;query=Chen%2C+M">Mozi Chen</a>, <a href="...?searchtype=author&amp;query=Liu%2C+K">Kezhong Liu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06685v1-abstract-short"> VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Althoug… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06685v1-abstract-full"> VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06606">arXiv:2509.06606</a><span> [<a href="...2509.06606">pdf</a>, <a href="...2509.06606">ps</a>, <a href="...2509.06606">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span></div></div><p class="title is-5 mathjax"> Unveiling the Listener Structure Underlying K-pop's Global Success: A <span class="search-hit mathjax">Large</span>-Scale Listening Data Analysis </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nakamura%2C+R">Ryota Nakamura</a>, <a href="...?searchtype=author&amp;query=Nishimoto%2C+K">Keita Nishimoto</a>, <a href="...?searchtype=author&amp;query=Sakata%2C+I">Ichiro Sakata</a>, <a href="...?searchtype=author&amp;query=Asatani%2C+K">Kimitaka Asatani</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06606v1-abstract-short"> From the mid-2000s to the 2010s, K-pop moved beyond its status as a regionally popular genre in Asia and established itself as a global music genre with enthusiastic fans around the world. However, little is known about how the vast number of music listeners across the globe have listened to and perceived K-pop. This study addresses this question by analyzing a large-scale listening dataset from L… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06606v1-abstract-full"> From the mid-2000s to the 2010s, K-pop moved beyond its status as a regionally popular genre in Asia and established itself as a global music genre with enthusiastic fans around the world. However, little is known about how the vast number of music listeners across the globe have listened to and perceived K-pop. This study addresses this question by analyzing a large-scale listening dataset from Last.fm. An analysis of the distribution of play counts reveals that K-pop experienced a significant increase in plays between 2005 and 2019, largely supported by a small group of heavy listeners. The Gini coefficient in play counts is notably greater than that of existing mainstream genres and other growing niche genres. Furthermore, an analysis based on user-assigned genre tags quantitatively demonstrates that between 2005 and 2010, K-pop shed its status as a local Asian genre and established itself as a distinct music genre in its own right. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06596">arXiv:2509.06596</a><span> [<a href="...2509.06596">pdf</a>, <a href="...2509.06596">ps</a>, <a href="...2509.06596">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Tong%2C+X">Xin Tong</a>, <a href="...?searchtype=author&amp;query=Lin%2C+Z">Zhi Lin</a>, <a href="...?searchtype=author&amp;query=Wang%2C+J">Jingya Wang</a>, <a href="...?searchtype=author&amp;query=Jin%2C+B">Bo Jin</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06596v1-abstract-short"> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that d… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06596v1-abstract-full"> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06525">arXiv:2509.06525</a><span> [<a href="...2509.06525">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span></div></div><p class="title is-5 mathjax"> GPUTB: Efficient Machine Learning Tight-Binding Method for <span class="search-hit mathjax">Large</span>-Scale Electronic Properties Calculations </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Wang%2C+Y">Yunlong Wang</a>, <a href="...?searchtype=author&amp;query=Liang%2C+Z">Zhixin Liang</a>, <a href="...?searchtype=author&amp;query=Ding%2C+C">Chi Ding</a>, <a href="...?searchtype=author&amp;query=Wang%2C+J">Junjie Wang</a>, <a href="...?searchtype=author&amp;query=Fan%2C+Z">Zheyong Fan</a>, <a href="...?searchtype=author&amp;query=Wang%2C+H">Hui-Tian Wang</a>, <a href="...?searchtype=author&amp;query=Xing%2C+D">Dingyu Xing</a>, <a href="...?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06525v1-abstract-short"> The high computational cost of ab-initio methods limits their application in predicting electronic properties at the device scale. Therefore, an efficient method is needed to map the atomic structure to the electronic structure quickly. Here, we develop GPUTB, a GPU-accelerated tight-binding (TB) machine learning framework. GPUTB employs atomic environment descriptors, enabling the model parameter… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06525v1-abstract-full"> The high computational cost of ab-initio methods limits their application in predicting electronic properties at the device scale. Therefore, an efficient method is needed to map the atomic structure to the electronic structure quickly. Here, we develop GPUTB, a GPU-accelerated tight-binding (TB) machine learning framework. GPUTB employs atomic environment descriptors, enabling the model parameters to incorporate environmental dependence. This allows the model to transfer to different basis, xc-functionals, and allotropes easily. Combined with the linear scaling quantum transport method, we have calculated the electronic density of states for up to 100 million atoms in pristine graphene. Trained on finite-temperature structures, the model can be easily extended to millions of atom finite-temperature systems. Furthermore, GPUTB can also successfully describe h-BN/graphene heterojunction systems, demonstrating its capability to handle complex material with high precision. We accurately reproduce the relationship between carrier concentration and room temperature mobility in graphene to verify the framework's accuracy. Therefore, our GPUTB framework presents a delicate balance between computational accuracy and efficiency, providing a powerful computational tool for investing electronic properties for large systems with millions of atoms. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06460">arXiv:2509.06460</a><span> [<a href="...2509.06460">pdf</a>, <a href="...2509.06460">ps</a>, <a href="...2509.06460">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Chemical Physics">physics.chem-ph</span></div></div><p class="title is-5 mathjax"> Stochastic resolution of identity to CC2 for <span class="search-hit mathjax">large</span> systems: Excited-state gradients and derivative couplings </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhao%2C+C">Chongxiao Zhao</a>, <a href="...?searchtype=author&amp;query=Li%2C+C">Chenyang Li</a>, <a href="...?searchtype=author&amp;query=Dou%2C+W">Wenjie Dou</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06460v1-abstract-short"> Excited-state gradients and derivative couplings are critical for simulating excited-state dynamics. However, their calculations are very expensive within the coupled-cluster framework due to the steep scaling. In this work, we present two implementations of stochastic resolution of identity to CC2 (sRI-CC2) for excited-state analytical gradients and derivative couplings. The first method employs… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06460v1-abstract-full"> Excited-state gradients and derivative couplings are critical for simulating excited-state dynamics. However, their calculations are very expensive within the coupled-cluster framework due to the steep scaling. In this work, we present two implementations of stochastic resolution of identity to CC2 (sRI-CC2) for excited-state analytical gradients and derivative couplings. The first method employs sRI for both Coulomb and exchange terms, reducing the formal scaling to cubic. However, this method has a significant stochastic noise. Consequently, we introduce a substitute, termed partial sRI-CC2, which applies sRI selectively to the exchange terms only. The partial sRI-CC2 shows a quartic scaling with a modest prefactor, rendering it a practical alternative. Compared to conventional RI-CC2, the partial sRI-CC2 can handle systems with hundreds or even thousands of electrons. This work is an extension to our previous implementation of sRI-CC2 method and provides essential ingredients for large-scale nonadiabatic dynamics. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06456">arXiv:2509.06456</a><span> [<a href="...2509.06456">pdf</a>, <a href="...2509.06456">ps</a>, <a href="...2509.06456">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> Cross3DReg: Towards a <span class="search-hit mathjax">Large</span>-scale Real-world Cross-source Point Cloud Registration Benchmark </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Xu%2C+Z">Zongyi Xu</a>, <a href="...?searchtype=author&amp;query=Lang%2C+Z">Zhongpeng Lang</a>, <a href="...?searchtype=author&amp;query=Chen%2C+Y">Yilong Chen</a>, <a href="...?searchtype=author&amp;query=Zhao%2C+S">Shanshan Zhao</a>, <a href="...?searchtype=author&amp;query=Huang%2C+X">Xiaoshui Huang</a>, <a href="...?searchtype=author&amp;query=Zuo%2C+Y">Yifan Zuo</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yan Zhang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Q">Qianni Zhang</a>, <a href="...?searchtype=author&amp;query=Gao%2C+X">Xinbo Gao</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06456v1-abstract-short"> Cross-source point cloud registration, which aims to align point cloud data from different sensors, is a fundamental task in 3D vision. However, compared to the same-source point cloud registration, cross-source registration faces two core challenges: the lack of publicly available large-scale real-world datasets for training the deep registration models, and the inherent differences in point clou… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06456v1-abstract-full"> Cross-source point cloud registration, which aims to align point cloud data from different sensors, is a fundamental task in 3D vision. However, compared to the same-source point cloud registration, cross-source registration faces two core challenges: the lack of publicly available large-scale real-world datasets for training the deep registration models, and the inherent differences in point clouds captured by multiple sensors. The diverse patterns induced by the sensors pose great challenges in robust and accurate point cloud feature extraction and matching, which negatively influence the registration accuracy. To advance research in this field, we construct Cross3DReg, the currently largest and real-world multi-modal cross-source point cloud registration dataset, which is collected by a rotating mechanical lidar and a hybrid semi-solid-state lidar, respectively. Moreover, we design an overlap-based cross-source registration framework, which utilizes unaligned images to predict the overlapping region between source and target point clouds, effectively filtering out redundant points in the irrelevant regions and significantly mitigating the interference caused by noise in non-overlapping areas. Then, a visual-geometric attention guided matching module is proposed to enhance the consistency of cross-source point cloud features by fusing image and geometric information to establish reliable correspondences and ultimately achieve accurate and robust registration. Extensive experiments show that our method achieves state-of-the-art registration performance. Our framework reduces the relative rotation error (RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$, respectively, and improves the registration recall (RR) by $5.4\%$, which validates its effectiveness in achieving accurate cross-source registration. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06436">arXiv:2509.06436</a><span> [<a href="...2509.06436">pdf</a>, <a href="...2509.06436">ps</a>, <a href="...2509.06436">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Tree of Agents: Improving Long-Context Capabilities of <span class="search-hit mathjax">Large</span> Language Models through Multi-Perspective Reasoning </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Yu%2C+S">Song Yu</a>, <a href="...?searchtype=author&amp;query=Xu%2C+X">Xiaofei Xu</a>, <a href="...?searchtype=author&amp;query=Deng%2C+K">Ke Deng</a>, <a href="...?searchtype=author&amp;query=Li%2C+L">Li Li</a>, <a href="...?searchtype=author&amp;query=Tian%2C+L">Lin Tian</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06436v1-abstract-short"> Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limit… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06436v1-abstract-full"> Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">19 pages, 5 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06429">arXiv:2509.06429</a><span> [<a href="...2509.06429">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span></div></div><p class="title is-5 mathjax"> Analyzing the Instability of <span class="search-hit mathjax">Large</span> Language Models in Automated Bug Injection and Correction </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Er%2C+M+B">Mehmet Bilal Er</a>, <a href="...?searchtype=author&amp;query=%C4%B0lhan%2C+N">Nagehan İlhan</a>, <a href="...?searchtype=author&amp;query=Kuran%2C+U">Umut Kuran</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06429v1-abstract-short"> The use of Large Language Models (LLMs) in software engineering tasks is growing, especially in the areas of bug fixing and code generation. Nevertheless, these models often yield unstable results; when executed at different times with the same input, they can generate radically different code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly assessed, despite the fact that… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06429v1-abstract-full"> The use of Large Language Models (LLMs) in software engineering tasks is growing, especially in the areas of bug fixing and code generation. Nevertheless, these models often yield unstable results; when executed at different times with the same input, they can generate radically different code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly assessed, despite the fact that this instability has typically been discussed in the literature in relation to code generation. The purpose of this study is to look into how unstable an LLM like ChatGPT is when it comes to fixing code bugs. We examine the structural, syntactic, and functional variations among several fix recommendations made in response to the same prompt using code samples with various error types. Additionally, we assess how instability is affected by the temperature settings (0, 0.5, and 1) used for the model's deterministic operation. For a total of 20 problems in the experimental analysis, the model produced three fix suggestions at each temperature value, comparing nine distinct outputs for each problem. The Syntax Similarity and Output Equivalence Rate (OER) metrics were used to assess the outputs' structural and functional consistency. The results demonstrate that the model's outputs become much more unstable and variable as the temperature rises, with high temperatures showing especially high rates of functional failure. According to syntax similarity analyses, the suggested fixes show notable structural differences at high temperatures but are fairly similar at low temperatures. The purpose of this study is to provide important methodological insights into how LLM-based error correction systems can be applied more consistently in software development processes while also casting doubt on their dependability. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06400">arXiv:2509.06400</a><span> [<a href="...2509.06400">pdf</a>, <a href="...2509.06400">ps</a>, <a href="...2509.06400">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> 3DOF+Quantization: 3DGS quantization for <span class="search-hit mathjax">large</span> scenes with limited Degrees of Freedom </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Gendrin%2C+M">Matthieu Gendrin</a>, <a href="...?searchtype=author&amp;query=Pateux%2C+S">Stéphane Pateux</a>, <a href="...?searchtype=author&amp;query=Ladune%2C+T">Théo Ladune</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06400v1-abstract-short"> 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06400v1-abstract-full"> 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of camera (3 other degrees). On large scenes, though, the input views are acquired from a limited zone in space, and the reconstruction is valuable for novel views from the same zone, even if the scene itself is almost unlimited in size. We refer to this particular case as 3DoF+, meaning that the 3 degrees of freedom of camera position are limited to small offsets around the central position. Considering the problem of coordinate quantization, the impact of position error on the projection error in pixels is studied. It is shown that the projection error is proportional to the squared inverse distance of the point being projected. Consequently, a new quantization scheme based on spherical coordinates is proposed. Rate-distortion performance of the proposed method are illustrated on the well-known Garden scene. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> CORESA - COmpression et REprésentation des Signaux Audiovisuels, Institut National des Sciences Appliquées - Rennes [INSA Rennes], Nov 2024, Rennes, France </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06337">arXiv:2509.06337</a><span> [<a href="...2509.06337">pdf</a>, <a href="...2509.06337">ps</a>, <a href="...2509.06337">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span> Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhao%2C+J">Jianpeng Zhao</a>, <a href="...?searchtype=author&amp;query=Yuan%2C+C">Chenyu Yuan</a>, <a href="...?searchtype=author&amp;query=Luo%2C+W">Weiming Luo</a>, <a href="...?searchtype=author&amp;query=Xie%2C+H">Haoling Xie</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+G">Guangwei Zhang</a>, <a href="...?searchtype=author&amp;query=Quan%2C+S+J">Steven Jige Quan</a>, <a href="...?searchtype=author&amp;query=Yuan%2C+Z">Zixuan Yuan</a>, <a href="...?searchtype=author&amp;query=Wang%2C+P">Pengyang Wang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+D">Denghui Zhang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06337v1-abstract-short"> Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribut… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06337v1-abstract-full"> Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), to systematically evaluate the ability of LLMs to generate accurate and demographically coherent responses. In PAS, the model predicts missing attributes based on partial respondent profiles, whereas FAS involves generating complete synthetic datasets under both zero-context and context-enhanced conditions. We curate a comprehensive benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey Simulation), that spans 11 real-world public datasets across four sociological domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B) reveals consistent trends in prediction performance, highlights failure modes, and demonstrates how context and prompt design impact simulation fidelity. This work establishes a rigorous foundation for LLM-driven survey simulations, offering scalable and cost-effective tools for sociological research and policy evaluation. Our code and dataset are available at: https://github.com/dart-lab-research/LLM-S-Cube-Benchmark <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06324">arXiv:2509.06324</a><span> [<a href="...2509.06324">pdf</a>, <a href="...2509.06324">ps</a>, <a href="...2509.06324">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span></div></div><p class="title is-5 mathjax"> A Generic and Efficient Python Runtime Verification System and its <span class="search-hit mathjax">Large</span>-scale Evaluation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Shen%2C+Z">Zhuohang Shen</a>, <a href="...?searchtype=author&amp;query=Yaseen%2C+M">Mohammed Yaseen</a>, <a href="...?searchtype=author&amp;query=Silva%2C+D">Denini Silva</a>, <a href="...?searchtype=author&amp;query=Guan%2C+K">Kevin Guan</a>, <a href="...?searchtype=author&amp;query=Lee%2C+J">Junho Lee</a>, <a href="...?searchtype=author&amp;query=d%27Amorim%2C+M">Marcelo d'Amorim</a>, <a href="...?searchtype=author&amp;query=Legunsen%2C+O">Owolabi Legunsen</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06324v1-abstract-short"> Runtime verification (RV) now scales for testing thousands of open-source Java projects, helping find hundreds of bugs. The popular Python ecosystem could use such benefits. But, today's Python RV systems are limited to a domain or specification logic, or slow. We propose PyMOP, a generic, extensible, and efficient RV system for Python. PyMOP supports five logics, implements five existing monitori… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06324v1-abstract-full"> Runtime verification (RV) now scales for testing thousands of open-source Java projects, helping find hundreds of bugs. The popular Python ecosystem could use such benefits. But, today's Python RV systems are limited to a domain or specification logic, or slow. We propose PyMOP, a generic, extensible, and efficient RV system for Python. PyMOP supports five logics, implements five existing monitoring algorithms, ships with 73 API specs of Python and widely-used libraries, supports three instrumentation strategies, and users can easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we find mainly that (i) the default monitoring algorithm for Java is often not the fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were fixed by developers. PyMOP's generality and efficiency position it well as an excellent platform for the next advances on RV for Python. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">23 pages, 7 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06307">arXiv:2509.06307</a><span> [<a href="...2509.06307">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Can AI Make Energy Retrofit Decisions? An Evaluation of <span class="search-hit mathjax">Large</span> Language Models </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Shu%2C+L">Lei Shu</a>, <a href="...?searchtype=author&amp;query=Zhao%2C+D">Dong Zhao</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06307v1-abstract-short"> Conventional approaches to building energy retrofit decision making suffer from limited generalizability and low interpretability, hindering adoption in diverse residential contexts. With the growth of Smart and Connected Communities, generative AI, especially large language models (LLMs), may help by processing contextual information and producing practitioner readable recommendations. We evaluat… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06307v1-abstract-full"> Conventional approaches to building energy retrofit decision making suffer from limited generalizability and low interpretability, hindering adoption in diverse residential contexts. With the growth of Smart and Connected Communities, generative AI, especially large language models (LLMs), may help by processing contextual information and producing practitioner readable recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok, Llama, and Claude) on residential retrofit decisions under two objectives: maximizing CO2 reduction (technical) and minimizing payback period (sociotechnical). Performance is assessed on four dimensions: accuracy, consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49 US states. LLMs generate effective recommendations in many cases, reaching up to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning. Performance is stronger for the technical objective, while sociotechnical decisions are limited by economic trade offs and local context. Agreement across models is low, and higher performing models tend to diverge from others. LLMs are sensitive to location and building geometry but less sensitive to technology and occupant behavior. Most models show step by step, engineering style reasoning, but it is often simplified and lacks deeper contextual awareness. Overall, LLMs are promising assistants for energy retrofit decision making, but improvements in accuracy, consistency, and context handling are needed for reliable practice. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06295">arXiv:2509.06295</a><span> [<a href="...2509.06295">pdf</a>, <a href="...2509.06295">ps</a>, <a href="...2509.06295">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Econometrics">econ.EM</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation">stat.CO</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span></div></div><p class="title is-5 mathjax"> Largevars: An R Package for Testing <span class="search-hit mathjax">Large</span> VARs for the Presence of Cointegration </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Bykhovskaya%2C+A">Anna Bykhovskaya</a>, <a href="...?searchtype=author&amp;query=Gorin%2C+V">Vadim Gorin</a>, <a href="...?searchtype=author&amp;query=Kiss%2C+E">Eszter Kiss</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06295v1-abstract-short"> Cointegration is a property of multivariate time series that determines whether its non-stationary, growing components have a stationary linear combination. Largevars R package conducts a cointegration test for high-dimensional vector autoregressions of order k based on the large N, T asymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a modification of the Johansen likeliho… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06295v1-abstract-full"> Cointegration is a property of multivariate time series that determines whether its non-stationary, growing components have a stationary linear combination. Largevars R package conducts a cointegration test for high-dimensional vector autoregressions of order k based on the large N, T asymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a modification of the Johansen likelihood ratio test. In the absence of cointegration the test converges to the partial sum of the Airy_1 point process, an object arising in random matrix theory. The package and this article contain simulated quantiles of the first ten partial sums of the Airy_1 </span></p></li></ol></div></main></body></html>