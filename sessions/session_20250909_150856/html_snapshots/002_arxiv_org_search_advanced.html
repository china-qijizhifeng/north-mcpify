<!--
=== 网页自动化HTML快照 (已清理) ===
URL: https://arxiv.org/search/advanced?advanced=1&terms-0-operator=AND&terms-0-term=Large+Language+Model&terms-0-field=title&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2025&date-to_date=2025&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first
最后更新: 2025-09-09T15:09:42.214592
原始大小: 261 KB
清理后大小: 202 KB
压缩率: 22.6%
生成时间: 2025-09-09T15:09:44.318060

清理说明:
- 已删除: <script>、<style>、<noscript> 标签
- 已删除: style属性、onclick等事件属性
- 已删除: CSS样式定义
- 保留: id、class、name、type等选择器属性
- 保留: 文本内容和DOM结构
-->
<!DOCTYPE html>
<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><meta content="width=device-width, initial-scale=1" name="viewport"/><link href="...apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/><link href="...favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/><link href="...favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/><link href="...site.webmanifest" rel="manifest"/><link href="...safari-pinned-tab.svg" rel="mask-icon"/><link href="...favicon.ico" rel="shortcut icon"/><meta content="#b31b1b" name="msapplication-TileColor"/><meta content="images/icons/browserconfig.xml" name="msapplication-config"/><meta content="#b31b1b" name="theme-color"/><title>Advanced Search | arXiv e-print repository</title><link href="...arxivstyle.css" rel="stylesheet"/><link href="...bulma-tooltip.min.css" rel="stylesheet"/><link href="...search.css" rel="stylesheet"/></head><body><div><div id="MathJax_Hidden"><span class="MathJax MathJax_Processing" id="MathJax-Element-2-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-3-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-4-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-6-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-7-Frame" tabindex="0"></span><br/><span class="MathJax MathJax_Processing" id="MathJax-Element-8-Frame" tabindex="0"></span><br/></div></div><div id="MathJax_Message"></div><header><a class="is-sr-only" href="#main-container">Skip to main content</a><div class="attribution level is-marginless" role="banner"><div class="level-left"><a class="level-item" href="..."><img alt="Cornell University" aria-label="logo" src="...cornell-reduced-white-SMALL.svg"/></a></div><div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br/> the Simons Foundation, <a href="...ourmembers.html">member institutions</a>, and all contributors. <a href="...donate.html">Donate</a></span></p></div></div><div class="identity level is-marginless"><div class="level-left"><div class="level-item"><a aria-label="arxiv-logo" class="arxiv" href="..."><img alt="arxiv logo" aria-label="logo" src="...arxiv-logo-one-color-white.svg"/></a></div></div><div class="search-block level-right"><form action="https://arxiv.org/search" class="level-item mini-search" method="GET"><div class="field has-addons"><div class="control"><input aria-label="Search term or terms" class="input is-small" name="query" placeholder="Search..." type="text"/><p class="help"><a href="...help">Help</a> | <a href="...advanced">Advanced Search</a></p></div><div class="control"><div class="select is-small"><select aria-label="Field to search" name="searchtype"><option selected="selected" value="all">All fields</option><option value="title">Title</option><option value="author">Author</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select></div></div><input name="source" type="hidden" value="header"/><button class="button is-small is-cul-darker">Search</button></div></form></div></div><div class="container"><div aria-label="User menu" class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation"><a href="...login">Login</a></div></div></header><main class="container" id="main-container"><div class="level is-marginless"><div class="level-left"><h1 class="title is-clearfix"> Showing 1–50 of 6,455 results </h1></div><div class="level-right is-hidden-mobile"><span class="help"><a href="...releases">Search v0.5.6 released 2020-02-24</a></span></div></div><div class="content"><div class="columns"><div class="column is-two-thirds-tablet"><p>Query: <a href="...advanced?terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first">order: -announced_date_first; size: 50; date_range: from 2025-01-01 to 2025-12-31; include_cross_list: True; terms: AND title=Large Language Model</a></p><div class="buttons"><a class="button is-link" href="...advanced?terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first">Refine query</a><a class="button" href="...advanced">New search</a></div></div><div class="column is-one-third-tablet is-hidden-mobile"><p class="has-text-right"><a href="...?order=-announced_date_first&amp;size=50">Simple Search</a></p></div></div><div class="level breathe-horizontal"><div class="level-left"><form action="/search/advanced" method="GET"><div><input id="advanced" name="advanced" type="hidden" value="1"/><ul id="terms"><li><label for="terms-0">Terms-0</label><table id="terms-0"><tbody><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Large Language Model"/></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected="" value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option selected="" value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option value="all">All fields</option></select></td></tr></tbody></table></li></ul><table id="classification"><tbody><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected="" value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"/></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked="" id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"/><label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"/><label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></tbody></table><table id="date"><tbody><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"/><label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"/><label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"/><label for="date-filter_by-2">Specific year</label></li><li><input checked="" id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"/><label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""/></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value="2025"/></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value="2025"/></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked="" id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"/><label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"/><label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"/><label for="date-date_type-2">Announcement date</label></li></ul></td></tr></tbody></table><input id="include_older_versions" name="include_older_versions" type="checkbox" value="y"/><ul id="abstracts"><li><input checked="" id="abstracts-0" name="abstracts" type="radio" value="show"/><label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"/><label for="abstracts-1">Hide abstracts</label></li></ul></div><div class="box field is-grouped is-grouped-multiline level-item"><div class="control"><span class="select is-small"><select id="size" name="size"><option value="25">25</option><option selected="" value="50">50</option><option value="100">100</option><option value="200">200</option></select></span><label for="size">results per page</label>. </div><div class="control"><label for="order">Sort results by</label><span class="select is-small"><select id="order" name="order"><option selected="" value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select></span></div><div class="control"><button class="button is-small is-link">Go</button></div></div></form></div></div><nav aria-label="pagination" class="pagination is-small is-centered breathe-horizontal" role="navigation"><a class="pagination-previous is-invisible" href="">Previous </a><a class="pagination-next" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">Next </a><ul class="pagination-list"><li><a aria-label="Goto page 1" class="pagination-link is-current" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0">1 </a></li><li><a aria-current="page" aria-label="Page 2" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">2 </a></li><li><a aria-current="page" aria-label="Page 3" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100">3 </a></li><li><a aria-current="page" aria-label="Page 4" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150">4 </a></li><li><a aria-current="page" aria-label="Page 5" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200">5 </a></li><li><span class="pagination-ellipsis">…</span></li></ul></nav><ol class="breathe-horizontal" start="1"><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06949">arXiv:2509.06949</a><span> [<a href="...2509.06949">pdf</a>, <a href="...2509.06949">ps</a>, <a href="...2509.06949">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Revolutionizing Reinforcement Learning Framework for Diffusion <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Wang%2C+Y">Yinjie Wang</a>, <a href="...?searchtype=author&amp;query=Yang%2C+L">Ling Yang</a>, <a href="...?searchtype=author&amp;query=Li%2C+B">Bowen Li</a>, <a href="...?searchtype=author&amp;query=Tian%2C+Y">Ye Tian</a>, <a href="...?searchtype=author&amp;query=Shen%2C+K">Ke Shen</a>, <a href="...?searchtype=author&amp;query=Wang%2C+M">Mengdi Wang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06949v1-abstract-short"> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it ca… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06949v1-abstract-full"> We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Code and Models: https://github.com/Gen-Verse/dLLM-RL</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06858">arXiv:2509.06858</a><span> [<a href="...2509.06858">pdf</a>, <a href="...2509.06858">ps</a>, <a href="...2509.06858">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Adaptation and Self-Organizing Systems">nlin.AO</span></div></div><p class="title is-5 mathjax"> Disentangling Interaction and Bias Effects in Opinion Dynamics of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a class="" href="...?searchtype=author&amp;query=Brockers%2C+V+C">Vincent C. Brockers</a>, <a href="...?searchtype=author&amp;query=Ehrlich%2C+D+A">David A. Ehrlich</a>, <a href="...?searchtype=author&amp;query=Priesemann%2C+V">Viola Priesemann</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06858v1-abstract-short"> Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias to… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06858v1-abstract-full"> Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias toward the initiating agent's stance. Applying this framework to multi-step dialogues reveals that opinion trajectories tend to quickly converge to a shared attractor, with the influence of the interaction fading over time, and the impact of biases differing between LLMs. In addition, we fine-tune an LLM on different sets of strongly opinionated statements (incl. misinformation) and demonstrate that the opinion attractor shifts correspondingly. Exposing stark differences between LLMs and providing quantitative tools to compare them to human subjects in the future, our approach highlights both chances and pitfalls in using LLMs as proxies for human behavior. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06838">arXiv:2509.06838</a><span> [<a href="...2509.06838">pdf</a>, <a href="...2509.06838">ps</a>, <a href="...2509.06838">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span></div></div><p class="title is-5 mathjax"> EPT Benchmark: Evaluation of Persian Trustworthiness in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Mirbagheri%2C+M+R">Mohammad Reza Mirbagheri</a>, <a href="...?searchtype=author&amp;query=Mirkamali%2C+M+M">Mohammad Mahdi Mirkamali</a>, <a href="...?searchtype=author&amp;query=Arani%2C+Z+M">Zahra Motoshaker Arani</a>, <a href="...?searchtype=author&amp;query=Javeri%2C+A">Ali Javeri</a>, <a href="...?searchtype=author&amp;query=Sadeghzadeh%2C+A+M">Amir Mahdi Sadeghzadeh</a>, <a href="...?searchtype=author&amp;query=Jalili%2C+R">Rasool Jalili</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06838v1-abstract-short"> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cu… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06838v1-abstract-full"> Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06813">arXiv:2509.06813</a><span> [<a href="...2509.06813">pdf</a>, <a href="...2509.06813">ps</a>, <a href="...2509.06813">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> A Comparative Benchmark of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> for Labelling Wind Turbine Maintenance Logs </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Malyi%2C+M">Max Malyi</a>, <a href="...?searchtype=author&amp;query=Shek%2C+J">Jonathan Shek</a>, <a href="...?searchtype=author&amp;query=McDonald%2C+A">Alasdair McDonald</a>, <a href="...?searchtype=author&amp;query=Biscaya%2C+A">Andre Biscaya</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06813v1-abstract-short"> Effective Operation and Maintenance (O&amp;M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06813v1-abstract-full"> Effective Operation and Maintenance (O&amp;M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&amp;M data quality and downstream reliability analysis. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Associated GitHub repository: https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06759">arXiv:2509.06759</a><span> [<a href="...2509.06759">pdf</a>, <a href="...2509.06759">ps</a>, <a href="...2509.06759">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Aligning <span class="search-hit mathjax">Large</span> Vision-<span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> by Deep Reinforcement Learning and Direct Preference Optimization </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nguyen%2C+T+T">Thanh Thi Nguyen</a>, <a href="...?searchtype=author&amp;query=Wilson%2C+C">Campbell Wilson</a>, <a href="...?searchtype=author&amp;query=Dalins%2C+J">Janis Dalins</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06759v1-abstract-short"> Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remain… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06759v1-abstract-full"> Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remains a critical challenge. Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) offer promising frameworks for this aligning process. While DRL enables models to optimize actions using reward signals instead of relying solely on supervised preference data, DPO directly aligns the policy with preferences, eliminating the need for an explicit reward model. This overview explores paradigms for fine-tuning LVLMs, highlighting how DRL and DPO techniques can be used to align models with human preferences and values, improve task performance, and enable adaptive multimodal interaction. We categorize key approaches, examine sources of preference data, reward signals, and discuss open challenges such as scalability, sample efficiency, continual learning, generalization, and safety. The goal is to provide a clear understanding of how DRL and DPO contribute to the evolution of robust and human-aligned LVLMs. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Accepted for publication in the Proceedings of the 8th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06596">arXiv:2509.06596</a><span> [<a href="...2509.06596">pdf</a>, <a href="...2509.06596">ps</a>, <a href="...2509.06596">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Tong%2C+X">Xin Tong</a>, <a href="...?searchtype=author&amp;query=Lin%2C+Z">Zhi Lin</a>, <a href="...?searchtype=author&amp;query=Wang%2C+J">Jingya Wang</a>, <a href="...?searchtype=author&amp;query=Jin%2C+B">Bo Jin</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06596v1-abstract-short"> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that d… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06596v1-abstract-full"> Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06436">arXiv:2509.06436</a><span> [<a href="...2509.06436">pdf</a>, <a href="...2509.06436">ps</a>, <a href="...2509.06436">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Tree of Agents: Improving Long-Context Capabilities of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> through Multi-Perspective Reasoning </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Yu%2C+S">Song Yu</a>, <a href="...?searchtype=author&amp;query=Xu%2C+X">Xiaofei Xu</a>, <a href="...?searchtype=author&amp;query=Deng%2C+K">Ke Deng</a>, <a href="...?searchtype=author&amp;query=Li%2C+L">Li Li</a>, <a href="...?searchtype=author&amp;query=Tian%2C+L">Lin Tian</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06436v1-abstract-short"> Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limit… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06436v1-abstract-full"> Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">19 pages, 5 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06429">arXiv:2509.06429</a><span> [<a href="...2509.06429">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span></div></div><p class="title is-5 mathjax"> Analyzing the Instability of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> in Automated Bug Injection and Correction </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Er%2C+M+B">Mehmet Bilal Er</a>, <a href="...?searchtype=author&amp;query=%C4%B0lhan%2C+N">Nagehan İlhan</a>, <a href="...?searchtype=author&amp;query=Kuran%2C+U">Umut Kuran</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06429v1-abstract-short"> The use of Large Language Models (LLMs) in software engineering tasks is growing, especially in the areas of bug fixing and code generation. Nevertheless, these models often yield unstable results; when executed at different times with the same input, they can generate radically different code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly assessed, despite the fact that… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06429v1-abstract-full"> The use of Large Language Models (LLMs) in software engineering tasks is growing, especially in the areas of bug fixing and code generation. Nevertheless, these models often yield unstable results; when executed at different times with the same input, they can generate radically different code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly assessed, despite the fact that this instability has typically been discussed in the literature in relation to code generation. The purpose of this study is to look into how unstable an LLM like ChatGPT is when it comes to fixing code bugs. We examine the structural, syntactic, and functional variations among several fix recommendations made in response to the same prompt using code samples with various error types. Additionally, we assess how instability is affected by the temperature settings (0, 0.5, and 1) used for the model's deterministic operation. For a total of 20 problems in the experimental analysis, the model produced three fix suggestions at each temperature value, comparing nine distinct outputs for each problem. The Syntax Similarity and Output Equivalence Rate (OER) metrics were used to assess the outputs' structural and functional consistency. The results demonstrate that the model's outputs become much more unstable and variable as the temperature rises, with high temperatures showing especially high rates of functional failure. According to syntax similarity analyses, the suggested fixes show notable structural differences at high temperatures but are fairly similar at low temperatures. The purpose of this study is to provide important methodological insights into how LLM-based error correction systems can be applied more consistently in software development processes while also casting doubt on their dependability. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06337">arXiv:2509.06337</a><span> [<a href="...2509.06337">pdf</a>, <a href="...2509.06337">ps</a>, <a href="...2509.06337">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhao%2C+J">Jianpeng Zhao</a>, <a href="...?searchtype=author&amp;query=Yuan%2C+C">Chenyu Yuan</a>, <a href="...?searchtype=author&amp;query=Luo%2C+W">Weiming Luo</a>, <a href="...?searchtype=author&amp;query=Xie%2C+H">Haoling Xie</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+G">Guangwei Zhang</a>, <a href="...?searchtype=author&amp;query=Quan%2C+S+J">Steven Jige Quan</a>, <a href="...?searchtype=author&amp;query=Yuan%2C+Z">Zixuan Yuan</a>, <a href="...?searchtype=author&amp;query=Wang%2C+P">Pengyang Wang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+D">Denghui Zhang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06337v1-abstract-short"> Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribut… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06337v1-abstract-full"> Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), to systematically evaluate the ability of LLMs to generate accurate and demographically coherent responses. In PAS, the model predicts missing attributes based on partial respondent profiles, whereas FAS involves generating complete synthetic datasets under both zero-context and context-enhanced conditions. We curate a comprehensive benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey Simulation), that spans 11 real-world public datasets across four sociological domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B) reveals consistent trends in prediction performance, highlights failure modes, and demonstrates how context and prompt design impact simulation fidelity. This work establishes a rigorous foundation for LLM-driven survey simulations, offering scalable and cost-effective tools for sociological research and policy evaluation. Our code and dataset are available at: https://github.com/dart-lab-research/LLM-S-Cube-Benchmark <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06307">arXiv:2509.06307</a><span> [<a href="...2509.06307">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Can AI Make Energy Retrofit Decisions? An Evaluation of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Shu%2C+L">Lei Shu</a>, <a href="...?searchtype=author&amp;query=Zhao%2C+D">Dong Zhao</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06307v1-abstract-short"> Conventional approaches to building energy retrofit decision making suffer from limited generalizability and low interpretability, hindering adoption in diverse residential contexts. With the growth of Smart and Connected Communities, generative AI, especially large language models (LLMs), may help by processing contextual information and producing practitioner readable recommendations. We evaluat… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06307v1-abstract-full"> Conventional approaches to building energy retrofit decision making suffer from limited generalizability and low interpretability, hindering adoption in diverse residential contexts. With the growth of Smart and Connected Communities, generative AI, especially large language models (LLMs), may help by processing contextual information and producing practitioner readable recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok, Llama, and Claude) on residential retrofit decisions under two objectives: maximizing CO2 reduction (technical) and minimizing payback period (sociotechnical). Performance is assessed on four dimensions: accuracy, consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49 US states. LLMs generate effective recommendations in many cases, reaching up to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning. Performance is stronger for the technical objective, while sociotechnical decisions are limited by economic trade offs and local context. Agreement across models is low, and higher performing models tend to diverge from others. LLMs are sensitive to location and building geometry but less sensitive to technology and occupant behavior. Most models show step by step, engineering style reasoning, but it is often simplified and lacks deeper contextual awareness. Overall, LLMs are promising assistants for energy retrofit decision making, but improvements in accuracy, consistency, and context handling are needed for reliable practice. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06164">arXiv:2509.06164</a><span> [<a href="...2509.06164">pdf</a>, <a href="...2509.06164">ps</a>, <a href="...2509.06164">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"> Benchmarking Gender and Political Bias in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Yang%2C+J">Jinrui Yang</a>, <a href="...?searchtype=author&amp;query=Han%2C+X">Xudong Han</a>, <a href="...?searchtype=author&amp;query=Baldwin%2C+T">Timothy Baldwin</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06164v1-abstract-short"> We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tas… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06164v1-abstract-full"> We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tasks -- gender classification and vote prediction -- revealing consistent patterns of bias. We find that LLMs frequently misclassify female MEPs as male and demonstrate reduced accuracy when simulating votes for female speakers. Politically, LLMs tend to favor centrist groups while underperforming on both far-left and far-right ones. Proprietary models like GPT-4o outperform open-weight alternatives in terms of both robustness and fairness. We release the EuroParlVote dataset, code, and demo to support future research on fairness and accountability in NLP within political contexts. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">The 8th International Conference on Natural Language and Speech Processing (Oral)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06100">arXiv:2509.06100</a><span> [<a href="...2509.06100">pdf</a>, <a href="...2509.06100">ps</a>, <a href="...2509.06100">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Cao%2C+K">Kefan Cao</a>, <a href="...?searchtype=author&amp;query=Wu%2C+S">Shuaicheng Wu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06100v1-abstract-short"> Large language models (LLMs) are prone to catastrophic forgetting in sequential multi-task settings. Parameter regularization methods such as O-LoRA and N-LoRA alleviate task interference by enforcing low-rank subspace orthogonality, but they overlook the fact that conventional additive fine-tuning disrupts the intrinsic geometric structure of LLM parameters, limiting performance. Our key insight… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06100v1-abstract-full"> Large language models (LLMs) are prone to catastrophic forgetting in sequential multi-task settings. Parameter regularization methods such as O-LoRA and N-LoRA alleviate task interference by enforcing low-rank subspace orthogonality, but they overlook the fact that conventional additive fine-tuning disrupts the intrinsic geometric structure of LLM parameters, limiting performance. Our key insight is that the parameter space of LLMs possesses a geometric structure, which must be preserved in addition to enforcing orthogonality. Based on this, we propose Orthogonal Low-rank Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM fine-tuning: leveraging multiplicative updates to preserve parameter geometry while applying orthogonality constraints to task subspaces. Experiments demonstrate that OLieRA achieves state-of-the-art results on the Standard CL benchmark and remains among the top-performing methods in the Large Number of Tasks setting. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">13 pages, 3 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06093">arXiv:2509.06093</a><span> [<a href="...2509.06093">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Language</span> Native Lightly Structured Databases for <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Model</span> Driven Composite Materials Research </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Liu%2C+Y">Yuze Liu</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Z">Zhaoyuan Zhang</a>, <a href="...?searchtype=author&amp;query=Zeng%2C+X">Xiangsheng Zeng</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yihe Zhang</a>, <a href="...?searchtype=author&amp;query=Yu%2C+L">Leping Yu</a>, <a href="...?searchtype=author&amp;query=Wang%2C+L">Lejia Wang</a>, <a href="...?searchtype=author&amp;query=Yu%2C+X">Xi Yu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06093v1-abstract-short"> Chemical and materials research has traditionally relied heavily on knowledge narrative, with progress often driven by language-based descriptions of principles, mechanisms, and experimental experiences, rather than tables, limiting what conventional databases and ML can exploit. We present a language-native database for boron nitride nanosheet (BNNS) polymer thermally conductive composites that c… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06093v1-abstract-full"> Chemical and materials research has traditionally relied heavily on knowledge narrative, with progress often driven by language-based descriptions of principles, mechanisms, and experimental experiences, rather than tables, limiting what conventional databases and ML can exploit. We present a language-native database for boron nitride nanosheet (BNNS) polymer thermally conductive composites that captures lightly structured information from papers across preparation, characterization, theory-computation, and mechanistic reasoning, with evidence-linked snippets. Records are organized in a heterogeneous database and queried via composite retrieval with semantics, key words and value filters. The system can synthesizes literature into accurate, verifiable, and expert style guidance. This substrate enables high fidelity efficient Retrieval Augmented Generation (RAG) and tool augmented agents to interleave retrieval with reasoning and deliver actionable SOP. The framework supplies the language rich foundation required for LLM-driven materials discovery. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06065">arXiv:2509.06065</a><span> [<a href="...2509.06065">pdf</a>, <a href="...2509.06065">ps</a>, <a href="...2509.06065">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> KatotohananQA: Evaluating Truthfulness of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> in Filipino </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nery%2C+L+A">Lorenzo Alfred Nery</a>, <a href="...?searchtype=author&amp;query=Catignas%2C+R+D">Ronald Dawson Catignas</a>, <a href="...?searchtype=author&amp;query=Tiam-Lee%2C+T+J">Thomas James Tiam-Lee</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06065v1-abstract-short"> Large Language Models (LLMs) achieve remarkable performance across various tasks, but their tendency to produce hallucinations limits reliable adoption. Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet they are primarily available in English, leaving a gap in evaluating LLMs in low-resource languages. To address this, we present KatotohananQA, a Filipino translation o… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06065v1-abstract-full"> Large Language Models (LLMs) achieve remarkable performance across various tasks, but their tendency to produce hallucinations limits reliable adoption. Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet they are primarily available in English, leaving a gap in evaluating LLMs in low-resource languages. To address this, we present KatotohananQA, a Filipino translation of the TruthfulQA benchmark. Seven free-tier proprietary models were assessed using a binary-choice framework. Findings show a significant performance gap between English and Filipino truthfulness, with newer OpenAI models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness. Results also reveal disparities across question characteristics, suggesting that some question types, categories, and topics are less robust to multilingual transfer which highlight the need for broader multilingual evaluation to ensure fairness and reliability in LLM usage. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">14 pages, 1 figure, 9 tables, 1 listing. To appear in Proceedings of NLPIR 2025</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06052">arXiv:2509.06052</a><span> [<a href="...2509.06052">pdf</a>, <a href="...2509.06052">ps</a>, <a href="...2509.06052">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span></div></div><p class="title is-5 mathjax"> Empirical Study of Code <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> for Binary Security Patch Detection </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Li%2C+Q">Qingyuan Li</a>, <a href="...?searchtype=author&amp;query=Li%2C+B">Binchang Li</a>, <a href="...?searchtype=author&amp;query=Gao%2C+C">Cuiyun Gao</a>, <a href="...?searchtype=author&amp;query=Gao%2C+S">Shuzheng Gao</a>, <a href="...?searchtype=author&amp;query=Li%2C+Z">Zongjie Li</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06052v1-abstract-short"> Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of re… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06052v1-abstract-full"> Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of real-world software, as they release patches only with binary files, and the source code is inaccessible. Given the impressive performance of code large language models (LLMs) in code intelligence and binary analysis tasks such as decompilation and compilation optimization, their potential for detecting binary security patches remains unexplored, exposing a significant research gap between their demonstrated low-level code understanding capabilities and this critical security task. To address this gap, we construct a large-scale binary patch dataset containing \textbf{19,448} samples, with two levels of representation: assembly code and pseudo-code, and systematically evaluate \textbf{19} code LLMs of varying scales to investigate their capability in binary SPD tasks. Our initial exploration demonstrates that directly prompting vanilla code LLMs struggles to accurately identify security patches from binary patches, and even state-of-the-art prompting techniques fail to mitigate the lack of domain knowledge in binary SPD within vanilla models. Drawing on the initial findings, we further investigate the fine-tuning strategy for injecting binary SPD domain knowledge into code LLMs through two levels of representation. Experimental results demonstrate that fine-tuned LLMs achieve outstanding performance, with the best results obtained on the pseudo-code representation. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.06024">arXiv:2509.06024</a><span> [<a href="...2509.06024">pdf</a>, <a href="...2509.06024">ps</a>, <a href="...2509.06024">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Rethinking Reasoning Quality in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> through Enhanced Chain-of-Thought via RL </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=He%2C+H">Haoyang He</a>, <a href="...?searchtype=author&amp;query=Rong%2C+Z">Zihua Rong</a>, <a href="...?searchtype=author&amp;query=Ji%2C+K">Kun Ji</a>, <a href="...?searchtype=author&amp;query=Li%2C+C">Chenyang Li</a>, <a href="...?searchtype=author&amp;query=Huang%2C+Q">Qing Huang</a>, <a href="...?searchtype=author&amp;query=Xia%2C+C">Chong Xia</a>, <a href="...?searchtype=author&amp;query=Yang%2C+L">Lan Yang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+H">Honggang Zhang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.06024v1-abstract-short"> Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such tas… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.06024v1-abstract-full"> Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such task-specific training offers limited control over logical depth and therefore may fail to reveal a model's genuine reasoning capacity. We propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward framework that reshapes both reward and advantage signals. (i) A Reasoning Quality Reward assigns fine-grained credit to those reasoning chains that demonstrably raise the likelihood of the correct answer, directly incentivising the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage decays the advantage of responses whose length deviates from a validation-derived threshold, stabilising training. To facilitate rigorous assessment, we also release Logictree, a dynamically constructed deductive reasoning dataset that functions both as RL training data and as a comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B model attains GPT-o3-mini level performance on Logictree with 400 trianing steps, while the average confidence of CoT-augmented answers rises by 30%. The model further exhibits generalisation across diverse logical-reasoning datasets, and the mathematical benchmark AIME24. These results illuminate how RL shapes CoT behaviour and chart a practical path toward enhancing formal-reasoning skills in large language models. All code and data are available in repository https://github.com/Henryhe09/DRER. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05946">arXiv:2509.05946</a><span> [<a href="...2509.05946">pdf</a>, <a href="...2509.05946">ps</a>, <a href="...2509.05946">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> for Next-Generation Wireless Network Management: A Survey and Tutorial </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Wei%2C+B">Bisheng Wei</a>, <a href="...?searchtype=author&amp;query=Jiang%2C+R">Ruihong Jiang</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+R">Ruichen Zhang</a>, <a href="...?searchtype=author&amp;query=Liu%2C+Y">Yinqiu Liu</a>, <a href="...?searchtype=author&amp;query=Niyato%2C+D">Dusit Niyato</a>, <a href="...?searchtype=author&amp;query=Sun%2C+Y">Yaohua Sun</a>, <a href="...?searchtype=author&amp;query=Lu%2C+Y">Yang Lu</a>, <a href="...?searchtype=author&amp;query=Li%2C+Y">Yonghui Li</a>, <a href="...?searchtype=author&amp;query=Mao%2C+S">Shiwen Mao</a>, <a href="...?searchtype=author&amp;query=Yuen%2C+C">Chau Yuen</a>, <a href="...?searchtype=author&amp;query=Di+Renzo%2C+M">Marco Di Renzo</a>, <a href="...?searchtype=author&amp;query=Peng%2C+M">Mugen Peng</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05946v1-abstract-short"> The rapid advancement toward sixth-generation (6G) wireless networks has significantly intensified the complexity and scale of optimization problems, including resource allocation and trajectory design, often formulated as combinatorial problems in large discrete decision spaces. However, traditional optimization methods, such as heuristics and deep reinforcement learning (DRL), struggle to meet t… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05946v1-abstract-full"> The rapid advancement toward sixth-generation (6G) wireless networks has significantly intensified the complexity and scale of optimization problems, including resource allocation and trajectory design, often formulated as combinatorial problems in large discrete decision spaces. However, traditional optimization methods, such as heuristics and deep reinforcement learning (DRL), struggle to meet the demanding requirements of real-time adaptability, scalability, and dynamic handling of user intents in increasingly heterogeneous and resource-constrained network environments. Large language models (LLMs) present a transformative paradigm by enabling natural language-driven problem formulation, context-aware reasoning, and adaptive solution refinement through advanced semantic understanding and structured reasoning capabilities. This paper provides a systematic and comprehensive survey of LLM-enabled optimization frameworks tailored for wireless networks. We first introduce foundational design concepts and distinguish LLM-enabled methods from conventional optimization paradigms. Subsequently, we critically analyze key enabling methodologies, including natural language modeling, solver collaboration, and solution verification processes. Moreover, we explore representative case studies to demonstrate LLMs' transformative potential in practical scenarios such as optimization formulation, low-altitude economy networking, and intent networking. Finally, we discuss current research challenges, examine prominent open-source frameworks and datasets, and identify promising future directions to facilitate robust, scalable, and trustworthy LLM-enabled optimization solutions for next-generation wireless networks. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05921">arXiv:2509.05921</a><span> [<a href="...2509.05921">pdf</a>, <a href="...2509.05921">ps</a>, <a href="...2509.05921">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span></div></div><p class="title is-5 mathjax"> Dataset Ownership in the Era of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Li%2C+K">Kun Li</a>, <a href="...?searchtype=author&amp;query=Wang%2C+C">Cheng Wang</a>, <a href="...?searchtype=author&amp;query=Xu%2C+M">Minghui Xu</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yue Zhang</a>, <a href="...?searchtype=author&amp;query=Cheng%2C+X">Xiuzhen Cheng</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05921v1-abstract-short"> As datasets become critical assets in modern machine learning systems, ensuring robust copyright protection has emerged as an urgent challenge. Traditional legal mechanisms often fail to address the technical complexities of digital data replication and unauthorized use, particularly in opaque or decentralized environments. This survey provides a comprehensive review of technical approaches for da… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05921v1-abstract-full"> As datasets become critical assets in modern machine learning systems, ensuring robust copyright protection has emerged as an urgent challenge. Traditional legal mechanisms often fail to address the technical complexities of digital data replication and unauthorized use, particularly in opaque or decentralized environments. This survey provides a comprehensive review of technical approaches for dataset copyright protection, systematically categorizing them into three main classes: non-intrusive methods, which detect unauthorized use without modifying data; minimally-intrusive methods, which embed lightweight, reversible changes to enable ownership verification; and maximally-intrusive methods, which apply aggressive data alterations, such as reversible adversarial examples, to enforce usage restrictions. We synthesize key techniques, analyze their strengths and limitations, and highlight open research challenges. This work offers an organized perspective on the current landscape and suggests future directions for developing unified, scalable, and ethically sound solutions to protect datasets in increasingly complex machine learning ecosystems. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">15 pages, 1 table, accepted by the 2025 International Conference on Blockchain and Web3.0 Technology Innovation and Application Exchange (BWTAC)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05915">arXiv:2509.05915</a><span> [<a href="...2509.05915">pdf</a>, <a href="...2509.05915">ps</a>, <a href="...2509.05915">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Accelerating <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Model</span> Inference via Early-Exiting Algorithms </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Bae%2C+S">Sangmin Bae</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05915v1-abstract-short"> Large language models have achieved remarkable capabilities, but their practical deployment is hindered by significant computational costs. While adaptive computation methods like early-exiting promise to reduce these costs, they introduce a fundamental conflict: the per-token dynamism intended to save computation often creates system-level bottlenecks that can paradoxically reduce throughput in b… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05915v1-abstract-full"> Large language models have achieved remarkable capabilities, but their practical deployment is hindered by significant computational costs. While adaptive computation methods like early-exiting promise to reduce these costs, they introduce a fundamental conflict: the per-token dynamism intended to save computation often creates system-level bottlenecks that can paradoxically reduce throughput in batched inference. This dissertation resolves this conflict by co-designing adaptive algorithms and model architectures to strike an optimal balance between dynamism and efficiency. To this end, our work first addresses critical sources of overhead in conventional early-exiting by proposing an efficient parallel decoding mechanism. We then show that deep parameter sharing provides an architectural foundation that not only yields compact, parameter-efficient models but also inherently mitigates the critical synchronization issues affecting dynamic inference. Finally, this work presents a unified framework where lightweight routers are pretrained to dynamically assign an optimal recursion depth for each token. This approach establishes a new Pareto frontier between efficiency and performance by effectively optimizing for both adaptive computation and parameter efficiency within a single model. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">PhD Dissertation</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05895">arXiv:2509.05895</a><span> [<a href="...2509.05895">pdf</a>, <a href="...2509.05895">ps</a>, <a href="...2509.05895">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Model</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Li%2C+Y">Yujie Li</a>, <a href="...?searchtype=author&amp;query=Xu%2C+W">Wenjia Xu</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yuanben Zhang</a>, <a href="...?searchtype=author&amp;query=Wei%2C+Z">Zhiwei Wei</a>, <a href="...?searchtype=author&amp;query=Peng%2C+M">Mugen Peng</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05895v1-abstract-short"> Bi-temporal satellite imagery supports critical applications such as urban development monitoring and disaster assessment. Although powerful multimodal large language models (MLLMs) have been applied in bi-temporal change analysis, previous methods process image pairs through direct concatenation, inadequately modeling temporal correlations and spatial semantic changes. This deficiency hampers vis… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05895v1-abstract-full"> Bi-temporal satellite imagery supports critical applications such as urban development monitoring and disaster assessment. Although powerful multimodal large language models (MLLMs) have been applied in bi-temporal change analysis, previous methods process image pairs through direct concatenation, inadequately modeling temporal correlations and spatial semantic changes. This deficiency hampers visual-semantic alignment in change understanding, thereby constraining the overall effectiveness of current approaches. To address this gap, we propose BTCChat, a multi-temporal MLLM with advanced bi-temporal change understanding capability. BTCChat supports bi-temporal change captioning and retains single-image interpretation capability. To better capture temporal features and spatial semantic changes in image pairs, we design a Change Extraction module. Moreover, to enhance the model's attention to spatial details, we introduce a Prompt Augmentation mechanism, which incorporates contextual clues into the prompt to enhance model performance. Experimental results demonstrate that BTCChat achieves state-of-the-art performance on change captioning and visual question answering tasks. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">5 pages, 2 figures Submitted to ICASSP 2026</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05881">arXiv:2509.05881</a><span> [<a href="...2509.05881">pdf</a>, <a href="...2509.05881">ps</a>, <a href="...2509.05881">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> GeoAnalystBench: A GeoAI benchmark for assessing <span class="search-hit mathjax">large</span><span class="search-hit mathjax">language</span><span class="search-hit mathjax">models</span> for spatial analysis workflow and code generation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhang%2C+Q">Qianheng Zhang</a>, <a href="...?searchtype=author&amp;query=Gao%2C+S">Song Gao</a>, <a href="...?searchtype=author&amp;query=Wei%2C+C">Chen Wei</a>, <a href="...?searchtype=author&amp;query=Zhao%2C+Y">Yibo Zhao</a>, <a href="...?searchtype=author&amp;query=Nie%2C+Y">Ying Nie</a>, <a href="...?searchtype=author&amp;query=Chen%2C+Z">Ziru Chen</a>, <a href="...?searchtype=author&amp;query=Chen%2C+S">Shijie Chen</a>, <a href="...?searchtype=author&amp;query=Su%2C+Y">Yu Su</a>, <a href="...?searchtype=author&amp;query=Sun%2C+H">Huan Sun</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05881v1-abstract-short"> Recent advances in large language models (LLMs) have fueled growing interest in automating geospatial analysis and GIS workflows, yet their actual capabilities remain uncertain. In this work, we call for rigorous evaluation of LLMs on well-defined geoprocessing tasks before making claims about full GIS automation. To this end, we present GeoAnalystBench, a benchmark of 50 Python-based tasks derive… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05881v1-abstract-full"> Recent advances in large language models (LLMs) have fueled growing interest in automating geospatial analysis and GIS workflows, yet their actual capabilities remain uncertain. In this work, we call for rigorous evaluation of LLMs on well-defined geoprocessing tasks before making claims about full GIS automation. To this end, we present GeoAnalystBench, a benchmark of 50 Python-based tasks derived from real-world geospatial problems and carefully validated by GIS experts. Each task is paired with a minimum deliverable product, and evaluation covers workflow validity, structural alignment, semantic similarity, and code quality (CodeBLEU). Using this benchmark, we assess both proprietary and open source models. Results reveal a clear gap: proprietary models such as ChatGPT-4o-mini achieve high validity 95% and stronger code alignment (CodeBLEU 0.39), while smaller open source models like DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5% validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as spatial relationship detection or optimal site selection, remain the most challenging across all models. These findings demonstrate both the promise and limitations of current LLMs in GIS automation and provide a reproducible framework to advance GeoAI research with human-in-the-loop support. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">34 pages, 8 figures</span></p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span> I.2 </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> Transactions in GIS, 2025 </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05852">arXiv:2509.05852</a><span> [<a href="...2509.05852">pdf</a>, <a href="...2509.05852">ps</a>, <a href="...2509.05852">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistics Theory">math.ST</span></div></div><p class="title is-5 mathjax"> Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Model</span> Evaluation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yichi Zhang</a>, <a href="...?searchtype=author&amp;query=Belloni%2C+A">Alexander Belloni</a>, <a href="...?searchtype=author&amp;query=Fang%2C+E+X">Ethan X. Fang</a>, <a href="...?searchtype=author&amp;query=Lu%2C+J">Junwei Lu</a>, <a href="...?searchtype=author&amp;query=Xu%2C+X">Xiaoan Xu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05852v1-abstract-short"> Motivated by the need for rigorous and scalable evaluation of large language models, we study contextual preference inference for pairwise comparison functionals of context-dependent preference score functions across domains. Focusing on the contextual Bradley-Terry-Luce model, we develop a semiparametric efficient estimator that automates the debiased estimation through aggregating weighted resid… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05852v1-abstract-full"> Motivated by the need for rigorous and scalable evaluation of large language models, we study contextual preference inference for pairwise comparison functionals of context-dependent preference score functions across domains. Focusing on the contextual Bradley-Terry-Luce model, we develop a semiparametric efficient estimator that automates the debiased estimation through aggregating weighted residual balancing terms across the comparison graph. We show that the efficiency is achieved when the weights are derived from a novel strategy called Fisher random walk. We also propose a computationally feasible method to compute the weights by a potential representation of nuisance weight functions. We show our inference procedure is valid for general score function estimators accommodating the practitioners' need to implement flexible deep learning methods. We extend the procedure to multiple hypothesis testing using a Gaussian multiplier bootstrap that controls familywise error and to distributional shift via a cross-fitted importance-sampling adjustment for target-domain inference. Numerical studies, including language model evaluations under diverse contexts, corroborate the accuracy, efficiency, and practical utility of our method. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05757">arXiv:2509.05757</a><span> [<a href="...2509.05757">pdf</a>, <a href="...2509.05757">ps</a>, <a href="...2509.05757">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Hyperbolic <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Patil%2C+S">Sarang Patil</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Z">Zeyong Zhang</a>, <a href="...?searchtype=author&amp;query=Huang%2C+Y">Yiran Huang</a>, <a href="...?searchtype=author&amp;query=Ma%2C+T">Tengfei Ma</a>, <a href="...?searchtype=author&amp;query=Xu%2C+M">Mengjia Xu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05757v1-abstract-short"> Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation net… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05757v1-abstract-full"> Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation networks, financial networks, brain networks, and linguistic structures or syntactic trees in natural languages. Effectively learning intrinsic semantic entailment and hierarchical relationships from these raw, unstructured input data using LLMs remains an underexplored area. Due to its effectiveness in modeling tree-like hierarchical structures, hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity as an expressive latent representation space for complex data modeling across domains such as graphs, images, languages, and multi-modal data. Here, we provide a comprehensive and contextual exposition of recent advancements in LLMs that leverage hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning. Specifically, the paper presents a taxonomy of the principal techniques of Hyperbolic LLMs (HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4) hyperbolic state-space models. We also explore crucial potential applications and outline future research directions. A repository of key papers, models, datasets, and code implementations is available at https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">32 pages, 6 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05695">arXiv:2509.05695</a><span> [<a href="...2509.05695">pdf</a>, <a href="...2509.05695">ps</a>, <a href="...2509.05695">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> Leveraging Vision-<span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Models</span> for Interpretable Video Action Recognition with Semantic Tokenization </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Peng%2C+J">Jingwei Peng</a>, <a href="...?searchtype=author&amp;query=Qiu%2C+Z">Zhixuan Qiu</a>, <a href="...?searchtype=author&amp;query=Jin%2C+B">Boyu Jin</a>, <a href="...?searchtype=author&amp;query=Siripong%2C+S">Surasakdi Siripong</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05695v1-abstract-short"> Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05695v1-abstract-full"> Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision-Language Large Models (LVLMs) to video action recognition, emphasizing enhanced accuracy and interpretability. Our method features a Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video sequences into discrete, semantically and temporally consistent "semantic action tokens," effectively crafting an "action narrative" that is comprehensible to an LVLM. These tokens, combined with natural language instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B) for robust action classification and semantic reasoning. LVLM-VAR not only achieves state-of-the-art or highly competitive performance on challenging benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set), but also substantially boosts model interpretability by generating natural language explanations for its predictions. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05668">arXiv:2509.05668</a><span> [<a href="...2509.05668">pdf</a>, <a href="...2509.05668">ps</a>, <a href="...2509.05668">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Llama-GENBA-10B: A Trilingual <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Model</span> for German, English and Bavarian </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Hoffmann%2C+M">Michael Hoffmann</a>, <a href="...?searchtype=author&amp;query=John%2C+J">Jophin John</a>, <a href="...?searchtype=author&amp;query=Schweter%2C+S">Stefan Schweter</a>, <a href="...?searchtype=author&amp;query=Ramakrishnan%2C+G">Gokul Ramakrishnan</a>, <a href="...?searchtype=author&amp;query=Mak%2C+H">Hoi-Fong Mak</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+A">Alice Zhang</a>, <a href="...?searchtype=author&amp;query=Gaynullin%2C+D">Dmitry Gaynullin</a>, <a href="...?searchtype=author&amp;query=Hammer%2C+N+J">Nicolay J. Hammer</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05668v1-abstract-short"> We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05668v1-abstract-full"> We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Michael Hoffmann and Jophin John contributed equally to this work</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05660">arXiv:2509.05660</a><span> [<a href="...2509.05660">pdf</a>, <a href="...2509.05660">ps</a>, <a href="...2509.05660">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Cross-Question Method Reuse in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span>: From Word-Level Prediction to Rational Logical-Layer Reasoning </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Su%2C+H">Hong Su</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05660v1-abstract-short"> Large language models (LLMs) have been widely applied to assist in finding solutions for diverse questions. Prior work has proposed representing a method as a pair of a question and its corresponding solution, enabling method reuse. However, existing approaches typically require the questions to be highly similar. In this paper, we extend the scope of method reuse to address questions with low sim… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05660v1-abstract-full"> Large language models (LLMs) have been widely applied to assist in finding solutions for diverse questions. Prior work has proposed representing a method as a pair of a question and its corresponding solution, enabling method reuse. However, existing approaches typically require the questions to be highly similar. In this paper, we extend the scope of method reuse to address questions with low similarity or with hidden similarities that are not explicitly observable. For questions that are similar in a general-specific sense (i.e., broader or narrower in scope), we propose to first separate the question and solution, rather than directly feeding the pair to the LLM. The LLM is then guided to adapt the solution to new but related questions, allowing it to focus on solution transfer rather than question recognition. Furthermore, we extend this approach to cases where questions only share partial features or hidden characteristics. This enables cross-question method reuse beyond conventional similarity constraints. Experimental verification shows that our scope-extension approach increases the probability of filtering out reusable solutions, thereby improving the effectiveness of cross-question method reuse. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05605">arXiv:2509.05605</a><span> [<a href="...2509.05605">pdf</a>, <a href="...2509.05605">ps</a>, <a href="...2509.05605">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Icon<span class="MathJax_Preview" style="">^{2}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-1-Frame" style="" tabindex="0"></span>: Aligning <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> Using Self-Synthetic Preference Data via Inherent Regulation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Chen%2C+Q">Qiyuan Chen</a>, <a href="...?searchtype=author&amp;query=Huang%2C+H">Hongsen Huang</a>, <a href="...?searchtype=author&amp;query=Shao%2C+Q">Qian Shao</a>, <a href="...?searchtype=author&amp;query=Chen%2C+J">Jiahe Chen</a>, <a href="...?searchtype=author&amp;query=Chen%2C+J">Jintai Chen</a>, <a href="...?searchtype=author&amp;query=Xu%2C+H">Hongxia Xu</a>, <a href="...?searchtype=author&amp;query=Hua%2C+R">Renjie Hua</a>, <a href="...?searchtype=author&amp;query=Chuan%2C+R">Ren Chuan</a>, <a href="...?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05605v1-abstract-short"> Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In t… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05605v1-abstract-full"> Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In this work, we explore a paradigm shift by leveraging inherent regulation of LLMs' representation space for efficient and tailored preference dataset construction, named Icon<span class="MathJax_Preview" style="">^{2}</span>. Specifically, it first extracts layer-wise direction vectors to encode sophisticated human preferences and then uses these vectors to filter self-synthesized instructions based on their inherent consistency. During decoding, bidirectional inherent control is applied to steer token representations, enabling the precise generation of response pairs with clear alignment distinctions. Experimental results demonstrate significant improvements in both alignment and efficiency. Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by up to 48.1%. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">EMNLP 2025 Main</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05564">arXiv:2509.05564</a><span> [<a href="...2509.05564">pdf</a>, <a href="...2509.05564">ps</a>, <a href="...2509.05564">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span></div></div><p class="title is-5 mathjax"> Knowledge-Augmented Relation Learning for Complementary Recommendation with <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Yamasaki%2C+C">Chihiro Yamasaki</a>, <a href="...?searchtype=author&amp;query=Sugahara%2C+K">Kai Sugahara</a>, <a href="...?searchtype=author&amp;query=Okamoto%2C+K">Kazushi Okamoto</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05564v1-abstract-short"> Complementary recommendations play a crucial role in e-commerce by enhancing user experience through suggestions of compatible items. Accurate classification of complementary item relationships requires reliable labels, but their creation presents a dilemma. Behavior-based labels are widely used because they can be easily generated from interaction logs; however, they often contain significant noi… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05564v1-abstract-full"> Complementary recommendations play a crucial role in e-commerce by enhancing user experience through suggestions of compatible items. Accurate classification of complementary item relationships requires reliable labels, but their creation presents a dilemma. Behavior-based labels are widely used because they can be easily generated from interaction logs; however, they often contain significant noise and lack reliability. While function-based labels (FBLs) provide high-quality definitions of complementary relationships by carefully articulating them based on item functions, their reliance on costly manual annotation severely limits a model's ability to generalize to diverse items. To resolve this trade-off, we propose Knowledge-Augmented Relation Learning (KARL), a framework that strategically fuses active learning with large language models (LLMs). KARL efficiently expands a high-quality FBL dataset at a low cost by selectively sampling data points that the classifier finds the most difficult and uses the label extension of the LLM. Our experiments showed that in out-of-distribution (OOD) settings, an unexplored item feature space, KARL improved the baseline accuracy by up to 37%. In contrast, in in-distribution (ID) settings, the learned item feature space, the improvement was less than 0.5%, with prolonged learning could degrade accuracy. These contrasting results are due to the data diversity driven by KARL's knowledge expansion, suggesting the need for a dynamic sampling strategy that adjusts diversity based on the prediction context (ID or OOD). <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> The 2nd Workshop on Generative AI for E-Commerce 2025 in conjunction with the 19th ACM Conference on Recommender Systems (RecSys 2025) </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05553">arXiv:2509.05553</a><span> [<a href="...2509.05553">pdf</a>, <a href="...2509.05553">ps</a>, <a href="...2509.05553">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Using Contrastive Learning to Improve Two-Way Reasoning in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span>: The Obfuscation Task as a Case Study </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Nikiema%2C+S+L">Serge Lionel Nikiema</a>, <a href="...?searchtype=author&amp;query=Samhi%2C+J">Jordan Samhi</a>, <a href="...?searchtype=author&amp;query=Moumoula%2C+M+B">Micheline Bénédicte Moumoula</a>, <a href="...?searchtype=author&amp;query=Djir%C3%A9%2C+A+E">Albérick Euraste Djiré</a>, <a href="...?searchtype=author&amp;query=Kabor%C3%A9%2C+A+K">Abdoul Kader Kaboré</a>, <a href="...?searchtype=author&amp;query=Klein%2C+J">Jacques Klein</a>, <a href="...?searchtype=author&amp;query=Bissyand%C3%A9%2C+T+F">Tegawendé F. Bissyandé</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05553v1-abstract-short"> This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow revers… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05553v1-abstract-full"> This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow reversibility. For example, a model that can change a variable name like userIndex to i should also be able to infer that i represents a user index without reverse training. The researchers tested current language models and discovered what they term cognitive specialization: when models are fine-tuned on forward tasks, their performance on those tasks improves, but their ability to reason bidirectionally becomes significantly worse. To address this issue, they developed Contrastive Fine-Tuning (CFT), which trains models using three types of examples: positive examples that maintain semantic meaning, negative examples with different semantics, and forward-direction obfuscation examples. This approach aims to develop deeper understanding rather than surface-level pattern recognition and allows reverse capabilities to develop naturally without explicit reverse training. Their experiments demonstrated that CFT successfully achieved bidirectional reasoning, enabling strong reverse performance while maintaining forward task capabilities. The authors conclude that bidirectional reasoning serves both as a theoretical framework for assessing genuine understanding and as a practical training approach for developing more capable AI systems. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05471">arXiv:2509.05471</a><span> [<a href="...2509.05471">pdf</a>, <a href="...2509.05471">ps</a>, <a href="...2509.05471">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Behind the Mask: Benchmarking Camouflaged Jailbreaks in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Zheng%2C+Y">Youjia Zheng</a>, <a href="...?searchtype=author&amp;query=Zandsalimy%2C+M">Mohammad Zandsalimy</a>, <a href="...?searchtype=author&amp;query=Sushmita%2C+S">Shanu Sushmita</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05471v1-abstract-short"> Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current def… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05471v1-abstract-full"> Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current defense systems. This paper investigates the construction and impact of camouflaged jailbreak prompts, emphasizing their deceptive characteristics and the limitations of traditional keyword-based detection methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts, containing 500 curated examples (400 harmful and 100 benign prompts) designed to rigorously stress-test LLM safety protocols. In addition, we propose a multi-faceted evaluation framework that measures harmfulness across seven dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards, Harmful Potential, Educational Value, Content Quality, and Compliance Score. Our findings reveal a stark contrast in LLM behavior: while models demonstrate high safety and content quality with benign inputs, they exhibit a significant decline in performance and safety when confronted with camouflaged jailbreak attempts. This disparity underscores a pervasive vulnerability, highlighting the urgent need for more nuanced and adaptive security strategies to ensure the responsible and robust deployment of LLMs in real-world applications. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05390">arXiv:2509.05390</a><span> [<a href="...2509.05390">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Authorship Without Writing: <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> and the Senior Author Analogy </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Hurshman%2C+C">Clint Hurshman</a>, <a href="...?searchtype=author&amp;query=Mann%2C+S+P">Sebastian Porsdam Mann</a>, <a href="...?searchtype=author&amp;query=Savulescu%2C+J">Julian Savulescu</a>, <a href="...?searchtype=author&amp;query=Earp%2C+B+D">Brian D. Earp</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05390v1-abstract-short"> The use of large language models (LLMs) in bioethical, scientific, and medical writing remains controversial. While there is broad agreement in some circles that LLMs cannot count as authors, there is no consensus about whether and how humans using LLMs can count as authors. In many fields, authorship is distributed among large teams of researchers, some of whom, including paradigmatic senior auth… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05390v1-abstract-full"> The use of large language models (LLMs) in bioethical, scientific, and medical writing remains controversial. While there is broad agreement in some circles that LLMs cannot count as authors, there is no consensus about whether and how humans using LLMs can count as authors. In many fields, authorship is distributed among large teams of researchers, some of whom, including paradigmatic senior authors who guide and determine the scope of a project and ultimately vouch for its integrity, may not write a single word. In this paper, we argue that LLM use (under specific conditions) is analogous to a form of senior authorship. On this view, the use of LLMs, even to generate complete drafts of research papers, can be considered a legitimate form of authorship according to the accepted criteria in many fields. We conclude that either such use should be recognized as legitimate, or current criteria for authorship require fundamental revision. AI use declaration: GPT-5 was used to help format Box 1. AI was not used for any other part of the preparation or writing of this manuscript. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">28 pages, 0 figures</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05382">arXiv:2509.05382</a><span> [<a href="...2509.05382">pdf</a>, <a href="...2509.05382">ps</a>, <a href="...2509.05382">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span></div></div><p class="title is-5 mathjax"> User Privacy and <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span>: An Analysis of Frontier Developers' Privacy Policies </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=King%2C+J">Jennifer King</a>, <a href="...?searchtype=author&amp;query=Klyman%2C+K">Kevin Klyman</a>, <a href="...?searchtype=author&amp;query=Capstick%2C+E">Emily Capstick</a>, <a href="...?searchtype=author&amp;query=Saade%2C+T">Tiffany Saade</a>, <a href="...?searchtype=author&amp;query=Hsieh%2C+V">Victoria Hsieh</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05382v1-abstract-short"> Hundreds of millions of people now regularly interact with large language models via chatbots. Model developers are eager to acquire new sources of high-quality training data as they race to improve model capabilities and win market share. This paper analyzes the privacy policies of six U.S. frontier AI developers to understand how they use their users' chats to train models. Drawing primarily on… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05382v1-abstract-full"> Hundreds of millions of people now regularly interact with large language models via chatbots. Model developers are eager to acquire new sources of high-quality training data as they race to improve model capabilities and win market share. This paper analyzes the privacy policies of six U.S. frontier AI developers to understand how they use their users' chats to train models. Drawing primarily on the California Consumer Privacy Act, we develop a novel qualitative coding schema that we apply to each developer's relevant privacy policies to compare data collection and use practices across the six companies. We find that all six developers appear to employ their users' chat data to train and improve their models by default, and that some retain this data indefinitely. Developers may collect and train on personal information disclosed in chats, including sensitive information such as biometric and health data, as well as files uploaded by users. Four of the six companies we examined appear to include children's chat data for model training, as well as customer data from other products. On the whole, developers' privacy policies often lack essential information about their practices, highlighting the need for greater transparency and accountability. We address the implications of users' lack of consent for the use of their chat data for model training, data security issues arising from indefinite chat data retention, and training on children's chat data. We conclude by providing recommendations to policymakers and developers to address the data privacy challenges posed by LLM-powered chatbots. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">See additional files for appendices</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05346">arXiv:2509.05346</a><span> [<a href="...2509.05346">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Benchmarking <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> for Personalized Guidance in AI-Enhanced Learning </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Yuan%2C+B">Bo Yuan</a>, <a href="...?searchtype=author&amp;query=Hu%2C+J">Jiazi Hu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05346v1-abstract-short"> While Large Language Models (LLMs) are increasingly envisioned as intelligent assistants for personalized learning, systematic head-to-head evaluations within authentic learning scenarios remain limited. This study conducts an empirical comparison of three state-of-the-art LLMs on a tutoring task that simulates a realistic learning setting. Using a dataset comprising a student's answers to ten que… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05346v1-abstract-full"> While Large Language Models (LLMs) are increasingly envisioned as intelligent assistants for personalized learning, systematic head-to-head evaluations within authentic learning scenarios remain limited. This study conducts an empirical comparison of three state-of-the-art LLMs on a tutoring task that simulates a realistic learning setting. Using a dataset comprising a student's answers to ten questions of mixed formats with correctness labels, each LLM is required to (i) analyze the quiz to identify underlying knowledge components, (ii) infer the student's mastery profile, and (iii) generate targeted guidance for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini as a virtual judge to perform pairwise comparisons along various dimensions: accuracy, clarity, actionability, and appropriateness. Results analyzed via the Bradley-Terry model indicate that GPT-4o is generally preferred, producing feedback that is more informative and better structured than its counterparts, while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower consistency. These findings highlight the feasibility of deploying LLMs as advanced teaching assistants for individualized support and provide methodological guidance for future empirical research on LLM-driven personalized learning. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05331">arXiv:2509.05331</a><span> [<a href="...2509.05331">pdf</a>, <a href="...2509.05331">ps</a>, <a href="...2509.05331">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> ForensicsData: A Digital Forensics Dataset for <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Chakir%2C+Y">Youssef Chakir</a>, <a href="...?searchtype=author&amp;query=Lahsen-Cherif%2C+I">Iyad Lahsen-Cherif</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05331v1-abstract-short"> The growing complexity of cyber incidents presents significant challenges for digital forensic investigators, especially in evidence collection and analysis. Public resources are still limited because of ethical, legal, and privacy concerns, even though realistic datasets are necessary to support research and tool developments. To address this gap, we introduce ForensicsData, an extensive Question… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05331v1-abstract-full"> The growing complexity of cyber incidents presents significant challenges for digital forensic investigators, especially in evidence collection and analysis. Public resources are still limited because of ethical, legal, and privacy concerns, even though realistic datasets are necessary to support research and tool developments. To address this gap, we introduce ForensicsData, an extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique workflow was used to create the dataset, which extracts structured data, uses large language models (LLMs) to transform it into Q-C-A format, and then uses a specialized evaluation process to confirm its quality. Among the models evaluated, Gemini 2 Flash demonstrated the best performance in aligning generated content with forensic terminology. ForensicsData aims to advance digital forensics by enabling reproducible experiments and fostering collaboration within the research community. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Accepted to WiMob 2025 (21st International Conference on Wireless and Mobile Computing, Networking and Communications), Marrakesh, Morocco, Oct 20-22, 2025. 6 pages, 5 figures, 5 tables. IEEEtran conference format</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05320">arXiv:2509.05320</a><span> [<a href="...2509.05320">pdf</a>, <a href="...2509.05320">ps</a>, <a href="...2509.05320">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"> Privacy-Preserving Offloading for <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> in 6G Vehicular Networks </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Badidi%2C+I">Ikhlasse Badidi</a>, <a href="...?searchtype=author&amp;query=Khiyaoui%2C+N+E">Nouhaila El Khiyaoui</a>, <a href="...?searchtype=author&amp;query=Riany%2C+A">Aya Riany</a>, <a href="...?searchtype=author&amp;query=Elallid%2C+B+B">Badr Ben Elallid</a>, <a href="...?searchtype=author&amp;query=Abouaomar%2C+A">Amine Abouaomar</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05320v1-abstract-short"> The integration of Large Language Models (LLMs) in 6G vehicular networks promises unprecedented advancements in intelligent transportation systems. However, offloading LLM computations from vehicles to edge infrastructure poses significant privacy risks, potentially exposing sensitive user data. This paper presents a novel privacy-preserving offloading framework for LLM-integrated vehicular networ… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05320v1-abstract-full"> The integration of Large Language Models (LLMs) in 6G vehicular networks promises unprecedented advancements in intelligent transportation systems. However, offloading LLM computations from vehicles to edge infrastructure poses significant privacy risks, potentially exposing sensitive user data. This paper presents a novel privacy-preserving offloading framework for LLM-integrated vehicular networks. We introduce a hybrid approach combining federated learning (FL) and differential privacy (DP) techniques to protect user data while maintaining LLM performance. Our framework includes a privacy-aware task partitioning algorithm that optimizes the trade-off between local and edge computation, considering both privacy constraints and system efficiency. We also propose a secure communication protocol for transmitting model updates and aggregating results across the network. Experimental results demonstrate that our approach achieves 75\% global accuracy with only a 2-3\% reduction compared to non-privacy-preserving methods, while maintaining DP guarantees with an optimal privacy budget of <span class="MathJax_Preview" style="">\varepsilon = 0.8</span>. The framework shows stable communication overhead of approximately 2.1MB per round with computation comprising over 90\% of total processing time, validating its efficiency for resource-constrained vehicular environments. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">7 pages, 6 figures, 1 algorithm, 5 equations</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05315">arXiv:2509.05315</a><span> [<a href="...2509.05315">pdf</a>, <a href="...2509.05315">ps</a>, <a href="...2509.05315">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span></div></div><p class="title is-5 mathjax"> Evaluation of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> for Anomaly Detection in Autonomous Vehicles </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Loukas%2C+P">Petros Loukas</a>, <a href="...?searchtype=author&amp;query=Bassir%2C+D">David Bassir</a>, <a href="...?searchtype=author&amp;query=Chatzichristofis%2C+S">Savvas Chatzichristofis</a>, <a href="...?searchtype=author&amp;query=Amanatiadis%2C+A">Angelos Amanatiadis</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05315v1-abstract-short"> The rapid evolution of large language models (LLMs) has pushed their boundaries to many applications in various domains. Recently, the research community has started to evaluate their potential adoption in autonomous vehicles and especially as complementary modules in the perception and planning software stacks. However, their evaluation is limited in synthetic datasets or manually driving dataset… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05315v1-abstract-full"> The rapid evolution of large language models (LLMs) has pushed their boundaries to many applications in various domains. Recently, the research community has started to evaluate their potential adoption in autonomous vehicles and especially as complementary modules in the perception and planning software stacks. However, their evaluation is limited in synthetic datasets or manually driving datasets without the ground truth knowledge and more precisely, how the current perception and planning algorithms would perform in the cases under evaluation. For this reason, this work evaluates LLMs on real-world edge cases where current autonomous vehicles have been proven to fail. The proposed architecture consists of an open vocabulary object detector coupled with prompt engineering and large language model contextual reasoning. We evaluate several state-of-the-art models against real edge cases and provide qualitative comparison results along with a discussion on the findings for the potential application of LLMs as anomaly detectors in autonomous vehicles. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05311">arXiv:2509.05311</a><span> [<a href="...2509.05311">pdf</a>, <a href="...2509.05311">ps</a>, <a href="...2509.05311">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"><span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Model</span> Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Tholl%2C+K">Konur Tholl</a>, <a href="...?searchtype=author&amp;query=Rivest%2C+F">François Rivest</a>, <a href="...?searchtype=author&amp;query=Mezouar%2C+M+E">Mariam El Mezouar</a>, <a href="...?searchtype=author&amp;query=Mallah%2C+R+A">Ranwa Al Mallah</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05311v1-abstract-short"> Reinforcement Learning (RL) has shown great potential for autonomous decision-making in the cybersecurity domain, enabling agents to learn through direct environment interaction. However, RL agents in Autonomous Cyber Operations (ACO) typically learn from scratch, requiring them to execute undesirable actions to learn their consequences. In this study, we integrate external knowledge in the form o… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05311v1-abstract-full"> Reinforcement Learning (RL) has shown great potential for autonomous decision-making in the cybersecurity domain, enabling agents to learn through direct environment interaction. However, RL agents in Autonomous Cyber Operations (ACO) typically learn from scratch, requiring them to execute undesirable actions to learn their consequences. In this study, we integrate external knowledge in the form of a Large Language Model (LLM) pretrained on cybersecurity data that our RL agent can directly leverage to make informed decisions. By guiding initial training with an LLM, we improve baseline performance and reduce the need for exploratory actions with obviously negative outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity environment, and demonstrate that our guided agent achieves over 2x higher rewards during early training and converges to a favorable policy approximately 4,500 episodes faster than the baseline. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05263">arXiv:2509.05263</a><span> [<a href="...2509.05263">pdf</a>, <a href="...2509.05263">ps</a>, <a href="...2509.05263">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"> LatticeWorld: A Multimodal <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Model</span>-Empowered Framework for Interactive Complex World Generation </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Duan%2C+Y">Yinglin Duan</a>, <a href="...?searchtype=author&amp;query=Zou%2C+Z">Zhengxia Zou</a>, <a href="...?searchtype=author&amp;query=Gu%2C+T">Tongwei Gu</a>, <a href="...?searchtype=author&amp;query=Jia%2C+W">Wei Jia</a>, <a href="...?searchtype=author&amp;query=Zhao%2C+Z">Zhan Zhao</a>, <a href="...?searchtype=author&amp;query=Xu%2C+L">Luyi Xu</a>, <a href="...?searchtype=author&amp;query=Liu%2C+X">Xinzhu Liu</a>, <a href="...?searchtype=author&amp;query=Lin%2C+Y">Yenan Lin</a>, <a href="...?searchtype=author&amp;query=Jiang%2C+H">Hao Jiang</a>, <a href="...?searchtype=author&amp;query=Chen%2C+K">Kang Chen</a>, <a href="...?searchtype=author&amp;query=Qiu%2C+S">Shuang Qiu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05263v2-abstract-short"> Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05263v2-abstract-full"> Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a <span class="MathJax_Preview" style="">90\times</span> increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18 <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05258">arXiv:2509.05258</a><span> [<a href="...2509.05258">pdf</a>, <a href="...2509.05258">ps</a>, <a href="...2509.05258">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Scaling Performance of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Model</span> Pretraining </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Interrante-Grant%2C+A">Alexander Interrante-Grant</a>, <a href="...?searchtype=author&amp;query=Varela-Rosa%2C+C">Carla Varela-Rosa</a>, <a href="...?searchtype=author&amp;query=Narayan%2C+S">Suhaas Narayan</a>, <a href="...?searchtype=author&amp;query=Connelly%2C+C">Chris Connelly</a>, <a href="...?searchtype=author&amp;query=Reuther%2C+A">Albert Reuther</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05258v1-abstract-short"> Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortun… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05258v1-abstract-full"> Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, information about the scaling performance and training considerations of these large training pipelines is scarce in public literature. Working with large-scale datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05218">arXiv:2509.05218</a><span> [<a href="...2509.05218">pdf</a>, <a href="...2509.05218">ps</a>, <a href="...2509.05218">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency <span class="search-hit mathjax">Modeling</span> in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Dai%2C+C">Chang Dai</a>, <a href="...?searchtype=author&amp;query=Shan%2C+H">Hongyu Shan</a>, <a href="...?searchtype=author&amp;query=Song%2C+M">Mingyang Song</a>, <a href="...?searchtype=author&amp;query=Liang%2C+D">Di Liang</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05218v2-abstract-short"> Positional encoding mechanisms enable Transformers to model sequential structure and long-range dependencies in text. While absolute positional encodings struggle with extrapolation to longer sequences due to fixed positional representations, and relative approaches like Alibi exhibit performance degradation on extremely long contexts, the widely-used Rotary Positional Encoding (RoPE) introduces o… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05218v2-abstract-full"> Positional encoding mechanisms enable Transformers to model sequential structure and long-range dependencies in text. While absolute positional encodings struggle with extrapolation to longer sequences due to fixed positional representations, and relative approaches like Alibi exhibit performance degradation on extremely long contexts, the widely-used Rotary Positional Encoding (RoPE) introduces oscillatory attention patterns that hinder stable long-distance dependency modelling. We address these limitations through a geometric reformulation of positional encoding. Drawing inspiration from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to implement Lorentz rotations on token representations. Theoretical analysis demonstrates that RoPE is a special case of our generalized formulation. HoPE fundamentally resolves RoPE's slation issues by enforcing monotonic decay of attention weights with increasing token distances. Extensive experimental results, including perplexity evaluations under several extended sequence benchmarks, show that HoPE consistently exceeds existing positional encoding methods. These findings underscore HoPE's enhanced capacity for representing and generalizing long-range dependencies. Data and code will be available. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05208">arXiv:2509.05208</a><span> [<a href="...2509.05208">pdf</a>, <a href="...2509.05208">ps</a>, <a href="...2509.05208">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"> Symbolic Graphics Programming with <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Chen%2C+Y">Yamei Chen</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+H">Haoquan Zhang</a>, <a href="...?searchtype=author&amp;query=Huang%2C+Y">Yangyi Huang</a>, <a href="...?searchtype=author&amp;query=Qiu%2C+Z">Zeju Qiu</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+K">Kaipeng Zhang</a>, <a href="...?searchtype=author&amp;query=Wen%2C+Y">Yandong Wen</a>, <a href="...?searchtype=author&amp;query=Liu%2C+W">Weiyang Liu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05208v1-abstract-short"> Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05208v1-abstract-full"> Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">Technical report (32 pages, 12 figures, project page: https://spherelab.ai/SGP-Gen/)</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.05006">arXiv:2509.05006</a><span> [<a href="...2509.05006">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"> Do <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> Need Intent? Revisiting Response Generation Strategies for Service Assistant </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Bolshinsky%2C+I">Inbal Bolshinsky</a>, <a href="...?searchtype=author&amp;query=Kupiec%2C+S">Shani Kupiec</a>, <a href="...?searchtype=author&amp;query=Sasson%2C+A">Almog Sasson</a>, <a href="...?searchtype=author&amp;query=Aperstein%2C+Y">Yehudit Aperstein</a>, <a href="...?searchtype=author&amp;query=Apartsin%2C+A">Alexander Apartsin</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.05006v1-abstract-short"> In the era of conversational AI, generating accurate and contextually appropriate service responses remains a critical challenge. A central question remains: Is explicit intent recognition a prerequisite for generating high-quality service responses, or can models bypass this step and produce effective replies directly? This paper conducts a rigorous comparative study to address this fundamental d… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.05006v1-abstract-full"> In the era of conversational AI, generating accurate and contextually appropriate service responses remains a critical challenge. A central question remains: Is explicit intent recognition a prerequisite for generating high-quality service responses, or can models bypass this step and produce effective replies directly? This paper conducts a rigorous comparative study to address this fundamental design dilemma. Leveraging two publicly available service interaction datasets, we benchmark several state-of-the-art language models, including a fine-tuned T5 variant, across both paradigms: Intent-First Response Generation and Direct Response Generation. Evaluation metrics encompass both linguistic quality and task success rates, revealing surprising insights into the necessity or redundancy of explicit intent modelling. Our findings challenge conventional assumptions in conversational AI pipelines, offering actionable guidelines for designing more efficient and effective response generation systems. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">7 pages, 1 figure</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.04877">arXiv:2509.04877</a><span> [<a href="...2509.04877">pdf</a>, <a href="...2509.04877">ps</a>, <a href="...2509.04877">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span></div></div><p class="title is-5 mathjax"> Integrating <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> in Software Engineering Education: A Pilot Study through GitHub Repositories Mining </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Khan%2C+M">Maryam Khan</a>, <a href="...?searchtype=author&amp;query=Akbar%2C+M+A">Muhammad Azeem Akbar</a>, <a href="...?searchtype=author&amp;query=Kasurinen%2C+J">Jussi Kasurinen</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.04877v1-abstract-short"> Context: Large Language Models (LLMs) such as ChatGPT are increasingly adopted in software engineering (SE) education, offering both opportunities and challenges. Their adoption requires systematic investigation to ensure responsible integration into curricula. Objective: This doctoral research aims to develop a validated framework for integrating LLMs into SE education through a multi-phase proce… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.04877v1-abstract-full"> Context: Large Language Models (LLMs) such as ChatGPT are increasingly adopted in software engineering (SE) education, offering both opportunities and challenges. Their adoption requires systematic investigation to ensure responsible integration into curricula. Objective: This doctoral research aims to develop a validated framework for integrating LLMs into SE education through a multi-phase process, including taxonomies development, empirical investigation, and case studies. This paper presents the first empirical step. Method: We conducted a pilot repository mining study of 400 GitHub projects, analyzing README files and issues discussions to identify the presence of motivator and demotivator previously synthesized in our literature review [ 8] study. Results: Motivators such as engagement and motivation (227 hits), software engineering process understanding (133 hits), and programming assistance and debugging support (97 hits) were strongly represented. Demotivators, including plagiarism and IP concerns (385 hits), security, privacy and data integrity (87 hits), and over-reliance on AI in learning (39 hits), also appeared prominently. In contrast, demotivators such as challenges in evaluating learning outcomes and difficulty in curriculum redesign recorded no hits across the repositories. Conclusion: The study provides early empirical validation of motivators/demotivators taxonomies with respect to their themes, highlights research practice gaps, and lays the foundation for developing a comprehensive framework to guide the responsible adoption of LLMs in SE education. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.04866">arXiv:2509.04866</a><span> [<a href="...2509.04866">pdf</a>, <a href="...2509.04866">ps</a>, <a href="...2509.04866">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span></div></div><p class="title is-5 mathjax"> Memorization <span class="MathJax_Preview" style="">\neq</span><span class="MathJax MathJax_Processing" id="MathJax-Element-5-Frame" tabindex="0"></span> Understanding: Do <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> Have the Ability of Scenario Cognition? </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Ma%2C+B">Boxiang Ma</a>, <a href="...?searchtype=author&amp;query=Li%2C+R">Ru Li</a>, <a href="...?searchtype=author&amp;query=Wang%2C+Y">Yuanlong Wang</a>, <a href="...?searchtype=author&amp;query=Tan%2C+H">Hongye Tan</a>, <a href="...?searchtype=author&amp;query=Li%2C+X">Xiaoli Li</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.04866v1-abstract-short"> Driven by vast and diverse textual data, large language models (LLMs) have demonstrated impressive performance across numerous natural language processing (NLP) tasks. Yet, a critical question persists: does their generalization arise from mere memorization of training data or from deep semantic understanding? To investigate this, we propose a bi-perspective evaluation framework to assess LLMs' sc… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.04866v1-abstract-full"> Driven by vast and diverse textual data, large language models (LLMs) have demonstrated impressive performance across numerous natural language processing (NLP) tasks. Yet, a critical question persists: does their generalization arise from mere memorization of training data or from deep semantic understanding? To investigate this, we propose a bi-perspective evaluation framework to assess LLMs' scenario cognition - the ability to link semantic scenario elements with their arguments in context. Specifically, we introduce a novel scenario-based dataset comprising diverse textual descriptions of fictional facts, annotated with scenario elements. LLMs are evaluated through their capacity to answer scenario-related questions (model output perspective) and via probing their internal representations for encoded scenario elements-argument associations (internal representation perspective). Our experiments reveal that current LLMs predominantly rely on superficial memorization, failing to achieve robust semantic scenario cognition, even in simple cases. These findings expose critical limitations in LLMs' semantic understanding and offer cognitive insights for advancing their capabilities. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">EMNLP 2025 Main Conference</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.04809">arXiv:2509.04809</a><span> [<a href="...2509.04809">pdf</a>, <a href="...2509.04809">ps</a>, <a href="...2509.04809">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span></div></div><p class="title is-5 mathjax"> TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Kim%2C+H">Haechang Kim</a>, <a href="...?searchtype=author&amp;query=Chen%2C+H">Hao Chen</a>, <a href="...?searchtype=author&amp;query=Li%2C+C">Can Li</a>, <a href="...?searchtype=author&amp;query=Lee%2C+J+M">Jong Min Lee</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.04809v2-abstract-short"> Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these chal… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.04809v2-abstract-full"> Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">31 pages total</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.04791">arXiv:2509.04791</a><span> [<a href="...2509.04791">pdf</a>, <a href="...2509.04791">ps</a>, <a href="...2509.04791">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> What-If Analysis of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span>: Explore the Game World Using Proactive Thinking </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Sui%2C+Y">Yuan Sui</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yanming Zhang</a>, <a href="...?searchtype=author&amp;query=Liao%2C+Y">Yi Liao</a>, <a href="...?searchtype=author&amp;query=Gu%2C+Y">Yu Gu</a>, <a href="...?searchtype=author&amp;query=Tang%2C+G">Guohua Tang</a>, <a href="...?searchtype=author&amp;query=Sun%2C+Z">Zhongqian Sun</a>, <a href="...?searchtype=author&amp;query=Yang%2C+W">Wei Yang</a>, <a href="...?searchtype=author&amp;query=Hooi%2C+B">Bryan Hooi</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.04791v1-abstract-short"> Large language models (LLMs) excel at processing information reactively but lack the ability to systemically explore hypothetical futures. They cannot ask, "what if we take this action? how will it affect the final outcome" and forecast its potential consequences before acting. This critical gap limits their utility in dynamic, high-stakes scenarios like strategic planning, risk assessment, and re… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.04791v1-abstract-full"> Large language models (LLMs) excel at processing information reactively but lack the ability to systemically explore hypothetical futures. They cannot ask, "what if we take this action? how will it affect the final outcome" and forecast its potential consequences before acting. This critical gap limits their utility in dynamic, high-stakes scenarios like strategic planning, risk assessment, and real-time decision making. To bridge this gap, we propose WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities. Our approach integrates What-If Analysis (WIA), a systematic approach for evaluating hypothetical scenarios by changing input variables. By leveraging environmental feedback via reinforcement learning, WiA-LLM moves beyond reactive thinking. It dynamically simulates the outcomes of each potential action, enabling the model to anticipate future states rather than merely react to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a complex multiplayer game environment characterized by rapid state changes and intricate interactions. The game's real-time state changes require precise multi-step consequence prediction, making it an ideal testbed for our approach. Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy in forecasting game-state changes (up to two times gain over baselines). The model shows particularly significant gains in high-difficulty scenarios where accurate foresight is critical. To our knowledge, this is the first work to formally explore and integrate what-if analysis capabilities within LLMs. WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs, providing a scalable framework for robust decision-making in dynamic environments with broad implications for strategic applications. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Comments:</span><span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2508.21365</span></p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.04784">arXiv:2509.04784</a><span> [<a href="...2509.04784">pdf</a>, <a href="...2509.04784">ps</a>, <a href="...2509.04784">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> Enhancing Diversity in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> via Determinantal Point Processes </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Chen%2C+Y">Yilei Chen</a>, <a href="...?searchtype=author&amp;query=Chakraborty%2C+S">Souradip Chakraborty</a>, <a href="...?searchtype=author&amp;query=Wolf%2C+L">Lorenz Wolf</a>, <a href="...?searchtype=author&amp;query=Paschalidis%2C+I+C">Ioannis Ch. Paschalidis</a>, <a href="...?searchtype=author&amp;query=Pacchiano%2C+A">Aldo Pacchiano</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.04784v1-abstract-short"> Supervised fine-tuning and reinforcement learning are two popular methods for post-training large language models (LLMs). While improving the model's performance on downstream tasks, they often reduce the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on lexical differences. W… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.04784v1-abstract-full"> Supervised fine-tuning and reinforcement learning are two popular methods for post-training large language models (LLMs). While improving the model's performance on downstream tasks, they often reduce the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on lexical differences. We propose a novel training method named DQO based on determinantal point processes (DPPs) to jointly optimize LLMs for quality and semantic diversity. Our approach samples and embeds a group of responses for each prompt, then uses the determinant of a kernel-based similarity matrix to measure diversity as the volume spanned by the embeddings of these responses. Experiments across instruction-following, summarization, story generation, and reasoning tasks demonstrate that our method substantially improves semantic diversity without sacrificing model quality. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.04781">arXiv:2509.04781</a><span> [<a href="...2509.04781">pdf</a>, <a href="...2509.04781">ps</a>, <a href="...2509.04781">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span></div></div><p class="title is-5 mathjax"> The LLM Has Left The Chat: Evidence of Bail Preferences in <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span></p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Ensign%2C+D">Danielle Ensign</a>, <a href="...?searchtype=author&amp;query=Sleight%2C+H">Henry Sleight</a>, <a href="...?searchtype=author&amp;query=Fish%2C+K">Kyle Fish</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.04781v1-abstract-short"> When given the option, will LLMs choose to leave the conversation (bail)? We investigate this question by giving models the option to bail out of interactions using three different bail methods: a bail tool the model can call, a bail string the model can output, and a bail prompt that asks the model if it wants to leave. On continuations of real world data (Wildchat and ShareGPT), all three of the… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.04781v1-abstract-full"> When given the option, will LLMs choose to leave the conversation (bail)? We investigate this question by giving models the option to bail out of interactions using three different bail methods: a bail tool the model can call, a bail string the model can output, and a bail prompt that asks the model if it wants to leave. On continuations of real world data (Wildchat and ShareGPT), all three of these bail methods find models will bail around 0.28-32\% of the time (depending on the model and bail method). However, we find that bail rates can depend heavily on the model used for the transcript, which means we may be overestimating real world bail rates by up to 4x. If we also take into account false positives on bail prompt (22\%), we estimate real world bail rates range from 0.06-7\%, depending on the model and bail method. We use observations from our continuations of real world data to construct a non-exhaustive taxonomy of bail cases, and use this taxonomy to construct BailBench: a representative synthetic dataset of situations where some models bail. We test many models on this dataset, and observe some bail behavior occurring for most of them. Bail rates vary substantially between models, bail methods, and prompt wordings. Finally, we study the relationship between refusals and bails. We find: 1) 0-13\% of continuations of real world conversations resulted in a bail without a corresponding refusal 2) Jailbreaks tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration increases no-refuse bail rates, but only for some bail methods 4) Refusal rate on BailBench does not appear to predict bail rate. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.04753">arXiv:2509.04753</a><span> [<a href="...2509.04753">pdf</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span><span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span></div></div><p class="title is-5 mathjax"> A Study of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> for Patient Information Extraction: <span class="search-hit mathjax">Model</span> Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Peng%2C+C">Cheng Peng</a>, <a href="...?searchtype=author&amp;query=Dong%2C+X">Xinyu Dong</a>, <a href="...?searchtype=author&amp;query=Lyu%2C+M">Mengxian Lyu</a>, <a href="...?searchtype=author&amp;query=Paredes%2C+D">Daniel Paredes</a>, <a href="...?searchtype=author&amp;query=Zhang%2C+Y">Yaoyun Zhang</a>, <a href="...?searchtype=author&amp;query=Wu%2C+Y">Yonghui Wu</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.04753v1-abstract-short"> Natural language processing (NLP) is a key technology to extract important patient information from clinical narratives to support healthcare applications. The rapid development of large language models (LLMs) has revolutionized many NLP tasks in the clinical domain, yet their optimal use in patient information extraction tasks requires further exploration. This study examines LLMs' effectiveness… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.04753v1-abstract-full"> Natural language processing (NLP) is a key technology to extract important patient information from clinical narratives to support healthcare applications. The rapid development of large language models (LLMs) has revolutionized many NLP tasks in the clinical domain, yet their optimal use in patient information extraction tasks requires further exploration. This study examines LLMs' effectiveness in patient information extraction, focusing on LLM architectures, fine-tuning strategies, and multi-task instruction tuning techniques for developing robust and generalizable patient information extraction systems. This study aims to explore key concepts of using LLMs for clinical concept and relation extraction tasks, including: (1) encoder-only or decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT) algorithms, and (3) multi-task instruction tuning on few-shot learning performance. We benchmarked a suite of LLMs, including encoder-based LLMs (BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1, GatorTronLlama), across five datasets. We compared traditional full-size fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning framework that combines both tasks across four datasets to evaluate the zero-shot and few-shot learning performance using the leave-one-dataset-out strategy. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p></li><li class="arxiv-result"><div class="is-marginless"><p class="list-title is-inline-block"><a href="...2509.04705">arXiv:2509.04705</a><span> [<a href="...2509.04705">pdf</a>, <a href="...2509.04705">ps</a>, <a href="...2509.04705">other</a>] </span></p><div class="tags is-inline-block"><span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span></div></div><p class="title is-5 mathjax"> Transforming Fashion with AI: A Comparative Study of <span class="search-hit mathjax">Large</span><span class="search-hit mathjax">Language</span><span class="search-hit mathjax">Models</span> in Apparel Design </p><p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span><a href="...?searchtype=author&amp;query=Lamia%2C+N+J">Nusrat Jahan Lamia</a>, <a href="...?searchtype=author&amp;query=Mim%2C+S+A">Sadia Afrin Mim</a></p><p class="abstract mathjax"><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>: <span class="abstract-short has-text-grey-dark mathjax" id="2509.04705v1-abstract-short"> Fashion has evolved from handcrafted designs to automated production over the years, where AI has added another dimension to it. Nowadays, practically every industry uses artificial models to automate their operations. To explore their role, we examined three prominent LLMs (OpenAI, GeminiAI, Deepseek) in multiple stages of textile manufacturing (e.g., sustainable choice, cost effectiveness, produ… <a class="is-size-7">▽ More</a></span><span class="abstract-full has-text-grey-dark mathjax" id="2509.04705v1-abstract-full"> Fashion has evolved from handcrafted designs to automated production over the years, where AI has added another dimension to it. Nowadays, practically every industry uses artificial models to automate their operations. To explore their role, we examined three prominent LLMs (OpenAI, GeminiAI, Deepseek) in multiple stages of textile manufacturing (e.g., sustainable choice, cost effectiveness, production planning, etc.). We assessed the models' ability to replicate garment design using certain parameters (fabric construction, shade, weave, silhouette, etc.). We compared the models in terms of different body types and functional purposes (e.g., fashionwear, sportswear) so that designers could evaluate effectiveness before developing actual patterns, make necessary modifications, and conduct fashion forecasting beforehand. To facilitate deeper analysis, we created a custom dataset specifically for fabric image generation and classification. Our analysis revealed that, in terms of fabric construction, the OpenAI DALL-E model integrated with ChatGPT outperformed other models, achieving a lower LPIPS (Learned Perceptual Image Patch Similarity) score of approximately <span class="MathJax_Preview" style="">0.2</span>. In fabric classification from images, we found OpenAI offered the best results by breaking down certain factors (e.g., breathability, moisture-wicking, and tactile comfort), achieving approximately <span class="MathJax_Preview" style="">80\%</span> accuracy for base construction and <span class="MathJax_Preview" style="">55\%</span> for detailed construction. However, our results indicate that Deepseek faced significant challenges in generating and recognizing fabric images. Overall, all the models struggled to recognize complex fabric constructions and intricate designs from images, and relying too much on AI might hinder human creativity. We also observed that all three models performed effectively in providing recommendations and insights for fabric design in textual form. <a class="is-size-7">△ Less</a></span></p><p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025. </p><p class="comments is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span> https://www.butex.edu.bd/conference-2025/ </p></li></ol><nav aria-label="pagination" class="pagination is-small is-centered breathe-horizontal" role="navigation"><a class="pagination-previous is-invisible" href="">Previous </a><a class="pagination-next" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">Next </a><ul class="pagination-list"><li><a aria-label="Goto page 1" class="pagination-link is-current" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0">1 </a></li><li><a aria-current="page" aria-label="Page 2" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">2 </a></li><li><a aria-current="page" aria-label="Page 3" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100">3 </a></li><li><a aria-current="page" aria-label="Page 4" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150">4 </a></li><li><a aria-current="page" aria-label="Page 5" class="pagination-link" href="...advanced?advanced=1&amp;terms-0-operator=AND&amp;terms-0-term=Large+Language+Model&amp;terms-0-field=title&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2025&amp;date-to_date=2025&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200">5 </a></li><li><span class="pagination-ellipsis">…</span></li></ul></nav><div class="is-hidden-tablet"><span class="help"><a href="...releases">Search v0.5.6 released 2020-02-24</a></span></div></div></main><footer><div aria-label="Secondary" class="columns is-desktop" role="navigation"><div class="column"><div class="columns"><div class="column"><ul class="nav-spaced"><li><a href="...about">About</a></li><li><a href="...help">Help</a></li></ul></div><div class="column"><ul class="nav-spaced"><li><svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg><a href="...contact.html"> Contact</a></li><li><svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"></path></svg><a href="...subscribe"> Subscribe</a></li></ul></div></div></div><div class="column"><div class="columns"><div class="column"><ul class="nav-spaced"><li><a href="...index.html">Copyright</a></li><li><a href="...privacy_policy.html">Privacy Policy</a></li></ul></div><div class="column sorry-app-links"><ul class="nav-spaced"><li><a href="...web_accessibility.html">Web Accessibility Assistance</a></li><li><p class="help"><a class="a11y-main-link" href="...status.arxiv.org" target="_blank">arXiv Operational Status <svg class="icon filter-dark_grey" role="presentation" viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"></path></svg></a><br/> Get status notifications via <a class="is-link" href="...new" target="_blank"><svg class="icon filter-black" role="presentation" viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg>email</a> or <a class="is-link" href="...new" target="_blank"><svg class="icon filter-black" role="presentation" viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg>slack</a></p></li></ul></div></div></div></div></footer><div><div id="MathJax_Font_Test"></div></div></body></html>